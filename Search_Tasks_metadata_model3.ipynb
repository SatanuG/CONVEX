{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2202,
     "status": "ok",
     "timestamp": 1597432354599,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "1cH0FHH2XGyl",
    "outputId": "75c262c6-abf0-4675-8adf-7a5994db9a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3265,
     "status": "ok",
     "timestamp": 1597432283556,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "miOJ_VF6XGyv"
   },
   "outputs": [],
   "source": [
    "def get_metadata_features(i):\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.head())\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    X_train_meta = df_X_train.drop(['Previous_User_Utterance'], axis=1)\n",
    "    X_test_meta = df_X_test.drop(['Previous_User_Utterance'], axis=1)\n",
    "    \n",
    "    #print(X_train_meta['Previous_Speech_Act'].unique())\n",
    "    #print(X_train_meta['Previous_Search_Act'].unique())\n",
    "    \n",
    "    return X_train_meta, X_test_meta, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6691,
     "status": "ok",
     "timestamp": 1597432291012,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "JF0FNdPuXGy1"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5495,
     "status": "ok",
     "timestamp": 1597432294199,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "iV8Z64mbXGzA"
   },
   "outputs": [],
   "source": [
    "def create_bilstm_model_meta(reshaped_data):\n",
    "    samples,timesteps,features = reshaped_data.shape\n",
    "    input_data = Input(shape=(timesteps,features))\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(input_data) \n",
    "    #x = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25))(input_data) \n",
    "    x, sent_coeffs = AttentionLayer(features,return_coefficients=True,name='sent_attention')(x)\n",
    "    #x = GlobalMaxPool1D()(x)\n",
    "    #x = Dense(100, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(4, activation=\"softmax\")(x) #output layer\n",
    "    model = Model(inputs=input_data, outputs=x)\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1191,
     "status": "ok",
     "timestamp": 1597432654501,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "hwQMhJSmXGzM"
   },
   "outputs": [],
   "source": [
    "def execute_bilstm_metadata(i):\n",
    "    df_train_meta, df_test_meta, df_Y_train, df_Y_test = get_metadata_features(i)\n",
    "    train_meta = df_train_meta.values\n",
    "    test_meta = df_test_meta.values\n",
    "    print(\"Shape check post file read:\")\n",
    "    #print(train_meta)\n",
    "    print(train_meta.shape, test_meta.shape)   \n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_meta.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped = np.concatenate([train_meta[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped = X_train_reshaped.reshape(staggering, timestamp, c) # c is the number of features\n",
    "    #print(X_train_reshaped)\n",
    "    print(\"X_train_reshaped.shape:\")\n",
    "    print(X_train_reshaped.shape)\n",
    "    \n",
    "    r2, c2 = df_test_meta.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped = np.concatenate([test_meta[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped = X_test_reshaped.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    #print(X_test_reshaped)\n",
    "    print(\"X_test_reshaped.shape:\")\n",
    "    print(X_test_reshaped.shape)\n",
    "    \n",
    "    df_Y_train = df_Y_train.iloc[timestamp-1:,]\n",
    "    df_Y_test = df_Y_test.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Shape check for train and test sets post reshaping for lstm:\")\n",
    "    print(X_train_reshaped.shape, X_test_reshaped.shape)\n",
    "    print(df_Y_train.shape, df_Y_test.shape)\n",
    "    \n",
    "    model = create_bilstm_model_meta(X_train_reshaped)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding the output labels\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_Y_train)\n",
    "    training_op_labels_encoded = encoder.transform(df_Y_train)\n",
    "    test_op_labels_encoded = encoder.transform(df_Y_test)\n",
    "    print(\"Training and test output labels(before one-hot/after enc)\")\n",
    "    print(training_op_labels_encoded.shape, test_op_labels_encoded.shape)\n",
    "    \"\"\"\n",
    "    converting the output labels to one-hot form\n",
    "    \"\"\"\n",
    "    training_op_labels_onehot= np_utils.to_categorical(training_op_labels_encoded)\n",
    "    test_op_labels_onehot = np_utils.to_categorical(test_op_labels_encoded)\n",
    "\n",
    "    #print(training_op_labels_onehot)\n",
    "    print(\"Training and test output labels(after one-hot)\")\n",
    "    print(training_op_labels_onehot.shape, len(training_op_labels_onehot))\n",
    "    print(test_op_labels_onehot.shape, len(test_op_labels_onehot))\n",
    "\n",
    "    print(\"X_train_reshaped\", X_train_reshaped.shape)\n",
    "    print(\"training_op_labels_onehot\", training_op_labels_onehot.shape)\n",
    "\n",
    "    X_test_reshaped = np.asarray(X_test_reshaped, dtype='float32')\n",
    "    X_train_reshaped = np.asarray(X_train_reshaped, dtype='float32')\n",
    "    model.fit(X_train_reshaped, training_op_labels_onehot, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)\n",
    "\n",
    "    \"\"\"\n",
    "    evaluate model\n",
    "    \"\"\"\n",
    "    print(model.metrics_names)\n",
    "    loss, binary_accuracy, categorical_accuracy = model.evaluate(X_test_reshaped, y = test_op_labels_onehot, batch_size=BATCH_SIZE, verbose=1)\n",
    "    \n",
    "    print(loss, binary_accuracy, categorical_accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "    predict the probabilty of output classes\n",
    "    and pick the best one\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_vector = model.predict(X_test_reshaped, batch_size=BATCH_SIZE, verbose=1)\n",
    "    predicted_classes = np.argmax(prediction_vector, axis=1)\n",
    "    original_classes = np.argmax(test_op_labels_onehot, axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # verification of correctness:\n",
    "    total_correct = sum(original_classes == predicted_classes)\n",
    "    print(\"Total number of correct predictions:\",total_correct)\n",
    "    print(\"Accuracy:\",total_correct/len(test_op_labels_onehot))\n",
    "    acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
    "    print('Overall accuracy: {} %'.format(acc*100))\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(predicted_classes, original_classes)\n",
    "    print(conf_mat)\n",
    "\n",
    "    \"\"\"\n",
    "    # show the inputs and predicted outputs\n",
    "    list_bilstm_prediction = []    \n",
    "    for i in range(len(X_test_reshaped)):\n",
    "        row = []\n",
    "        row.extend(X_test_reshaped[i])\n",
    "        row.extend(test_op_labels_onehot[i])\n",
    "        list_bilstm_prediction.append(row)\n",
    "    df_bilstm_prediction = pd.DataFrame(data=list_bilstm_prediction)\n",
    "    df_bilstm_prediction.to_pickle(\"nlp_bilstm_predictions.pkl\")\n",
    "    \"\"\"\n",
    "       \n",
    "    return predicted_classes, categorical_accuracy, binary_accuracy, conf_mat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13229,
     "status": "ok",
     "timestamp": 1597432817527,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "wBvx5PMKXGzV",
    "outputId": "bf7b532d-6dfa-4738-d58a-e1d3ff5c8958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape check post file read:\n",
      "(406, 8) (102, 8)\n",
      "X_train_reshaped.shape:\n",
      "(406, 1, 8)\n",
      "X_test_reshaped.shape:\n",
      "(102, 1, 8)\n",
      "Shape check for train and test sets post reshaping for lstm:\n",
      "(406, 1, 8) (102, 1, 8)\n",
      "(406, 1) (102, 1)\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 1, 8)]            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 1, 64)             10496     \n",
      "_________________________________________________________________\n",
      "sent_attention (AttentionLay [(None, 64), (None, 1, 1) 528       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 11,284\n",
      "Trainable params: 11,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training and test output labels(before one-hot/after enc)\n",
      "(406,) (102,)\n",
      "Training and test output labels(after one-hot)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n",
      "X_train_reshaped (406, 1, 8)\n",
      "training_op_labels_onehot (406, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 47ms/step - loss: 1.4499 - binary_accuracy: 0.7500 - categorical_accuracy: 0.1863 - val_loss: 1.3435 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.2195\n",
      "['loss', 'binary_accuracy', 'categorical_accuracy']\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4095 - binary_accuracy: 0.7500 - categorical_accuracy: 0.2353\n",
      "1.4094624519348145 0.75 0.23529411852359772\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[[ 5  3  5  0]\n",
      " [ 2  4  3  2]\n",
      " [19 14 12  5]\n",
      " [11  8  6  3]]\n",
      "Shape check post file read:\n",
      "(406, 8) (102, 8)\n",
      "X_train_reshaped.shape:\n",
      "(406, 1, 8)\n",
      "X_test_reshaped.shape:\n",
      "(102, 1, 8)\n",
      "Shape check for train and test sets post reshaping for lstm:\n",
      "(406, 1, 8) (102, 1, 8)\n",
      "(406, 1) (102, 1)\n",
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 1, 8)]            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 1, 64)             10496     \n",
      "_________________________________________________________________\n",
      "sent_attention (AttentionLay [(None, 64), (None, 1, 1) 528       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 11,284\n",
      "Trainable params: 11,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training and test output labels(before one-hot/after enc)\n",
      "(406,) (102,)\n",
      "Training and test output labels(after one-hot)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n",
      "X_train_reshaped (406, 1, 8)\n",
      "training_op_labels_onehot (406, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 48ms/step - loss: 1.3807 - binary_accuracy: 0.7452 - categorical_accuracy: 0.2493 - val_loss: 1.4242 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.2683\n",
      "['loss', 'binary_accuracy', 'categorical_accuracy']\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3626 - binary_accuracy: 0.7500 - categorical_accuracy: 0.2941\n",
      "1.3625977039337158 0.75 0.29411765933036804\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[[28 19 36  6]\n",
      " [ 4  0  6  1]\n",
      " [ 0  0  2  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import array\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=1 #300\n",
    "\n",
    "file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-attn/bilstm_meta_attn_op.txt','w') #overwrites previous\n",
    "file.close()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    df_prediction = pd.DataFrame()\n",
    "    df_accuracy =  pd.DataFrame()\n",
    "    file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-attn/bilstm_meta_attn_op.txt','a') #append mode \n",
    "    for i in range(1,3):\n",
    "        outputname = 'meta_attn_'+ str(i)        \n",
    "        predictions, acc_cat, acc_bin, conf_matrix = execute_bilstm_metadata(i)\n",
    "        df_prediction[outputname] = predictions\n",
    "        df_accuracy[i] = [acc_cat]\n",
    "        file.write(\"\\nIteration:\" + str(i) + \"\\nCategorical Accuracy:\" + str(acc_cat) + \n",
    "                   \"\\nBinary Accuracy:\" + str(acc_bin) + \"\\nConfusion Matrix:\\n\" + str(conf_matrix) + \"\\n\\n\")\n",
    "        df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-attn/predictions_bilstm_attn_meta_' + str(i) + '.csv')    \n",
    "        df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-attn/accuracy_bilstm_attn_meta_' + str(i) + '.csv') \n",
    "        \n",
    "    df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-attn/predictions_bilstm_attn_meta.csv')    \n",
    "    df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-attn/accuracy_bilstm_attn_meta.csv')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0HdjTcOXGzi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOoYx6S8XGzq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Search_Tasks_convex_model3_metadata_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35982,
     "status": "ok",
     "timestamp": 1597439698071,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "OWFwZS_hbSav",
    "outputId": "185089a1-da6b-44fc-b3e9-f2309c521bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oco7YUpKbSa2"
   },
   "source": [
    "## Generating Word-level Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2303,
     "status": "ok",
     "timestamp": 1597439714500,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "-uTAeLMybSa3"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2940,
     "status": "ok",
     "timestamp": 1597439719070,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "zguNUxaabSbC"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates word embeddings for training and test and writes the embedding matrix to file.\n",
    "\"\"\"\n",
    "def generate_word_embeddings(i):\n",
    "    print(\"Inside generate_word_embeddings\")\n",
    "    # Extracted the text for word embeddings    \n",
    "    df_X_train= pd.read_pickle('data-files\\\\X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('data-files\\\\X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('data-files\\\\Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('data-files\\\\Y_test_' + str(i) + '.pkl')\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "    \n",
    "    train = df_train['Content'].fillna(\"_na_\").values\n",
    "    test = df_test['Content'].fillna(\"_na_\").values\n",
    "    \n",
    "    \"\"\"\n",
    "    Fitting the tokenizer on training data and then using it to generate X_train and X_test\n",
    "    NEEDS TO BE RERUN if training and test are changed\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=input_length)\n",
    "    tokenizer.fit_on_texts(list(train))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test)\n",
    "    X_train_we = pad_sequences(train_tokenized, maxlen=input_length, padding='post')\n",
    "    X_test_we = pad_sequences(test_tokenized, maxlen=input_length, padding='post')\n",
    "    \n",
    "    print(\"Printing embedded X_train:\", X_train_we.shape, X_train_we)\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "    \n",
    "    all_embeddings = np.stack(embeddings_index.values())\n",
    "    embeddings_mean,embeddings_std = all_embeddings.mean(), all_embeddings.std()\n",
    "    print(embeddings_mean,embeddings_std)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    print(len(word_index), type(word_index))\n",
    "    \n",
    "    nb_words = min(vocab_size, len(word_index))\n",
    "    print(nb_words)\n",
    "    \n",
    "    embedding_matrix = np.random.normal(embeddings_mean, embeddings_std, (nb_words, embed_size))\n",
    "    #print(len(embedding_matrix), type(embedding_matrix), embedding_matrix.shape, embedding_matrix[1].dtype)\n",
    "    \n",
    "    for word, index in word_index.items(): # for all words in the dictioanry\n",
    "        if index >= nb_words: continue # if the word index is greater than the vocabulary size, ignore and fetch next word\n",
    "        embedding_vector = embeddings_index.get(word) # get the embedding vector for the word\n",
    "        if embedding_vector is not None: embedding_matrix[index] = embedding_vector # add the vector to the embedding matrix  \n",
    "    \n",
    "\n",
    "    #Writing the word embedding matrix for future use\n",
    "    f = open(\"output\\\\word-bilstm\\\\word_embedding_matrix_\"+ str(i) + \".txt\", \"bw\")\n",
    "    embedding_matrix.tofile(f)\n",
    "    \n",
    "    return train, test, X_train_we, X_test_we, nb_words, embedding_matrix, input_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZCc7IEubSbG"
   },
   "source": [
    "## Generating NLP Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1708,
     "status": "ok",
     "timestamp": 1597439722845,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "cCKKnpHIbSbH"
   },
   "outputs": [],
   "source": [
    "def encode_nlp_features(df, name, i):\n",
    "    print(\"Inside encode_nlp_features\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"ne\")), columns=mlb.classes_, index=df.index).add_prefix('ne_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"iob\")), columns=mlb.classes_, index=df.index).add_prefix('iob_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"alpha\")), columns=mlb.classes_, index=df.index).add_prefix('al_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"digit\")), columns=mlb.classes_, index=df.index).add_prefix('dig_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"punc\")), columns=mlb.classes_, index=df.index).add_prefix('punc_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"url\")), columns=mlb.classes_, index=df.index).add_prefix('url_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"oov\")), columns=mlb.classes_, index=df.index).add_prefix('oov_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"stop\")), columns=mlb.classes_, index=df.index).add_prefix('stop_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"coarsepos\")), columns=mlb.classes_, index=df.index).add_prefix('cpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"finepos\")), columns=mlb.classes_, index=df.index).add_prefix('fpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"dep\")), columns=mlb.classes_, index=df.index).add_prefix('dep_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"offset\")), columns=mlb.classes_, index=df.index).add_prefix('off_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    #print(df.columns.values.tolist())\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1597439724850,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "twXtj55PbSbR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method should generate all the natural language processing features\n",
    "\"\"\"\n",
    "def generate_nlp_features(df, i, name=\"\"):\n",
    "    print(\"Inside generate_nlp_features\")\n",
    "    sentence_ne = []\n",
    "    sentence_iob = []\n",
    "    sentence_alpha = []\n",
    "    sentence_isdigit = []\n",
    "    sentence_ispunc = []\n",
    "    sentence_isurl = []\n",
    "    sentence_isoov = []\n",
    "    sentence_isstop = []\n",
    "    sentence_coarsepos = []\n",
    "    sentence_finepos = []\n",
    "    sentence_dep = []\n",
    "    sentence_offset = []\n",
    "\n",
    "    i=0\n",
    "   \n",
    "    \"\"\"NEED TO CHANGE THE FUNCTIONAL COLUMN\"\"\"\n",
    "    for entries in df['Previous_User_Utterance']: #each entry is a sentence\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        i=i+1\n",
    "\n",
    "        doc = nlp(entries) #Spacy function to get tags\n",
    "        #print(doc)\n",
    "\n",
    "        ne = []\n",
    "        iob = []\n",
    "        alpha = []\n",
    "        isdigit = []\n",
    "        ispunc = []\n",
    "        isurl = []\n",
    "        isoov = []\n",
    "        isstop = []\n",
    "        coarsepos = []\n",
    "        finepos = []\n",
    "        dep = []\n",
    "        offset = []\n",
    "\n",
    "        \"\"\"\n",
    "        For words in the sentence\n",
    "        \"\"\"\n",
    "        for token in doc:\n",
    "            \"\"\"\n",
    "            Named Entity Type\n",
    "            \"\"\"\n",
    "            #print(\"NE Type:\", token.ent_type)\n",
    "            ne.append(token.ent_type_)\n",
    "\n",
    "            \"\"\"\n",
    "            IOB code of named entity tag. B means the token begins an entity, \n",
    "            I means it is inside an entity, O means it is outside an entity, and \n",
    "            '' means no entity tag is set.\n",
    "            \"\"\"\n",
    "            #print(\"IOB:\", token.ent_iob) \n",
    "            iob.append(token.ent_iob_)\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain alphabetic characters?\n",
    "            \"\"\"\n",
    "            #print(\"Alpha:\", token.is_alpha)\n",
    "            alpha.append(str(token.is_alpha))\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain digits?\n",
    "            \"\"\"\n",
    "            #print(\"Digits:\", token.is_digit)\n",
    "            isdigit.append(str(token.is_digit))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token punctuation?\n",
    "            \"\"\"\n",
    "            #print(\"Punc:\", token.is_punct)\n",
    "            ispunc.append(str(token.is_punct))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token like urls?\n",
    "            \"\"\"\n",
    "            #print(\"Url:\", token.like_url)\n",
    "            isurl.append(str(token.like_url))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token out-of-vocabulary?\n",
    "            \"\"\"\n",
    "            #print(\"OOV:\", token.is_oov)\n",
    "            isoov.append(str(token.is_oov))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token a stopword?\n",
    "            \"\"\"\n",
    "            #print(\"Stop:\", token.is_stop)\n",
    "            isstop.append(str(token.is_stop))\n",
    "\n",
    "            \"\"\"\n",
    "            Coarse-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Coarse POS:\", token.pos_)\n",
    "            coarsepos.append(token.pos_)\n",
    "\n",
    "            \"\"\"\n",
    "            Fine-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Fine POS:\", token.tag_)\n",
    "            finepos.append(token.tag_)\n",
    "\n",
    "            \"\"\"\n",
    "            Syntactic Dependency Relation\n",
    "            \"\"\"\n",
    "            #print(\"Dependency:\", token.dep_)\n",
    "            dep.append(token.dep_)\n",
    "\n",
    "            \"\"\"\n",
    "            Character offset\n",
    "            \"\"\"\n",
    "            #print(\"Offset:\", token.idx)\n",
    "            coff = token.idx\n",
    "            if(coff<100):\n",
    "                off = 1\n",
    "            elif(coff<200): \n",
    "                off =2\n",
    "            elif(coff<300): \n",
    "                off =3\n",
    "            elif(coff<400): \n",
    "                off =4\n",
    "            elif(coff<500): \n",
    "                off =5\n",
    "            elif(coff<600): \n",
    "                off =6\n",
    "            elif(coff<700): \n",
    "                off =7\n",
    "            elif(coff<800): \n",
    "                off =8\n",
    "            elif(coff<900): \n",
    "                off =9\n",
    "            elif(coff<1000): \n",
    "                off =10\n",
    "            else: \n",
    "                off =11\n",
    "            offset.append(off)\n",
    "            \n",
    "            #print(type(token.is_alpha))\n",
    "            #break\n",
    "\n",
    "        sentence_ne.append(ne)\n",
    "        sentence_iob.append(iob)\n",
    "        sentence_alpha.append(alpha)\n",
    "        sentence_isdigit.append(isdigit)\n",
    "        sentence_ispunc.append(ispunc)\n",
    "        sentence_isurl.append(isurl)\n",
    "        sentence_isoov.append(isoov)\n",
    "        sentence_isstop.append(isstop)\n",
    "        sentence_coarsepos.append(coarsepos)\n",
    "        sentence_finepos.append(finepos)\n",
    "        sentence_dep.append(dep)\n",
    "        sentence_offset.append(offset)\n",
    "    df[\"ne\"] = sentence_ne\n",
    "    df[\"iob\"] = sentence_iob\n",
    "    df[\"alpha\"] = sentence_alpha\n",
    "    df[\"digit\"] = sentence_isdigit\n",
    "    df[\"punc\"] = sentence_ispunc\n",
    "    df[\"url\"] = sentence_isurl\n",
    "    df[\"oov\"] = sentence_isoov\n",
    "    df[\"stop\"] = sentence_isstop\n",
    "    df[\"coarsepos\"] = sentence_coarsepos\n",
    "    df[\"finepos\"] = sentence_finepos\n",
    "    df[\"dep\"] = sentence_dep\n",
    "    df[\"offset\"] = sentence_offset\n",
    "    \n",
    "    print(len(sentence_ne) == len(sentence_iob) == len(sentence_alpha) == len(sentence_isdigit) == \\\n",
    "          len(sentence_ispunc) == len(sentence_isurl) == len(sentence_isoov) == len(sentence_isstop) \\\n",
    "          == len(sentence_coarsepos) == len(sentence_finepos) == len(sentence_dep) == len(sentence_offset))\n",
    "\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_intermediate_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1597439726506,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "An_I7cr2bSbX"
   },
   "outputs": [],
   "source": [
    "def get_nlp_encodings(i):\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    print(\"Inside get_nlp_encodings\")\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    merging the output labels to predictors\n",
    "    \"\"\"\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "    print(\"The size of training and test sets:\", df_train.shape, df_test.shape)\n",
    "    r,c = df_train.shape\n",
    "    #print(\"Rows in training data =\", r)\n",
    "    \"\"\"\n",
    "    merging training and test data to one dataframe\n",
    "    \"\"\"\n",
    "    df = pd.concat([df_train,df_test])\n",
    "    #print(\"Final df:\", df.shape)\n",
    "    #print(df.head(2))\n",
    "    \"\"\"\n",
    "    generating the nlp features using spacy\n",
    "    \"\"\"\n",
    "    df_nlp = generate_nlp_features(df, i, 'all')\n",
    "    #print(\"After-spacy:\", df_nlp.shape, df_nlp.columns.values)\n",
    "    #print(df_nlp.head(2))\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding the nlp features using multilabelbinarizer\n",
    "    \"\"\"\n",
    "    df_nlp_enc = encode_nlp_features(df_nlp, 'all_nlp_enc', i)\n",
    "    #print(df_nlp_enc.shape)\n",
    "    #print(df_nlp_enc.head(2))\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    df_nlp_enc.drop(['Utterance_Number', 'Duration', 'Speaker', 'System_Number', 'Search_Task', 'Previous_User_Utterance', 'Previous_User_Speech_Act', 'Previous_Speech_Act', 'Previous_Search_Act', 'Search_acts'], axis=1, inplace=True)\n",
    "                     #['Transcript', 'Query_counter', 'Length', 'If_Intermediary', 'Complexity', 'Speech_acts'], axis=1, inplace=True)\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    train_split = df_nlp_enc.iloc[:r,:]\n",
    "    test_split = df_nlp_enc.iloc[r:,:]\n",
    "    print(\"Size after split:\",train_split.shape, test_split.shape, df_Y_train.shape, df_Y_test.shape)\n",
    "    return train_split, test_split, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4uDcAe_bSbf"
   },
   "source": [
    "## Generating Metadata Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1597439729344,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "CRo5kUZ4bSbg"
   },
   "outputs": [],
   "source": [
    "def get_metadata_features(i):\n",
    "    print(\"Inside get_metadata_features\")\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.head())\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    X_train_meta = df_X_train.drop(['Previous_User_Utterance'], axis=1)\n",
    "    X_test_meta = df_X_test.drop(['Previous_User_Utterance'], axis=1)\n",
    "    \n",
    "    #print(X_train_meta['Previous_Speech_Act'].unique())\n",
    "    #print(X_train_meta['Previous_Search_Act'].unique())\n",
    "    \n",
    "    return X_train_meta, X_test_meta, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2890,
     "status": "ok",
     "timestamp": 1597439733195,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "QbM58jWFbSbl"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "   \n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1597439734984,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "I-UIR_4lbSbs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1597439735498,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "QB1WT26BbSbz"
   },
   "outputs": [],
   "source": [
    "def create_bilstm_nlp_meta_channels(reshaped_data_nlp, reshaped_data_meta):\n",
    "    samples,timesteps,features = reshaped_data_nlp.shape\n",
    "    inp_nlp = Input(shape=(timesteps,features), name='input_nlp')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x1 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_nlp) \n",
    "    x1, sent_coeffs1 = AttentionLayer(features,return_coefficients=True,name='sent_attention1')(x1)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x1 = Dense(4, activation=\"softmax\")(x1) #output layer  \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_meta.shape\n",
    "    inp_meta = Input(shape=(timesteps,features), name='input_meta')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x3 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_meta) \n",
    "    x3, sent_coeffs3 = AttentionLayer(features,return_coefficients=True,name='sent_attention3')(x3)\n",
    "    #x3 = GlobalMaxPool1D()(x3)\n",
    "    #x3 = Dense(100, activation=\"relu\")(x3)\n",
    "    x3 = Dropout(0.25)(x3)\n",
    "    x3 = Dense(4, activation=\"softmax\")(x3) #output layer\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Merging outputs of nlp and meta\n",
    "    \"\"\"\n",
    "    merge_nlp_meta = concatenate([x1, x3])\n",
    "    output_nlp_meta = Dense(4, activation=\"softmax\")(merge_nlp_meta)\n",
    "    model_nlp_meta_combined = Model(inputs=[inp_nlp, inp_meta], outputs=output_nlp_meta)\n",
    "    \n",
    "    return model_nlp_meta_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1683,
     "status": "ok",
     "timestamp": 1597439739884,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "LbS3j_XJbSb3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1597440007599,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "X__oozTrbScA"
   },
   "outputs": [],
   "source": [
    "def execute_bilstm_nlp_meta_channel(i):\n",
    "    \"\"\"\n",
    "    nlp data\n",
    "    \"\"\"\n",
    "    df_train_nlp_encodings, df_test_nlp_encodings, df_Y_train_nlp, df_Y_test_nlp = get_nlp_encodings(i)\n",
    "    train_nlp_enc = df_train_nlp_encodings.values\n",
    "    test_nlp_enc = df_test_nlp_encodings.values\n",
    "    \n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_nlp_encodings.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_nlp = np.concatenate([train_nlp_enc[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_nlp = X_train_reshaped_nlp.reshape(staggering, timestamp, c) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_nlp)\n",
    "    print(X_train_reshaped_nlp.shape)\n",
    "    \n",
    "    r2, c2 = df_test_nlp_encodings.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_nlp = np.concatenate([test_nlp_enc[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_nlp = X_test_reshaped_nlp.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_nlp)\n",
    "    print(X_test_reshaped_nlp.shape)\n",
    "    \n",
    "    df_Y_train_nlp = df_Y_train_nlp.iloc[timestamp-1:,]\n",
    "    df_Y_test_nlp = df_Y_test_nlp.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_nlp.shape, df_Y_test_nlp.shape)    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    meta data\n",
    "    \"\"\"\n",
    "    df_train_meta, df_test_meta, df_Y_train_meta, df_Y_test_meta = get_metadata_features(i)\n",
    "    train_meta = df_train_meta.values\n",
    "    test_meta = df_test_meta.values\n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_meta.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_meta = np.concatenate([train_meta[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_meta = X_train_reshaped_meta.reshape(staggering, timestamp, c) # c is the number of features\n",
    "   \n",
    "    r2, c2 = df_test_meta.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_meta = np.concatenate([test_meta[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_meta = X_test_reshaped_meta.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    \n",
    "    print(\"Meta data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_meta)\n",
    "    print(X_train_reshaped_meta.shape)\n",
    "    \n",
    "    print(\"Meta data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_meta)\n",
    "    print(X_test_reshaped_meta.shape)\n",
    "    \n",
    "    df_Y_train_meta = df_Y_train_meta.iloc[timestamp-1:,]\n",
    "    df_Y_test_meta = df_Y_test_meta.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_meta.shape, df_Y_test_meta.shape)\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # ----------------------------------------------------\n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "    \n",
    "    model_nlp_meta = create_bilstm_nlp_meta_channels(X_train_reshaped_nlp, X_train_reshaped_meta)\n",
    "    model_nlp_meta.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    model_nlp_meta.summary()  \n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel --starts\n",
    "    \"\"\"\n",
    "    encoding the output labels\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_Y_train_nlp)\n",
    "    training_op_labels_encoded = encoder.transform(df_Y_train_nlp)\n",
    "    test_op_labels_encoded = encoder.transform(df_Y_test_nlp)\n",
    "    print(\"Output Labels\\n-------------------\")\n",
    "    print(training_op_labels_encoded.shape, test_op_labels_encoded.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    converting the output labels to one-hot form\n",
    "    \"\"\"\n",
    "    training_op_labels_onehot= np_utils.to_categorical(training_op_labels_encoded)\n",
    "    test_op_labels_onehot = np_utils.to_categorical(test_op_labels_encoded)\n",
    "\n",
    "    print(training_op_labels_onehot.shape, len(training_op_labels_onehot))\n",
    "    print(test_op_labels_onehot.shape, len(test_op_labels_onehot))\n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel -ends\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # X_train_reshaped_nlp, X_test_reshaped_nlp\n",
    "    # X_train_we, X_test_we\n",
    "    # training_op_labels_onehot, test_op_labels_onehot\n",
    "    # ----------------------------------------------------\n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "    #change the input_\n",
    "    #train data\n",
    "    X_train_reshaped_meta = np.asarray(X_train_reshaped_meta, dtype='float32')\n",
    "    X_train_reshaped_nlp = np.asarray(X_train_reshaped_nlp, dtype='float32')\n",
    "    X_test_reshaped_meta = np.asarray(X_test_reshaped_meta, dtype='float32')\n",
    "    X_test_reshaped_nlp = np.asarray(X_test_reshaped_nlp, dtype='float32')\n",
    "\n",
    "\n",
    "\n",
    "    model_nlp_meta_train = model_nlp_meta.fit({'input_nlp': X_train_reshaped_nlp, 'input_meta':X_train_reshaped_meta}, \\\n",
    "                                  training_op_labels_onehot, \\\n",
    "                                  batch_size=BATCH_SIZE, \\\n",
    "                                  epochs=EPOCHS, \\\n",
    "                                  validation_split=0.1)\n",
    "    # load model if required \n",
    "    # compile model\n",
    "    model_nlp_meta.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    # evaluate model\n",
    "    print(model_nlp_meta.metrics_names)\n",
    "    #change the input_\n",
    "    loss, binary_accuracy, categorical_accuracy = model_nlp_meta.evaluate({'input_nlp': X_test_reshaped_nlp, 'input_meta':X_test_reshaped_meta},\\\n",
    "                                                                    y = test_op_labels_onehot,\\\n",
    "                                                                    batch_size=BATCH_SIZE,\\\n",
    "                                                                    verbose=1)\n",
    "    print(loss, binary_accuracy, categorical_accuracy)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    predict the probabilty of output classes\n",
    "    and pick the best one\n",
    "    \"\"\"\n",
    "    #change the input_\n",
    "    prediction_vector = model_nlp_meta.predict({'input_nlp': X_test_reshaped_nlp, 'input_meta':X_test_reshaped_meta}, \\\n",
    "                           batch_size=BATCH_SIZE,\\\n",
    "                           verbose=1)\n",
    "    predicted_classes = np.argmax(prediction_vector, axis=1)\n",
    "    original_classes = np.argmax(test_op_labels_onehot, axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # verification of correctness:\n",
    "    total_correct = sum(original_classes == predicted_classes)\n",
    "    print(\"Total number of correct predictions:\",total_correct)\n",
    "    print(\"Accuracy:\",total_correct/len(test_op_labels_onehot))\n",
    "    acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
    "    print('Overall accuracy: {} %'.format(acc*100))\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(predicted_classes, original_classes)\n",
    "    print(\"\\n\", conf_mat, \"\\n\")\n",
    "    \n",
    "    return predicted_classes, categorical_accuracy, binary_accuracy, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35739,
     "status": "ok",
     "timestamp": 1597440043973,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "i6ip5c5ibScF",
    "outputId": "d55e9384-ef8f-4720-9ac2-4ff05deb3dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (406, 10) (102, 10)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (406, 123) (102, 123) (406, 1) (102, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(406, 1, 123)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(102, 1, 123)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[0.0 7.0 0 ... 1 2 2]]\n",
      "\n",
      " [[0.0 43.0 0 ... 1 11 1]]\n",
      "\n",
      " [[0.0 6.0 0 ... 1 1 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0 5.0 0 ... 10 2 2]]\n",
      "\n",
      " [[0.0 41.0 0 ... 1 1 3]]\n",
      "\n",
      " [[0.0 16.0 0 ... 12 12 3]]]\n",
      "(406, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[0.0 8.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 6.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 14.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 24.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 79.0 0 1 1 9 9 2]]\n",
      "\n",
      " [[0.0 15.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 6.0 0 1 1 1 1 2]]\n",
      "\n",
      " [[0.0 9.0 0 0 0 1 1 2]]\n",
      "\n",
      " [[0.0 13.0 0 0 1 1 11 2]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 11 2 1]]\n",
      "\n",
      " [[0.0 3.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 37.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 64.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 107.0 0 0 1 1 2 4]]\n",
      "\n",
      " [[0.0 35.0 0 0 2 10 10 1]]\n",
      "\n",
      " [[0.0 4.0 0 0 1 1 2 4]]\n",
      "\n",
      " [[0.0 7.0 0 0 1 1 11 4]]\n",
      "\n",
      " [[0.0 37.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 1 1 11 1]]\n",
      "\n",
      " [[0.0 7.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 20.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 9.0 0 1 2 1 2 3]]\n",
      "\n",
      " [[0.0 19.0 0 1 2 1 2 3]]\n",
      "\n",
      " [[0.0 9.0 0 1 1 6 2 1]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 34.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 25.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 25.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 9.0 0 1 2 1 2 1]]\n",
      "\n",
      " [[0.0 11.0 0 1 1 6 2 3]]\n",
      "\n",
      " [[0.0 3.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 1 2 4]]\n",
      "\n",
      " [[0.0 21.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 50.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 64.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 3.0 0 1 1 10 10 2]]\n",
      "\n",
      " [[0.0 11.0 0 0 0 6 2 3]]\n",
      "\n",
      " [[0.0 8.0 0 1 1 1 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 6 5 3]]\n",
      "\n",
      " [[0.0 26.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 55.0 0 0 1 1 2 2]]\n",
      "\n",
      " [[0.0 26.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 21.0 0 1 1 1 2 4]]\n",
      "\n",
      " [[0.0 15.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 15.0 0 0 0 12 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 1]]\n",
      "\n",
      " [[0.0 13.0 0 1 2 1 2 1]]\n",
      "\n",
      " [[0.0 16.0 0 1 1 6 6 3]]\n",
      "\n",
      " [[0.0 28.0 0 0 2 10 2 3]]\n",
      "\n",
      " [[0.0 12.0 0 1 1 10 10 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 14.0 0 1 1 1 1 4]]\n",
      "\n",
      " [[0.0 67.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 9.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 6.0 0 0 0 10 2 3]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 0 0 1 2 4]]\n",
      "\n",
      " [[0.0 7.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 46.0 0 1 1 1 2 3]]\n",
      "\n",
      " [[0.0 10.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 1 5 2]]\n",
      "\n",
      " [[0.0 15.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 8.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 0 1 2 4]]\n",
      "\n",
      " [[0.0 4.0 0 1 2 10 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 2]]\n",
      "\n",
      " [[0.0 59.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 13.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 12.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 53.0 0 1 1 10 11 3]]\n",
      "\n",
      " [[0.0 13.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 18.0 0 1 1 10 10 2]]\n",
      "\n",
      " [[0.0 4.0 0 1 2 1 1 2]]\n",
      "\n",
      " [[0.0 6.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 16.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 16.0 0 1 1 1 5 3]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 9.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 55.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 11 2 3]]\n",
      "\n",
      " [[0.0 17.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 8.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 7.0 0 0 1 1 11 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 1 1 1 4]]\n",
      "\n",
      " [[0.0 15.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 27.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 10.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 10 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 27.0 0 1 2 1 1 3]]\n",
      "\n",
      " [[0.0 47.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 15.0 0 1 2 10 10 4]]\n",
      "\n",
      " [[0.0 31.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 57.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 16.0 0 1 2 10 11 1]]]\n",
      "(102, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 123)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 64)        39936       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  8118        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention3 (AttentionLayer [(None, 64), (None,  528         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           sent_attention3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            260         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            260         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            36          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 59,634\n",
      "Trainable params: 59,634\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(406,) (102,)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 94ms/step - loss: 1.4319 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3370 - val_loss: 1.3021 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.3902\n",
      "[]\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4048 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3627\n",
      "1.4048480987548828 0.75 0.36274510622024536\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "\n",
      " [[37 29 26 10]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (406, 10) (102, 10)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (406, 123) (102, 123) (406, 1) (102, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(406, 1, 123)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 1 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 1 ... 1 0 0]]]\n",
      "(102, 1, 123)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[0.0 31.0 0 ... 10 2 3]]\n",
      "\n",
      " [[0.0 6.0 0 ... 10 2 1]]\n",
      "\n",
      " [[0.0 34.0 0 ... 10 2 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0 6.0 0 ... 1 2 3]]\n",
      "\n",
      " [[0.0 25.0 0 ... 10 2 2]]\n",
      "\n",
      " [[0.0 88.0 0 ... 10 2 1]]]\n",
      "(406, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[0.0 27.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 3.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 18.0 0 1 2 1 2 3]]\n",
      "\n",
      " [[0.0 67.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 14.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 59.0 0 1 2 10 2 3]]\n",
      "\n",
      " [[0.0 43.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 2]]\n",
      "\n",
      " [[0.0 16.0 0 1 2 10 11 1]]\n",
      "\n",
      " [[0.0 7.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 53.0 0 0 2 1 1 3]]\n",
      "\n",
      " [[0.0 20.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 61.0 0 1 2 10 2 2]]\n",
      "\n",
      " [[0.0 27.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 12.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 40.0 0 0 2 6 11 1]]\n",
      "\n",
      " [[0.0 3.0 0 1 1 10 10 2]]\n",
      "\n",
      " [[0.0 4.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 26.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 40.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 40.0 0 0 2 6 11 3]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 12.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 6.0 0 1 2 10 11 1]]\n",
      "\n",
      " [[0.0 9.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 10 2 3]]\n",
      "\n",
      " [[0.0 57.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 9.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 24.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 3]]\n",
      "\n",
      " [[0.0 13.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 0.0 0 0 1 10 2 3]]\n",
      "\n",
      " [[0.0 21.0 0 1 1 1 2 4]]\n",
      "\n",
      " [[0.0 23.0 0 0 0 1 1 3]]\n",
      "\n",
      " [[0.0 12.0 0 0 0 1 1 3]]\n",
      "\n",
      " [[0.0 12.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 10.0 0 0 2 1 5 3]]\n",
      "\n",
      " [[0.0 6.0 0 1 2 10 2 3]]\n",
      "\n",
      " [[0.0 13.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 4.0 0 0 0 1 1 1]]\n",
      "\n",
      " [[0.0 13.0 0 0 1 1 11 1]]\n",
      "\n",
      " [[0.0 8.0 0 0 2 10 2 1]]\n",
      "\n",
      " [[0.0 43.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 27.0 0 1 1 10 2 4]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 9.0 0 1 1 1 2 3]]\n",
      "\n",
      " [[0.0 15.0 0 1 1 10 4 2]]\n",
      "\n",
      " [[0.0 7.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 19.0 0 1 2 6 4 3]]\n",
      "\n",
      " [[0.0 27.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 6.0 0 0 2 6 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 65.0 0 1 2 10 2 2]]\n",
      "\n",
      " [[0.0 7.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 27.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 12.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 10.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 15.0 0 1 1 10 2 4]]\n",
      "\n",
      " [[0.0 11.0 0 0 2 6 6 2]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 6.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 21.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 21.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 7.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 23.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 6.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 45.0 0 0 1 1 2 4]]\n",
      "\n",
      " [[0.0 7.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 4.0 0 1 2 1 1 2]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 9.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 35.0 0 0 2 10 10 1]]\n",
      "\n",
      " [[0.0 19.0 0 1 1 1 4 1]]\n",
      "\n",
      " [[0.0 7.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 28.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 0 0 1 1 3]]\n",
      "\n",
      " [[0.0 17.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 2 12 12 3]]\n",
      "\n",
      " [[0.0 35.0 0 0 2 10 10 3]]\n",
      "\n",
      " [[0.0 4.0 0 0 0 10 10 4]]\n",
      "\n",
      " [[0.0 15.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 14.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 43.0 0 1 2 1 5 3]]\n",
      "\n",
      " [[0.0 19.0 0 1 1 1 4 1]]\n",
      "\n",
      " [[0.0 9.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 25.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 15.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 25.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 18.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 30.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 20.0 0 0 1 1 11 2]]\n",
      "\n",
      " [[0.0 15.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 43.0 0 1 2 1 5 1]]\n",
      "\n",
      " [[0.0 8.0 0 1 2 10 5 2]]\n",
      "\n",
      " [[0.0 34.0 0 0 0 1 2 3]]]\n",
      "(102, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 123)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 64)        39936       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  8118        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention3 (AttentionLayer [(None, 64), (None,  528         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           sent_attention3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            260         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            260         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8)            0           dense_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            36          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 59,634\n",
      "Trainable params: 59,634\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(406,) (102,)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 96ms/step - loss: 1.4655 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3699 - val_loss: 1.4593 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.3171\n",
      "[]\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5020 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3137\n",
      "1.5020395517349243 0.75 0.3137255012989044\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "\n",
      " [[32 19 44  7]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "\"\"\"\n",
    "Required for NLP model\n",
    "\"\"\"\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()   \n",
    "\n",
    "# set parameters for word embeddings\n",
    "embed_size = 100 # how big is each word vector\n",
    "vocab_size = 25000 # how many unique words to use (i.e num rows in embedding vector) max\n",
    "input_length = 100 # max number of words in the input \n",
    "\n",
    "#set parameters for bilstm\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=1 #300\n",
    "\n",
    "EMBEDDING_FILE='glove.6B.100d.txt'    \n",
    "\n",
    "file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-attn/bilstm_nlp_meta_attn_op.txt','w') #overwrites previous\n",
    "file.close()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    df_prediction = pd.DataFrame()\n",
    "    df_accuracy =  pd.DataFrame()\n",
    "    file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-attn/bilstm_nlp_meta_attn_op.txt','a') #append mode \n",
    "    \"\"\"\n",
    "    Change the range before executing\n",
    "    \"\"\"\n",
    "    for i in range(1,3):\n",
    "        outputname = 'nlp_meta_attn'+ str(i)        \n",
    "        predictions, acc_cat, acc_bin, conf_matrix = execute_bilstm_nlp_meta_channel(i)\n",
    "        df_prediction[outputname] = predictions\n",
    "        df_accuracy[i] = [acc_cat]\n",
    "        file.write(\"\\nIteration:\" + str(i) + \"\\nCategorical Accuracy:\" + str(acc_cat) + \n",
    "                   \"\\nBinary Accuracy:\" + str(acc_bin) + \"\\nConfusion Matrix:\\n\" + str(conf_matrix) + \"\\n\\n\")\n",
    "        df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-attn/predictions_bilstm_nlp_meta_attn_' + str(i) + '.csv')    \n",
    "        df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-attn/accuracy_bilstm_nlp_meta_attn_' + str(i) + '.csv')    \n",
    "    \n",
    "    df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-attn/predictions_bilstm_nlp_meta_attn.csv')    \n",
    "    df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-attn/accuracy_bilstm_nlp_meta_attn.csv')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uc3hwQsdbScN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Search_Tasks_convex_model6_nlp_meta_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

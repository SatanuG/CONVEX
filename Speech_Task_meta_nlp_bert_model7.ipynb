{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 84647,
     "status": "ok",
     "timestamp": 1596929832510,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "KQecsX9Tuw68",
    "outputId": "a47eb157-4d6e-4981-c026-442a8bbc3032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9134,
     "status": "ok",
     "timestamp": 1596929841521,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "F1xqAIbVuxqg",
    "outputId": "a0c21890-ba07-4425-a54d-3cb421d44b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 4.3MB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 25.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 32.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 43.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4fec8b499747590cac24e7949fe2f42da45dfcef95df6936e8190189f399b55f\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13833,
     "status": "ok",
     "timestamp": 1596929847042,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "TSu-QYYnuzh0"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13253,
     "status": "ok",
     "timestamp": 1596929847046,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "D_ckyu5su1XI",
    "outputId": "d229e69f-d473-49a2-d2bf-1a9bb33ab79a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12651,
     "status": "ok",
     "timestamp": 1596929847047,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "GSoAhawbu3GY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "a816e6080f1b40eea121db12e9b2253d",
      "5bf5ba6b1a414bffaae298093772f87b",
      "f4e947288fbc4ceb8ab00973d24c3841",
      "992db696177f454a92505c6062d1bbb0",
      "9d453881557f42dd811a89e42a050e4e",
      "2cd7bfda228d4f57a13fff3b2b50c001",
      "e88eee6091464be7aef4b793e5a045a4",
      "d97e84f51dd34c419d9a0914ddbe7431",
      "846e12525dab4a86ae55975afd71dee9",
      "f15f16fc189a45ea940b19baec791109",
      "f6350fc2017b47378ee22aa64a6dca5b",
      "4eed3deffbc2402d9d53078dd259494c",
      "39f88875ada640af98ae75412365082c",
      "ed6ade70ab52453a92a81c35c8e1dc47",
      "4778e0cef55e485d9295b9a776452004",
      "815880f21d4440ebb862a4617d84253d"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44203,
     "status": "ok",
     "timestamp": 1596929879148,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qUyUffSWu4lo",
    "outputId": "e3a6b264-a4ac-4a19-d490-437df9f141fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a816e6080f1b40eea121db12e9b2253d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846e12525dab4a86ae55975afd71dee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'distilbert', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp_bert = transformers.TFBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "ec0291ee9087408a93eb7abcfc82073c",
      "ef7da9e0f1fc4ac7b1bb14a29ad5f1e6",
      "5516c7c0e99446e7af244f6308e5bbd2",
      "6074a61e1c964963b945516602abeb2d",
      "0e678526290a490c9dd01180acf7f38a",
      "0fbaac0e4f7244778feb8ccd14a1e60c",
      "dd467ead277543c4a2a2d2f9573e2f9a",
      "f2f31ee8108948cfae2663d0a7e65730"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44908,
     "status": "ok",
     "timestamp": 1596929880379,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qMz_8LGFu7kZ",
    "outputId": "27c5d651-c78f-4db0-a762-5f5753fa07c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0291ee9087408a93eb7abcfc82073c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JFvoYplrH52"
   },
   "source": [
    "# Generating META features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41794,
     "status": "ok",
     "timestamp": 1596929880381,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "8kqqqQpwrHiY"
   },
   "outputs": [],
   "source": [
    "def get_metadata_features(i):\n",
    "    print(\"Inside get_metadata_features\")\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_train_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.head())\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    X_train_meta = df_X_train.drop(['Content', 'Previous_User_Utterance'], axis=1)\n",
    "    X_test_meta = df_X_test.drop(['Content', 'Previous_User_Utterance'], axis=1)\n",
    "    \n",
    "    #print(X_train_meta['Previous_Speech_Act'].unique())\n",
    "    #print(X_train_meta['Previous_Search_Act'].unique())\n",
    "    \n",
    "    return X_train_meta, X_test_meta, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzV6yCpJxh1D"
   },
   "source": [
    "# Generating BERT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40645,
     "status": "ok",
     "timestamp": 1596929880382,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "kw7Kl1sXu9_L"
   },
   "outputs": [],
   "source": [
    "def load_files(i):\n",
    "\n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_test_' + str(i) + '.pkl')\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "\n",
    "    #print(list(df_train.columns))\n",
    "\n",
    "    df_train = df_train[[\"Speech_acts\",\"Content\"]]\n",
    "    df_train = df_train.rename(columns={\"Speech_acts\":\"y\", \"Content\":\"text\"})\n",
    "\n",
    "    \n",
    "\n",
    "    df_test = df_test[[\"Speech_acts\",\"Content\"]]\n",
    "    df_test = df_test.rename(columns={\"Speech_acts\":\"y\", \"Content\":\"text\"})\n",
    "\n",
    "    return df_train,df_test,df_Y_train,df_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40154,
     "status": "ok",
     "timestamp": 1596929880383,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "GPJkdcfJxSyn"
   },
   "outputs": [],
   "source": [
    "def generate_maskid(training, test):\n",
    "  #print(training['text'])\n",
    "  corpus_train = training['text']\n",
    "  corpus_test = test['text']\n",
    "  #the length of the feature vector is 150\n",
    "  maxlen = 150\n",
    "\n",
    "  #add special tokens\n",
    "  maxqnans = np.int((maxlen-20)/2)\n",
    "  corpus_tokenized_train = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_train]\n",
    "  corpus_tokenized_test = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_test]\n",
    "  #generate masks\n",
    "  masks_train = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_train]\n",
    "  masks_test = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_test]\n",
    "\n",
    "  #padding\n",
    "  txt2seq_train = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_train]\n",
    "  txt2seq_test = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_test]\n",
    "\n",
    "  #generate idx\n",
    "  idx_train = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_train]\n",
    "  idx_test = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_test]      \n",
    "\n",
    "  ## feature matrix\n",
    "  X_train = [np.asarray(idx_train, dtype='int32'), \n",
    "            np.asarray(masks_train, dtype='int32')]\n",
    "  X_test = [np.asarray(idx_test, dtype='int32'), \n",
    "            np.asarray(masks_test, dtype='int32')]\n",
    "  \n",
    "\n",
    "  idx_train = np.asarray(idx_train, dtype='int32')\n",
    "  masks_train = np.asarray(masks_train, dtype='int32')\n",
    "  idx_test = np.asarray(idx_test, dtype='int32')\n",
    "  masks_test = np.asarray(masks_test, dtype='int32')\n",
    "\n",
    "  #print(\"txt: \", training[\"text\"].iloc[0])\n",
    "  #print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "  #print(\"idx: \", X_train[0][i])\n",
    "  #print(\"mask: \", X_train[1][i])\n",
    "  return idx_train, masks_train, idx_test, masks_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39540,
     "status": "ok",
     "timestamp": 1596929880385,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Ss99j1L5xWqQ"
   },
   "outputs": [],
   "source": [
    "def model_generate(y_train):\n",
    "\n",
    "  ## inputs\n",
    "  idx = layers.Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "  masks = layers.Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "  ## pre-trained bert with config\n",
    "  config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "  config.output_hidden_states = False\n",
    "  nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "  bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "  ## fine-tuning\n",
    "  x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)\n",
    "  #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "  y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "  #nlp2 = transformers.TFDistilBertModel.from_pretrained('distilroberta-base', config=config)\n",
    "  #bert_out2 = nlp2(idx, attention_mask=masks)[0]\n",
    "  #x2 = layers.GlobalAveragePooling1D()(bert_out2)\n",
    "  #x2 = layers.Dense(32, activation=\"relu\")(x2)\n",
    "  #y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x2)\n",
    "\n",
    "\n",
    "\n",
    "  ## compile\n",
    "  model = models.Model([idx, masks], y_out)\n",
    "  for layer in model.layers[:3]:\n",
    "      layer.trainable = False\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38985,
     "status": "ok",
     "timestamp": 1596929880386,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ovBhxTYzxbW1"
   },
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, model):\n",
    "\n",
    "  ## encode y\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  print(dic_y_mapping)\n",
    "  inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "  print(inverse_dic)\n",
    "  y_train2 = np.array([inverse_dic[y] for y in y_train])\n",
    "  ## train\n",
    "  training = model.fit(x=X_train, y=y_train2, batch_size=32, epochs=300, shuffle=True, verbose=1, validation_split=0.3)\n",
    "\n",
    "  training_acc = training.history['accuracy']\n",
    "  training_loss = training.history['loss']\n",
    "  #print(acc)\n",
    "  #loss, accuracy = model.evaluate(x=X_test, y=y_test, batch_size=32, verbose=1)\n",
    "  return training_acc, training_loss\n",
    "  #loss, accuracy = model.evaluate(x=X_train, y=y_train, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30832,
     "status": "ok",
     "timestamp": 1596929880387,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "OuVvBsLUxfeC"
   },
   "outputs": [],
   "source": [
    "def test_classification(X_test, y_test, y_train):\n",
    "\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  #print(np.unique(y_test))\n",
    "  predicted_prob = model.predict(X_test)\n",
    "  #print(predicted_prob)\n",
    "  predicted = [dic_y_mapping[np.argmax(pred)] for pred in predicted_prob]\n",
    "\n",
    "  return predicted, predicted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qfp6B9MKyl4s"
   },
   "source": [
    "# Generating NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29152,
     "status": "ok",
     "timestamp": 1596929880388,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "T_PvYkfHyTeV"
   },
   "outputs": [],
   "source": [
    "def encode_nlp_features(df, name, i):\n",
    "    print(\"Inside encode_nlp_features\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"ne\")), columns=mlb.classes_, index=df.index).add_prefix('ne_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"iob\")), columns=mlb.classes_, index=df.index).add_prefix('iob_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"alpha\")), columns=mlb.classes_, index=df.index).add_prefix('al_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"digit\")), columns=mlb.classes_, index=df.index).add_prefix('dig_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"punc\")), columns=mlb.classes_, index=df.index).add_prefix('punc_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"url\")), columns=mlb.classes_, index=df.index).add_prefix('url_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"oov\")), columns=mlb.classes_, index=df.index).add_prefix('oov_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"stop\")), columns=mlb.classes_, index=df.index).add_prefix('stop_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"coarsepos\")), columns=mlb.classes_, index=df.index).add_prefix('cpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"finepos\")), columns=mlb.classes_, index=df.index).add_prefix('fpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"dep\")), columns=mlb.classes_, index=df.index).add_prefix('dep_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"offset\")), columns=mlb.classes_, index=df.index).add_prefix('off_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    #print(df.columns.values.tolist())\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29026,
     "status": "ok",
     "timestamp": 1596929880871,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Lc5ouptRyX0P"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method should generate all the natural language processing features\n",
    "\"\"\"\n",
    "def generate_nlp_features(df, i, name=\"\"):\n",
    "    print(\"Inside generate_nlp_features\")\n",
    "    sentence_ne = []\n",
    "    sentence_iob = []\n",
    "    sentence_alpha = []\n",
    "    sentence_isdigit = []\n",
    "    sentence_ispunc = []\n",
    "    sentence_isurl = []\n",
    "    sentence_isoov = []\n",
    "    sentence_isstop = []\n",
    "    sentence_coarsepos = []\n",
    "    sentence_finepos = []\n",
    "    sentence_dep = []\n",
    "    sentence_offset = []\n",
    "\n",
    "    i=0\n",
    "   \n",
    "    \"\"\"NEED TO CHANGE THE FUNCTIONAL COLUMN\"\"\"\n",
    "    for entries in df[\"Content\"]: #each entry is a sentence\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        i=i+1\n",
    "\n",
    "        doc = nlp(entries) #Spacy function to get tags\n",
    "        #print(doc)\n",
    "\n",
    "        ne = []\n",
    "        iob = []\n",
    "        alpha = []\n",
    "        isdigit = []\n",
    "        ispunc = []\n",
    "        isurl = []\n",
    "        isoov = []\n",
    "        isstop = []\n",
    "        coarsepos = []\n",
    "        finepos = []\n",
    "        dep = []\n",
    "        offset = []\n",
    "\n",
    "        \"\"\"\n",
    "        For words in the sentence\n",
    "        \"\"\"\n",
    "        for token in doc:\n",
    "            \"\"\"\n",
    "            Named Entity Type\n",
    "            \"\"\"\n",
    "            #print(\"NE Type:\", token.ent_type)\n",
    "            ne.append(token.ent_type_)\n",
    "\n",
    "            \"\"\"\n",
    "            IOB code of named entity tag. B means the token begins an entity, \n",
    "            I means it is inside an entity, O means it is outside an entity, and \n",
    "            '' means no entity tag is set.\n",
    "            \"\"\"\n",
    "            #print(\"IOB:\", token.ent_iob) \n",
    "            iob.append(token.ent_iob_)\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain alphabetic characters?\n",
    "            \"\"\"\n",
    "            #print(\"Alpha:\", token.is_alpha)\n",
    "            alpha.append(str(token.is_alpha))\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain digits?\n",
    "            \"\"\"\n",
    "            #print(\"Digits:\", token.is_digit)\n",
    "            isdigit.append(str(token.is_digit))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token punctuation?\n",
    "            \"\"\"\n",
    "            #print(\"Punc:\", token.is_punct)\n",
    "            ispunc.append(str(token.is_punct))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token like urls?\n",
    "            \"\"\"\n",
    "            #print(\"Url:\", token.like_url)\n",
    "            isurl.append(str(token.like_url))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token out-of-vocabulary?\n",
    "            \"\"\"\n",
    "            #print(\"OOV:\", token.is_oov)\n",
    "            isoov.append(str(token.is_oov))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token a stopword?\n",
    "            \"\"\"\n",
    "            #print(\"Stop:\", token.is_stop)\n",
    "            isstop.append(str(token.is_stop))\n",
    "\n",
    "            \"\"\"\n",
    "            Coarse-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Coarse POS:\", token.pos_)\n",
    "            coarsepos.append(token.pos_)\n",
    "\n",
    "            \"\"\"\n",
    "            Fine-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Fine POS:\", token.tag_)\n",
    "            finepos.append(token.tag_)\n",
    "\n",
    "            \"\"\"\n",
    "            Syntactic Dependency Relation\n",
    "            \"\"\"\n",
    "            #print(\"Dependency:\", token.dep_)\n",
    "            dep.append(token.dep_)\n",
    "\n",
    "            \"\"\"\n",
    "            Character offset\n",
    "            \"\"\"\n",
    "            #print(\"Offset:\", token.idx)\n",
    "            coff = token.idx\n",
    "            if(coff<100):\n",
    "                off = 1\n",
    "            elif(coff<200): \n",
    "                off =2\n",
    "            elif(coff<300): \n",
    "                off =3\n",
    "            elif(coff<400): \n",
    "                off =4\n",
    "            elif(coff<500): \n",
    "                off =5\n",
    "            elif(coff<600): \n",
    "                off =6\n",
    "            elif(coff<700): \n",
    "                off =7\n",
    "            elif(coff<800): \n",
    "                off =8\n",
    "            elif(coff<900): \n",
    "                off =9\n",
    "            elif(coff<1000): \n",
    "                off =10\n",
    "            else: \n",
    "                off =11\n",
    "            offset.append(off)\n",
    "            \n",
    "            #print(type(token.is_alpha))\n",
    "            #break\n",
    "\n",
    "        sentence_ne.append(ne)\n",
    "        sentence_iob.append(iob)\n",
    "        sentence_alpha.append(alpha)\n",
    "        sentence_isdigit.append(isdigit)\n",
    "        sentence_ispunc.append(ispunc)\n",
    "        sentence_isurl.append(isurl)\n",
    "        sentence_isoov.append(isoov)\n",
    "        sentence_isstop.append(isstop)\n",
    "        sentence_coarsepos.append(coarsepos)\n",
    "        sentence_finepos.append(finepos)\n",
    "        sentence_dep.append(dep)\n",
    "        sentence_offset.append(offset)\n",
    "    df[\"ne\"] = sentence_ne\n",
    "    df[\"iob\"] = sentence_iob\n",
    "    df[\"alpha\"] = sentence_alpha\n",
    "    df[\"digit\"] = sentence_isdigit\n",
    "    df[\"punc\"] = sentence_ispunc\n",
    "    df[\"url\"] = sentence_isurl\n",
    "    df[\"oov\"] = sentence_isoov\n",
    "    df[\"stop\"] = sentence_isstop\n",
    "    df[\"coarsepos\"] = sentence_coarsepos\n",
    "    df[\"finepos\"] = sentence_finepos\n",
    "    df[\"dep\"] = sentence_dep\n",
    "    df[\"offset\"] = sentence_offset\n",
    "    \n",
    "    print(len(sentence_ne) == len(sentence_iob) == len(sentence_alpha) == len(sentence_isdigit) == \\\n",
    "          len(sentence_ispunc) == len(sentence_isurl) == len(sentence_isoov) == len(sentence_isstop) \\\n",
    "          == len(sentence_coarsepos) == len(sentence_finepos) == len(sentence_dep) == len(sentence_offset))\n",
    "\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_intermediate_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28514,
     "status": "ok",
     "timestamp": 1596929880873,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "UAaBZiwrycQS"
   },
   "outputs": [],
   "source": [
    "def get_nlp_encodings(i):\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    print(\"Inside get_nlp_encodings\")\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    merging the output labels to predictors\n",
    "    \"\"\"\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "    print(\"The size of training and test sets:\", df_train.shape, df_test.shape)\n",
    "    r,c = df_train.shape\n",
    "    #print(\"Rows in training data =\", r)\n",
    "    \"\"\"\n",
    "    merging training and test data to one dataframe\n",
    "    \"\"\"\n",
    "    df = pd.concat([df_train,df_test])\n",
    "    #print(\"Final df:\", df.shape)\n",
    "    #print(df.head(2))\n",
    "    \"\"\"\n",
    "    generating the nlp features using spacy\n",
    "    \"\"\"\n",
    "    df_nlp = generate_nlp_features(df, i, 'all')\n",
    "    #print(\"After-spacy:\", df_nlp.shape, df_nlp.columns.values)\n",
    "    #print(df_nlp.head(2))\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding the nlp features using multilabelbinarizer\n",
    "    \"\"\"\n",
    "    df_nlp_enc = encode_nlp_features(df_nlp, 'all_nlp_enc', i)\n",
    "    #print(df_nlp_enc.shape)\n",
    "    #print(df_nlp_enc.head(2))\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    df_nlp_enc.drop(['Content', 'Utterance_Number', 'Duration', 'Speaker', 'System_Number', 'Search_Task', 'Previous_User_Utterance', 'Previous_User_Speech_Act', 'Previous_Speech_Act', 'Previous_Search_Act', 'Speech_acts'], axis=1, inplace=True)\n",
    "                     #['Transcript', 'Query_counter', 'Length', 'If_Intermediary', 'Complexity', 'Speech_acts'], axis=1, inplace=True)\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    train_split = df_nlp_enc.iloc[:r,:]\n",
    "    test_split = df_nlp_enc.iloc[r:,:]\n",
    "    print(\"Size after split:\",train_split.shape, test_split.shape, df_Y_train.shape, df_Y_test.shape)\n",
    "    return train_split, test_split, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F108rkzjys3d"
   },
   "source": [
    "# The Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32153,
     "status": "ok",
     "timestamp": 1596929885773,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "4EnT574ZB5Yl",
    "outputId": "36b2a9d9-99ec-48dc-9773-c8ef68b8f75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Layer\n",
      "  Downloading https://files.pythonhosted.org/packages/43/9c/bedf88724d34e8f5caa4ce0555d22fc846a2cf17409653b777ad474b2605/layer-0.1.14-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from Layer) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.30.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.6.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.18.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.12.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.3.3)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.9.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.3.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (49.2.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.17.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Installing collected packages: Layer\n",
      "Successfully installed Layer-0.1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31616,
     "status": "ok",
     "timestamp": 1596929885775,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "cjblrAR8yiVZ"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self,attention_dim=1000000000000000,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CG-0wBxOqeGb"
   },
   "source": [
    "# DNN model creation and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30278,
     "status": "ok",
     "timestamp": 1596929885780,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "W_qwgVVuywu3"
   },
   "outputs": [],
   "source": [
    "def create_bilstm_meta_nlp_bert_channels(reshaped_data_nlp, reshaped_data_meta):\n",
    "    \n",
    "    idx = Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "    masks = Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "    ## pre-trained bert with config\n",
    "    config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "    config.output_hidden_states = False\n",
    "    nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "    bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "    ## fine-tuning\n",
    "    x = GlobalAveragePooling1D()(bert_out)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    y_out = Dense(12, activation='softmax')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_nlp.shape\n",
    "    inp_nlp = Input(shape=(timesteps,features), name='input_nlp')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x1 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_nlp) \n",
    "    x1, sent_coeffs1 = AttentionLayer(features,return_coefficients=True,name='sent_attention1')(x1)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x1 = Dense(12, activation=\"softmax\")(x1) #output layer  \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_meta.shape\n",
    "    inp_meta = Input(shape=(timesteps,features), name='input_meta')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x2 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_meta) \n",
    "    x2, sent_coeffs2 = AttentionLayer(features,return_coefficients=True,name='sent_attention2')(x2)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x2 = Dropout(0.25)(x2)\n",
    "    x2 = Dense(12, activation=\"softmax\")(x2) #output layer  \n",
    "      \n",
    "    \n",
    "    \"\"\"\n",
    "    Merging outputs of nlp and meta\n",
    "    \"\"\"\n",
    "    merge_nlp_bert = concatenate([x1,x2,y_out])\n",
    "\n",
    "\n",
    "    output_meta_nlp_bert = Dense(12, activation=\"softmax\")(merge_nlp_bert)\n",
    "    model_meta_nlp_bert_combined = Model(inputs=[inp_nlp, inp_meta, idx, masks], outputs=output_meta_nlp_bert)\n",
    "    \n",
    "    return model_meta_nlp_bert_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1261,
     "status": "ok",
     "timestamp": 1596930054386,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Sfq9h5Pvzq5b"
   },
   "outputs": [],
   "source": [
    "def execute_bilstm_meta_nlp_bert_channel(i):\n",
    "    \"\"\"\n",
    "    nlp data\n",
    "    \"\"\"\n",
    "    df_train_nlp_encodings, df_test_nlp_encodings, df_Y_train_nlp, df_Y_test_nlp = get_nlp_encodings(i)\n",
    "    train_nlp_enc = df_train_nlp_encodings.values\n",
    "    test_nlp_enc = df_test_nlp_encodings.values\n",
    "    \n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_nlp_encodings.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_nlp = np.concatenate([train_nlp_enc[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_nlp = X_train_reshaped_nlp.reshape(staggering, timestamp, c) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_nlp)\n",
    "    print(X_train_reshaped_nlp.shape)\n",
    "    \n",
    "    r2, c2 = df_test_nlp_encodings.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_nlp = np.concatenate([test_nlp_enc[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_nlp = X_test_reshaped_nlp.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_nlp)\n",
    "    print(X_test_reshaped_nlp.shape)\n",
    "    \n",
    "    df_Y_train_nlp = df_Y_train_nlp.iloc[timestamp-1:,]\n",
    "    df_Y_test_nlp = df_Y_test_nlp.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_nlp.shape, df_Y_test_nlp.shape)    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    meta data\n",
    "    \"\"\"\n",
    "    df_train_meta, df_test_meta, df_Y_train_meta, df_Y_test_meta = get_metadata_features(i)\n",
    "    train_meta = df_train_meta.values\n",
    "    test_meta = df_test_meta.values\n",
    "        \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "        \n",
    "    r, c = df_train_meta.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_meta = np.concatenate([train_meta[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_meta = X_train_reshaped_meta.reshape(staggering, timestamp, c) # c is the number of features\n",
    "      \n",
    "    r2, c2 = df_test_meta.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_meta = np.concatenate([test_meta[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_meta = X_test_reshaped_meta.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "        \n",
    "    print(\"Meta data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_meta)\n",
    "    print(X_train_reshaped_meta.shape)\n",
    "        \n",
    "    print(\"Meta data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_meta)\n",
    "    print(X_test_reshaped_meta.shape)\n",
    "        \n",
    "    df_Y_train_meta = df_Y_train_meta.iloc[timestamp-1:,]\n",
    "    df_Y_test_meta = df_Y_test_meta.iloc[timestamp-1:,]\n",
    "    \n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_meta.shape, df_Y_test_meta.shape)\n",
    "\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    bert data\n",
    "    \"\"\"\n",
    "\n",
    "    df_train_bert,df_test_bert,df_Y_train_bert,df_Y_test_bert = load_files(i)\n",
    "    y_train_bert = df_Y_train_bert['Speech_acts'].to_list()\n",
    "    y_test_bert = df_Y_test_bert['Speech_acts'].to_list()\n",
    "    X_train_id, X_train_mask, X_test_id, X_test_mask = generate_maskid(df_train_bert,df_test_bert)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "    \n",
    "    model_meta_nlp_bert = create_bilstm_meta_nlp_bert_channels(X_train_reshaped_nlp,X_train_reshaped_meta)\n",
    "\n",
    "\n",
    "    print(\"\\n\\n THESE ARE THE LAYERS\")\n",
    "    for layer in model_meta_nlp_bert.layers:\n",
    "      if(layer.name == 'input_idx' or layer.name == 'input_masks' or layer.name == 'tf_distil_bert_model'):\n",
    "        layer.trainable = False\n",
    "      \n",
    "    model_meta_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    model_meta_nlp_bert.summary()  \n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel --starts\n",
    "    \"\"\"\n",
    "    encoding the output labels\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_Y_train_nlp)\n",
    "    training_op_labels_encoded = encoder.transform(df_Y_train_nlp)\n",
    "    test_op_labels_encoded = encoder.transform(df_Y_test_nlp)\n",
    "    print(\"Output Labels\\n-------------------\")\n",
    "    print(training_op_labels_encoded.shape, test_op_labels_encoded.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    converting the output labels to one-hot form\n",
    "    \"\"\"\n",
    "    training_op_labels_onehot= np_utils.to_categorical(training_op_labels_encoded)\n",
    "    test_op_labels_onehot = np_utils.to_categorical(test_op_labels_encoded)\n",
    "\n",
    "    print(training_op_labels_onehot.shape, len(training_op_labels_onehot))\n",
    "    print(test_op_labels_onehot.shape, len(test_op_labels_onehot))\n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel -ends\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # X_train_reshaped_nlp, X_test_reshaped_nlp\n",
    "    # X_train_we, X_test_we\n",
    "    # training_op_labels_onehot, test_op_labels_onehot\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #change the input_\n",
    "    #train data\n",
    "   \n",
    "    X_train_reshaped_meta = np.asarray(X_train_reshaped_meta, dtype='float32')\n",
    "    model_meta_nlp_bert_train = model_meta_nlp_bert.fit(x=[X_train_reshaped_nlp,X_train_reshaped_meta, X_train_id,X_train_mask], y=training_op_labels_onehot,batch_size=BATCH_SIZE, epochs=EPOCHS,  validation_split=0.1)\n",
    "    # load model if required \n",
    "    # compile model\n",
    "\n",
    "\n",
    "    model_meta_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    # evaluate model\n",
    "    print(model_meta_nlp_bert.metrics_names)\n",
    "    #change the input_\n",
    "\n",
    "    #loss, binary_accuracy, categorical_accuracy = model_nlp_bert.evaluate(x=[X_test_reshaped_nlp,X_test_id,X_test_mask], y=test_op_labels_onehot,batch_size=BATCH_SIZE,verbose=1)\n",
    "    \n",
    "    #print(loss, binary_accuracy, categorical_accuracy)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    predict the probabilty of output classes\n",
    "    and pick the best one\n",
    "    \"\"\"\n",
    "    #change the input_\n",
    "    X_test_reshaped_meta = np.asarray(X_test_reshaped_meta, dtype='float32')\n",
    "    prediction_vector = model_meta_nlp_bert.predict(x=[X_test_reshaped_nlp,X_test_reshaped_meta, X_test_id,X_test_mask], \\\n",
    "                           batch_size=BATCH_SIZE,\\\n",
    "                           verbose=1)\n",
    "    predicted_classes = np.argmax(prediction_vector, axis=1)\n",
    "    original_classes = np.argmax(test_op_labels_onehot, axis=1)\n",
    "    accuracy = metrics.accuracy_score(original_classes, predicted_classes)\n",
    "\n",
    "    print(\"ACCURACY:\")\n",
    "    print(accuracy)\n",
    "    \"\"\"\n",
    "    # verification of correctness:\n",
    "    total_correct = sum(original_classes == predicted_classes)\n",
    "    print(\"Total number of correct predictions:\",total_correct)\n",
    "    print(\"Accuracy:\",total_correct/len(test_op_labels_onehot))\n",
    "    acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
    "    print('Overall accuracy: {} %'.format(acc*100))\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(predicted_classes, original_classes)\n",
    "    print(\"\\n\", conf_mat, \"\\n\")\n",
    "    \n",
    "    return predicted_classes, accuracy, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOtihh1fqyiS"
   },
   "source": [
    "# Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 620228,
     "status": "ok",
     "timestamp": 1596930675852,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ZnlXZmJJ1LFf",
    "outputId": "25d4308e-9bb7-473a-e1ec-844b47501295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[67.0 11.0 0 ... 1 1 3]]\n",
      "\n",
      " [[19.0 13.0 0 ... 6 4 3]]\n",
      "\n",
      " [[13.0 67.0 0 ... 1 1 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[65.0 15.0 0 ... 10 4 3]]\n",
      "\n",
      " [[29.0 5.0 0 ... 1 1 2]]\n",
      "\n",
      " [[0.0 3.0 1 ... 7 7 4]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[17.0 19.0 0 ... 6 2 3]]\n",
      "\n",
      " [[14.0 12.0 1 ... 9 9 4]]\n",
      "\n",
      " [[20.0 11.0 1 ... 10 4 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.0 31.0 0 ... 1 1 4]]\n",
      "\n",
      " [[15.0 2.0 1 ... 1 4 2]]\n",
      "\n",
      " [[10.0 7.0 0 ... 1 1 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf_distil_bert_model_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           49216       global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           780         dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 12)           780         dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 12)           780         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 36)           0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 12)           444         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 23s 541ms/step - loss: 2.3842 - binary_accuracy: 0.9167 - categorical_accuracy: 0.2333 - val_loss: 2.2602 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.3401\n",
      "[]\n",
      "12/12 [==============================] - 2s 143ms/step\n",
      "ACCURACY:\n",
      "0.3024523160762943\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0 46  0  0  2  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [67 25  4 58  7  1 65 28 10  3  2 47]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[11.0 8.0 1 ... 6 9 2]]\n",
      "\n",
      " [[10.0 11.0 1 ... 10 4 3]]\n",
      "\n",
      " [[2.0 11.0 0 ... 12 12 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[8.0 5.0 1 ... 6 5 1]]\n",
      "\n",
      " [[9.0 5.0 1 ... 9 9 3]]\n",
      "\n",
      " [[35.0 8.0 0 ... 1 1 3]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[33.0 5.0 1 ... 1 1 2]]\n",
      "\n",
      " [[12.0 10.0 0 ... 1 1 3]]\n",
      "\n",
      " [[16.0 18.0 0 ... 10 2 2]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.0 6.0 1 ... 1 1 4]]\n",
      "\n",
      " [[13.0 14.0 1 ... 9 9 3]]\n",
      "\n",
      " [[6.0 5.0 0 ... 1 2 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_2 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 768)          0           tf_distil_bert_model_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)            (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)            (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           49216       global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 12)           780         dropout_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 12)           780         dropout_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 12)           780         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 36)           0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 12)           444         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 563ms/step - loss: 2.3401 - binary_accuracy: 0.9167 - categorical_accuracy: 0.2705 - val_loss: 2.2993 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.1769\n",
      "[]\n",
      "12/12 [==============================] - 2s 148ms/step\n",
      "ACCURACY:\n",
      "0.21798365122615804\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [73  5  2 36  5  5 71 25 10  1  3 45]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  2  9 50  0  1  0  2  0  0  9]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[23.0 6.0 1 ... 1 9 2]]\n",
      "\n",
      " [[18.0 16.0 0 ... 1 11 2]]\n",
      "\n",
      " [[20.0 6.0 0 ... 10 10 2]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[10.0 32.0 0 ... 1 2 4]]\n",
      "\n",
      " [[14.0 9.0 1 ... 1 3 4]]\n",
      "\n",
      " [[10.0 19.0 0 ... 10 2 3]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[40.0 6.0 1 ... 9 9 3]]\n",
      "\n",
      " [[4.0 4.0 0 ... 1 1 2]]\n",
      "\n",
      " [[4.0 43.0 0 ... 1 1 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[14.0 4.0 0 ... 1 1 2]]\n",
      "\n",
      " [[14.0 6.0 0 ... 1 1 3]]\n",
      "\n",
      " [[9.0 1.0 1 ... 1 4 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_3 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 768)          0           tf_distil_bert_model_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_120 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 64)           49216       global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 12)           780         dropout_119[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 12)           780         dropout_120[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 12)           780         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 36)           0           dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 12)           444         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 582ms/step - loss: 2.3754 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1598 - val_loss: 2.2793 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.1701\n",
      "[]\n",
      "12/12 [==============================] - 2s 148ms/step\n",
      "ACCURACY:\n",
      "0.14168937329700274\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [69 28  7 55 56  7 52 40  3  1  2 47]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[20.0 6.0 1 ... 1 9 1]]\n",
      "\n",
      " [[8.0 12.0 0 ... 1 4 3]]\n",
      "\n",
      " [[4.0 3.0 0 ... 1 1 4]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[10.0 6.0 0 ... 1 1 4]]\n",
      "\n",
      " [[18.0 8.0 1 ... 1 9 2]]\n",
      "\n",
      " [[15.0 6.0 1 ... 9 9 4]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[16.0 9.0 1 ... 9 9 4]]\n",
      "\n",
      " [[9.0 36.0 0 ... 9 9 3]]\n",
      "\n",
      " [[7.0 5.0 0 ... 10 10 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[10.0 40.0 0 ... 1 1 2]]\n",
      "\n",
      " [[7.0 30.0 0 ... 1 1 4]]\n",
      "\n",
      " [[22.0 5.0 1 ... 8 5 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_4 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 768)          0           tf_distil_bert_model_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_141 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 64)           49216       global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 12)           780         dropout_140[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 12)           780         dropout_141[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 12)           780         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 36)           0           dense_22[0][0]                   \n",
      "                                                                 dense_23[0][0]                   \n",
      "                                                                 dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 12)           444         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 566ms/step - loss: 2.3673 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1621 - val_loss: 2.3252 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.1633\n",
      "[]\n",
      "12/12 [==============================] - 2s 150ms/step\n",
      "ACCURACY:\n",
      "0.17983651226158037\n",
      "\n",
      " [[18  0  0 25  0  1  8  3  0  0  0  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [61 30  5 37 48  3 54 24  8  1  1 33]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 1 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[7.0 2.0 0 ... 10 10 1]]\n",
      "\n",
      " [[1.0 2.0 1 ... 12 12 4]]\n",
      "\n",
      " [[26.0 5.0 0 ... 10 10 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[5.0 4.0 1 ... 1 9 1]]\n",
      "\n",
      " [[53.0 38.0 0 ... 10 2 3]]\n",
      "\n",
      " [[11.0 7.0 1 ... 9 9 4]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[23.0 14.0 0 ... 1 2 3]]\n",
      "\n",
      " [[16.0 22.0 0 ... 9 9 2]]\n",
      "\n",
      " [[36.0 8.0 0 ... 1 1 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.0 5.0 0 ... 12 12 2]]\n",
      "\n",
      " [[16.0 12.0 0 ... 1 4 3]]\n",
      "\n",
      " [[29.0 7.0 0 ... 1 2 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_5 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 768)          0           tf_distil_bert_model_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_161 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_162 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64)           49216       global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 12)           780         dropout_161[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 12)           780         dropout_162[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 12)           780         dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 36)           0           dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 12)           444         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 581ms/step - loss: 2.3921 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1962 - val_loss: 2.2837 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.2653\n",
      "[]\n",
      "12/12 [==============================] - 2s 150ms/step\n",
      "ACCURACY:\n",
      "0.27520435967302453\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 19  7 50  3  0  0  1  2  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [58  0  1  6 58  5 51 35  3  1 34]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[4.0 8.0 0 ... 6 6 1]]\n",
      "\n",
      " [[7.0 7.0 0 ... 1 1 2]]\n",
      "\n",
      " [[25.0 20.0 1 ... 10 2 2]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[7.0 8.0 1 ... 11 4 3]]\n",
      "\n",
      " [[14.0 7.0 1 ... 1 5 3]]\n",
      "\n",
      " [[4.0 7.0 1 ... 6 6 2]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1.0 5.0 0 ... 12 12 4]]\n",
      "\n",
      " [[7.0 6.0 0 ... 6 6 2]]\n",
      "\n",
      " [[32.0 2.0 1 ... 6 5 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.0 6.0 0 ... 12 12 1]]\n",
      "\n",
      " [[54.0 8.0 0 ... 1 1 2]]\n",
      "\n",
      " [[17.0 6.0 0 ... 1 1 2]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_6 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 768)          0           tf_distil_bert_model_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_182 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_183 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 64)           49216       global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 12)           780         dropout_182[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 12)           780         dropout_183[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 12)           780         dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 36)           0           dense_32[0][0]                   \n",
      "                                                                 dense_33[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 12)           444         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 572ms/step - loss: 2.3307 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1727 - val_loss: 2.2239 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.2245\n",
      "[]\n",
      "12/12 [==============================] - 2s 152ms/step\n",
      "ACCURACY:\n",
      "0.22343324250681199\n",
      "\n",
      " [[82 17  8 53 50  1 66 29 11  2  2 46]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 1 1 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[7.0 6.0 0 ... 1 1 2]]\n",
      "\n",
      " [[7.0 15.0 1 ... 1 5 1]]\n",
      "\n",
      " [[18.0 5.0 0 ... 10 10 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[24.0 15.0 0 ... 11 11 3]]\n",
      "\n",
      " [[30.0 7.0 1 ... 10 4 4]]\n",
      "\n",
      " [[8.0 9.0 0 ... 6 6 1]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[13.0 67.0 0 ... 9 9 3]]\n",
      "\n",
      " [[1.0 5.0 1 ... 12 12 3]]\n",
      "\n",
      " [[15.0 24.0 0 ... 1 2 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[10.0 11.0 1 ... 6 4 2]]\n",
      "\n",
      " [[22.0 35.0 0 ... 9 9 3]]\n",
      "\n",
      " [[30.0 7.0 0 ... 7 7 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_7 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 768)          0           tf_distil_bert_model_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_203 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_204 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 64)           49216       global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 12)           780         dropout_203[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 12)           780         dropout_204[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 12)           780         dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 36)           0           dense_37[0][0]                   \n",
      "                                                                 dense_38[0][0]                   \n",
      "                                                                 dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 12)           444         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 583ms/step - loss: 2.3328 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1939 - val_loss: 2.2454 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.2585\n",
      "[]\n",
      "12/12 [==============================] - 2s 151ms/step\n",
      "ACCURACY:\n",
      "0.2615803814713896\n",
      "\n",
      " [[28  0  0  2  0  1  6  4  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [19 18  6 47 45  0  5  5  7  1 22]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [32  3  1  0  2  4 53 27  5  1 21]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[14.0 5.0 0 ... 1 1 4]]\n",
      "\n",
      " [[36.0 11.0 0 ... 1 2 2]]\n",
      "\n",
      " [[24.0 24.0 1 ... 1 4 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[8.0 5.0 1 ... 1 4 4]]\n",
      "\n",
      " [[4.0 31.0 0 ... 1 1 4]]\n",
      "\n",
      " [[8.0 22.0 0 ... 6 2 4]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[11.0 17.0 0 ... 1 2 3]]\n",
      "\n",
      " [[24.0 55.0 0 ... 11 11 1]]\n",
      "\n",
      " [[4.0 34.0 0 ... 1 1 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[31.0 11.0 0 ... 10 10 3]]\n",
      "\n",
      " [[40.0 22.0 1 ... 1 4 3]]\n",
      "\n",
      " [[39.0 5.0 1 ... 9 4 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_8 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 768)          0           tf_distil_bert_model_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_224 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_225 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 64)           49216       global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 12)           780         dropout_224[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 12)           780         dropout_225[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 12)           780         dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 36)           0           dense_42[0][0]                   \n",
      "                                                                 dense_43[0][0]                   \n",
      "                                                                 dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 12)           444         concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 570ms/step - loss: 2.3875 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1530 - val_loss: 2.2873 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.1701\n",
      "[]\n",
      "12/12 [==============================] - 2s 148ms/step\n",
      "ACCURACY:\n",
      "0.2016348773841962\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [72 18  3 41 60  5 74 27 15  5 47]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[36.0 1.0 1 ... 1 5 3]]\n",
      "\n",
      " [[26.0 6.0 0 ... 1 1 3]]\n",
      "\n",
      " [[7.0 11.0 1 ... 1 5 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[21.0 18.0 0 ... 1 2 1]]\n",
      "\n",
      " [[8.0 6.0 0 ... 10 10 3]]\n",
      "\n",
      " [[4.0 2.0 0 ... 1 1 3]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[3.0 3.0 1 ... 12 12 3]]\n",
      "\n",
      " [[6.0 4.0 0 ... 1 1 2]]\n",
      "\n",
      " [[12.0 106.0 0 ... 1 2 2]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[14.0 9.0 1 ... 1 5 2]]\n",
      "\n",
      " [[5.0 10.0 0 ... 1 12 3]]\n",
      "\n",
      " [[8.0 3.0 0 ... 1 1 3]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_9 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 768)          0           tf_distil_bert_model_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_245 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_246 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 64)           49216       global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 12)           780         dropout_245[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 12)           780         dropout_246[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 12)           780         dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 36)           0           dense_47[0][0]                   \n",
      "                                                                 dense_48[0][0]                   \n",
      "                                                                 dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 12)           444         concatenate_9[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 575ms/step - loss: 2.4074 - binary_accuracy: 0.9167 - categorical_accuracy: 0.1439 - val_loss: 2.3205 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.2245\n",
      "[]\n",
      "12/12 [==============================] - 2s 150ms/step\n",
      "ACCURACY:\n",
      "0.30517711171662126\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [27  6  0 46  0  0 16  2  2  3  0 16]\n",
      " [56 13  6 10 53  3 29 35  8  1  1 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 13  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[12.0 10.0 0 ... 12 12 4]]\n",
      "\n",
      " [[9.0 3.0 0 ... 1 1 3]]\n",
      "\n",
      " [[40.0 50.0 0 ... 10 2 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[36.0 8.0 0 ... 1 1 1]]\n",
      "\n",
      " [[38.0 8.0 1 ... 1 9 3]]\n",
      "\n",
      " [[10.0 5.0 1 ... 1 4 4]]]\n",
      "(1467, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1.0 2.0 1 ... 12 12 3]]\n",
      "\n",
      " [[10.0 12.0 0 ... 10 2 3]]\n",
      "\n",
      " [[15.0 26.0 0 ... 6 6 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.0 6.0 0 ... 1 1 3]]\n",
      "\n",
      " [[28.0 8.0 0 ... 9 2 2]]\n",
      "\n",
      " [[24.0 4.0 0 ... 10 10 1]]]\n",
      "(367, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_10 (TFDist ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 768)          0           tf_distil_bert_model_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_266 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_267 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 64)           49216       global_average_pooling1d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 12)           780         dropout_266[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 12)           780         dropout_267[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 12)           780         dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 36)           0           dense_52[0][0]                   \n",
      "                                                                 dense_53[0][0]                   \n",
      "                                                                 dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 12)           444         concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 66,482,974\n",
      "Trainable params: 66,482,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 24s 572ms/step - loss: 2.3447 - binary_accuracy: 0.9167 - categorical_accuracy: 0.2015 - val_loss: 2.2451 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.3673\n",
      "[]\n",
      "12/12 [==============================] - 2s 148ms/step\n",
      "ACCURACY:\n",
      "0.3678474114441417\n",
      "\n",
      " [[69 18  5 30  9  4 62 35  9  2  1 46]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0 24  0  0  1  0  0  1  0  0]\n",
      " [ 1  0  5  0 42  0  0  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "\"\"\"\n",
    "Required for NLP model\n",
    "\"\"\"\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()   \n",
    "\n",
    "# set parameters for word embeddings\n",
    "embed_size = 100 # how big is each word vector\n",
    "vocab_size = 25000 # how many unique words to use (i.e num rows in embedding vector) max\n",
    "input_length = 100 # max number of words in the input \n",
    "\n",
    "#set parameters for bilstm\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=1\n",
    "\n",
    "#EMBEDDING_FILE='glove.6B.100d.txt'    \n",
    "\n",
    "file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/meta-nlp-bert-attn/bilstm_nlp_bert_attn_op.txt','w') #overwrites previous\n",
    "file.close()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    df_prediction = pd.DataFrame()\n",
    "    df_accuracy =  pd.DataFrame()\n",
    "    file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/meta-nlp-bert-attn/bilstm_meta_nlp_bert_attn_op.txt','a') #append mode \n",
    "    \"\"\"\n",
    "    Change the range before executing\n",
    "    \"\"\"\n",
    "    for i in range(1,11):\n",
    "        outputname = 'meta_meta_nlp_bert_attn'+ str(i)        \n",
    "        predictions, acc, conf_matrix = execute_bilstm_meta_nlp_bert_channel(i)\n",
    "        df_prediction[outputname] = predictions\n",
    "        df_accuracy[i] = [acc]\n",
    "        file.write(\"\\nIteration:\" + str(i) + \"\\nCategorical Accuracy:\" + str(acc) + \n",
    "                    \"\\nConfusion Matrix:\\n\" + str(conf_matrix) + \"\\n\\n\")\n",
    "        df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/meta-nlp-bert-attn/predictions_bilstm_meta_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "        df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/meta-nlp-bert-attn/accuracy_bilstm_meta_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "    \n",
    "    df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/meta-nlp-bert-attn/predictions_bilstm_meta_nlp_bert_attn.csv')    \n",
    "    df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/meta-nlp-bert-attn/accuracy_bilstm_nlp_meta_bert_attn.csv')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOVi8tWCg9mt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei8ZJF5c7h0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMJSRv35BJGllX89ZdTeT6q",
   "collapsed_sections": [
    "5JFvoYplrH52",
    "NzV6yCpJxh1D",
    "Qfp6B9MKyl4s",
    "F108rkzjys3d"
   ],
   "name": "meta_nlp_bert_attn_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e678526290a490c9dd01180acf7f38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0fbaac0e4f7244778feb8ccd14a1e60c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cd7bfda228d4f57a13fff3b2b50c001": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39f88875ada640af98ae75412365082c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4778e0cef55e485d9295b9a776452004": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4eed3deffbc2402d9d53078dd259494c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_815880f21d4440ebb862a4617d84253d",
      "placeholder": "​",
      "style": "IPY_MODEL_4778e0cef55e485d9295b9a776452004",
      "value": " 363M/363M [00:29&lt;00:00, 12.5MB/s]"
     }
    },
    "5516c7c0e99446e7af244f6308e5bbd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fbaac0e4f7244778feb8ccd14a1e60c",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e678526290a490c9dd01180acf7f38a",
      "value": 231508
     }
    },
    "5bf5ba6b1a414bffaae298093772f87b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6074a61e1c964963b945516602abeb2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2f31ee8108948cfae2663d0a7e65730",
      "placeholder": "​",
      "style": "IPY_MODEL_dd467ead277543c4a2a2d2f9573e2f9a",
      "value": " 232k/232k [00:00&lt;00:00, 794kB/s]"
     }
    },
    "815880f21d4440ebb862a4617d84253d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "846e12525dab4a86ae55975afd71dee9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6350fc2017b47378ee22aa64a6dca5b",
       "IPY_MODEL_4eed3deffbc2402d9d53078dd259494c"
      ],
      "layout": "IPY_MODEL_f15f16fc189a45ea940b19baec791109"
     }
    },
    "992db696177f454a92505c6062d1bbb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d97e84f51dd34c419d9a0914ddbe7431",
      "placeholder": "​",
      "style": "IPY_MODEL_e88eee6091464be7aef4b793e5a045a4",
      "value": " 442/442 [00:29&lt;00:00, 15.0B/s]"
     }
    },
    "9d453881557f42dd811a89e42a050e4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a816e6080f1b40eea121db12e9b2253d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4e947288fbc4ceb8ab00973d24c3841",
       "IPY_MODEL_992db696177f454a92505c6062d1bbb0"
      ],
      "layout": "IPY_MODEL_5bf5ba6b1a414bffaae298093772f87b"
     }
    },
    "d97e84f51dd34c419d9a0914ddbe7431": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd467ead277543c4a2a2d2f9573e2f9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e88eee6091464be7aef4b793e5a045a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec0291ee9087408a93eb7abcfc82073c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5516c7c0e99446e7af244f6308e5bbd2",
       "IPY_MODEL_6074a61e1c964963b945516602abeb2d"
      ],
      "layout": "IPY_MODEL_ef7da9e0f1fc4ac7b1bb14a29ad5f1e6"
     }
    },
    "ed6ade70ab52453a92a81c35c8e1dc47": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef7da9e0f1fc4ac7b1bb14a29ad5f1e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f15f16fc189a45ea940b19baec791109": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2f31ee8108948cfae2663d0a7e65730": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e947288fbc4ceb8ab00973d24c3841": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cd7bfda228d4f57a13fff3b2b50c001",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d453881557f42dd811a89e42a050e4e",
      "value": 442
     }
    },
    "f6350fc2017b47378ee22aa64a6dca5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed6ade70ab52453a92a81c35c8e1dc47",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39f88875ada640af98ae75412365082c",
      "value": 363423424
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1963,
     "status": "ok",
     "timestamp": 1597427327763,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "KQecsX9Tuw68",
    "outputId": "cf23bce3-a897-4143-e9b0-c5301748e643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20313,
     "status": "ok",
     "timestamp": 1597427125968,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "F1xqAIbVuxqg",
    "outputId": "58f772c9-448a-4c15-ef84-3fa00eae6a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 20kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 40kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 51kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 61kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 71kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 81kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 92kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 102kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 112kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 122kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 133kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 143kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 153kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 163kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 174kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 184kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 194kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 204kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 215kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 225kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 235kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 245kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 256kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 266kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 276kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 286kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 296kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 307kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 317kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 327kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 337kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 348kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 358kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 368kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 378kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 389kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 399kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 409kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 419kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 430kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 440kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 450kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 460kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 471kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 481kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 491kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 501kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 512kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 522kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 532kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 542kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 552kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 563kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 573kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 583kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 593kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 604kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 614kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 624kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 634kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 645kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 655kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 665kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 675kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 686kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 696kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 706kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 716kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 727kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 737kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 747kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 757kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 768kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 778kB 3.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 20.5MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 31.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 41.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=47d8fac46cf668c9d1152112341ed15e8d43cbed4d5f60979cde1433613f3dab\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24466,
     "status": "ok",
     "timestamp": 1597427130894,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "TSu-QYYnuzh0"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23805,
     "status": "ok",
     "timestamp": 1597427130900,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "D_ckyu5su1XI",
    "outputId": "c3fe2b64-8090-4de2-f6d6-19ad508dfda5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23050,
     "status": "ok",
     "timestamp": 1597427130901,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "GSoAhawbu3GY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "5dde9ce5cd2141159b0c6e9888c96e78",
      "e2e48bfb501b454b968bf8386cab0e8e",
      "b72331ddef8f4d1cbac390509df6eb80",
      "2a0ad0e8d4344dd0bea2b3963d81adfc",
      "e5ad8bc1269d42b486fd7fb881a1e030",
      "af13ffb5f26b47439255bd0523393678",
      "4009910ba5b8491b8cea23902e036496",
      "e30f401f9fbb4b7382ab0881e56924a2",
      "9e848744877a4a569347039d7d0b1ec4",
      "bbd8104c97c44add90e6147f4d0b71cb",
      "90e9b7bbfc4f45d4a2376ff0cd1ed422",
      "875671c42b8d49e482c6b3e1323172a0",
      "99804226178a4bec95d8d4c799773909",
      "c9e71b438a4e409385a535029547af3e",
      "a721d1680a0d4b91a7e3881793a402ff",
      "86ec9b9a6b364ac484c04e73d8597f84"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54420,
     "status": "ok",
     "timestamp": 1597427162981,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qUyUffSWu4lo",
    "outputId": "899cd957-049e-41c0-e4a3-04ed873dc160"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dde9ce5cd2141159b0c6e9888c96e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e848744877a4a569347039d7d0b1ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['vocab_projector', 'vocab_layer_norm', 'distilbert', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp_bert = transformers.TFBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "a4c7cc3c35b84e46b3da8eeac1d946f8",
      "e638abf8b82f438cb6b1a5cedc477d34",
      "b0264c51c84d4d3984fcb763bf6e3afa",
      "e68ee058f6a44ff68adc284b9d56e892",
      "b7cfb8428c6f467993d91fdc40b8b70f",
      "f70563f2fd1f4a4e979ffa7a8a838e2f",
      "74191a15d53a4f5db9bb54b159e760a2",
      "93c2b114654b4b8088c3c21c27a93263"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55604,
     "status": "ok",
     "timestamp": 1597427164835,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qMz_8LGFu7kZ",
    "outputId": "51ed3c7a-2a87-410e-a805-2de4c95b4589"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c7cc3c35b84e46b3da8eeac1d946f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JFvoYplrH52"
   },
   "source": [
    "# Generating META features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54507,
     "status": "ok",
     "timestamp": 1597427164837,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "8kqqqQpwrHiY"
   },
   "outputs": [],
   "source": [
    "def get_metadata_features(i):\n",
    "    print(\"Inside get_metadata_features\")\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.head())\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    X_train_meta = df_X_train.drop(['Previous_User_Utterance'], axis=1)\n",
    "    X_test_meta = df_X_test.drop(['Previous_User_Utterance'], axis=1)\n",
    "    \n",
    "    #print(X_train_meta['Previous_Speech_Act'].unique())\n",
    "    #print(X_train_meta['Previous_Search_Act'].unique())\n",
    "    \n",
    "    return X_train_meta, X_test_meta, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzV6yCpJxh1D"
   },
   "source": [
    "# Generating BERT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53430,
     "status": "ok",
     "timestamp": 1597427164838,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "kw7Kl1sXu9_L"
   },
   "outputs": [],
   "source": [
    "def load_files(i):\n",
    "\n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "\n",
    "    #print(list(df_train.columns))\n",
    "\n",
    "    df_train = df_train[[\"Search_acts\",\"Previous_User_Utterance\"]]\n",
    "    df_train = df_train.rename(columns={\"Search_acts\":\"y\", \"Previous_User_Utterance\":\"text\"})\n",
    "\n",
    "    \n",
    "\n",
    "    df_test = df_test[[\"Search_acts\",\"Previous_User_Utterance\"]]\n",
    "    df_test = df_test.rename(columns={\"Search_acts\":\"y\", \"Previous_User_Utterance\":\"text\"})\n",
    "\n",
    "    return df_train,df_test,df_Y_train,df_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52891,
     "status": "ok",
     "timestamp": 1597427164839,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "GPJkdcfJxSyn"
   },
   "outputs": [],
   "source": [
    "def generate_maskid(training, test):\n",
    "  #print(training['text'])\n",
    "  corpus_train = training['text']\n",
    "  corpus_test = test['text']\n",
    "  #the length of the feature vector is 150\n",
    "  maxlen = 150\n",
    "\n",
    "  #add special tokens\n",
    "  maxqnans = np.int((maxlen-20)/2)\n",
    "  corpus_tokenized_train = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_train]\n",
    "  corpus_tokenized_test = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_test]\n",
    "  #generate masks\n",
    "  masks_train = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_train]\n",
    "  masks_test = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_test]\n",
    "\n",
    "  #padding\n",
    "  txt2seq_train = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_train]\n",
    "  txt2seq_test = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_test]\n",
    "\n",
    "  #generate idx\n",
    "  idx_train = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_train]\n",
    "  idx_test = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_test]      \n",
    "\n",
    "  ## feature matrix\n",
    "  X_train = [np.asarray(idx_train, dtype='int32'), \n",
    "            np.asarray(masks_train, dtype='int32')]\n",
    "  X_test = [np.asarray(idx_test, dtype='int32'), \n",
    "            np.asarray(masks_test, dtype='int32')]\n",
    "  \n",
    "\n",
    "  idx_train = np.asarray(idx_train, dtype='int32')\n",
    "  masks_train = np.asarray(masks_train, dtype='int32')\n",
    "  idx_test = np.asarray(idx_test, dtype='int32')\n",
    "  masks_test = np.asarray(masks_test, dtype='int32')\n",
    "\n",
    "  #print(\"txt: \", training[\"text\"].iloc[0])\n",
    "  #print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "  #print(\"idx: \", X_train[0][i])\n",
    "  #print(\"mask: \", X_train[1][i])\n",
    "  return idx_train, masks_train, idx_test, masks_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52322,
     "status": "ok",
     "timestamp": 1597427164840,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Ss99j1L5xWqQ"
   },
   "outputs": [],
   "source": [
    "def model_generate(y_train):\n",
    "\n",
    "  ## inputs\n",
    "  idx = layers.Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "  masks = layers.Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "  ## pre-trained bert with config\n",
    "  config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "  config.output_hidden_states = False\n",
    "  nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "  bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "  ## fine-tuning\n",
    "  x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)\n",
    "  #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "  y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "  #nlp2 = transformers.TFDistilBertModel.from_pretrained('distilroberta-base', config=config)\n",
    "  #bert_out2 = nlp2(idx, attention_mask=masks)[0]\n",
    "  #x2 = layers.GlobalAveragePooling1D()(bert_out2)\n",
    "  #x2 = layers.Dense(32, activation=\"relu\")(x2)\n",
    "  #y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x2)\n",
    "\n",
    "\n",
    "\n",
    "  ## compile\n",
    "  model = models.Model([idx, masks], y_out)\n",
    "  for layer in model.layers[:3]:\n",
    "      layer.trainable = False\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51585,
     "status": "ok",
     "timestamp": 1597427164841,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ovBhxTYzxbW1"
   },
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, model):\n",
    "\n",
    "  ## encode y\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  print(dic_y_mapping)\n",
    "  inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "  print(inverse_dic)\n",
    "  y_train2 = np.array([inverse_dic[y] for y in y_train])\n",
    "  ## train\n",
    "  training = model.fit(x=X_train, y=y_train2, batch_size=32, epochs=300, shuffle=True, verbose=1, validation_split=0.3)\n",
    "\n",
    "  training_acc = training.history['accuracy']\n",
    "  training_loss = training.history['loss']\n",
    "  #print(acc)\n",
    "  #loss, accuracy = model.evaluate(x=X_test, y=y_test, batch_size=32, verbose=1)\n",
    "  return training_acc, training_loss\n",
    "  #loss, accuracy = model.evaluate(x=X_train, y=y_train, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51028,
     "status": "ok",
     "timestamp": 1597427164842,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "OuVvBsLUxfeC"
   },
   "outputs": [],
   "source": [
    "def test_classification(X_test, y_test, y_train):\n",
    "\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  #print(np.unique(y_test))\n",
    "  predicted_prob = model.predict(X_test)\n",
    "  #print(predicted_prob)\n",
    "  predicted = [dic_y_mapping[np.argmax(pred)] for pred in predicted_prob]\n",
    "\n",
    "  return predicted, predicted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qfp6B9MKyl4s"
   },
   "source": [
    "# Generating NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49749,
     "status": "ok",
     "timestamp": 1597427164843,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "T_PvYkfHyTeV"
   },
   "outputs": [],
   "source": [
    "def encode_nlp_features(df, name, i):\n",
    "    print(\"Inside encode_nlp_features\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"ne\")), columns=mlb.classes_, index=df.index).add_prefix('ne_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"iob\")), columns=mlb.classes_, index=df.index).add_prefix('iob_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"alpha\")), columns=mlb.classes_, index=df.index).add_prefix('al_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"digit\")), columns=mlb.classes_, index=df.index).add_prefix('dig_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"punc\")), columns=mlb.classes_, index=df.index).add_prefix('punc_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"url\")), columns=mlb.classes_, index=df.index).add_prefix('url_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"oov\")), columns=mlb.classes_, index=df.index).add_prefix('oov_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"stop\")), columns=mlb.classes_, index=df.index).add_prefix('stop_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"coarsepos\")), columns=mlb.classes_, index=df.index).add_prefix('cpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"finepos\")), columns=mlb.classes_, index=df.index).add_prefix('fpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"dep\")), columns=mlb.classes_, index=df.index).add_prefix('dep_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"offset\")), columns=mlb.classes_, index=df.index).add_prefix('off_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    #print(df.columns.values.tolist())\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49062,
     "status": "ok",
     "timestamp": 1597427164844,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Lc5ouptRyX0P"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method should generate all the natural language processing features\n",
    "\"\"\"\n",
    "def generate_nlp_features(df, i, name=\"\"):\n",
    "    print(\"Inside generate_nlp_features\")\n",
    "    sentence_ne = []\n",
    "    sentence_iob = []\n",
    "    sentence_alpha = []\n",
    "    sentence_isdigit = []\n",
    "    sentence_ispunc = []\n",
    "    sentence_isurl = []\n",
    "    sentence_isoov = []\n",
    "    sentence_isstop = []\n",
    "    sentence_coarsepos = []\n",
    "    sentence_finepos = []\n",
    "    sentence_dep = []\n",
    "    sentence_offset = []\n",
    "\n",
    "    i=0\n",
    "   \n",
    "    \"\"\"NEED TO CHANGE THE FUNCTIONAL COLUMN\"\"\"\n",
    "    for entries in df['Previous_User_Utterance']: #each entry is a sentence\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        i=i+1\n",
    "\n",
    "        doc = nlp(entries) #Spacy function to get tags\n",
    "        #print(doc)\n",
    "\n",
    "        ne = []\n",
    "        iob = []\n",
    "        alpha = []\n",
    "        isdigit = []\n",
    "        ispunc = []\n",
    "        isurl = []\n",
    "        isoov = []\n",
    "        isstop = []\n",
    "        coarsepos = []\n",
    "        finepos = []\n",
    "        dep = []\n",
    "        offset = []\n",
    "\n",
    "        \"\"\"\n",
    "        For words in the sentence\n",
    "        \"\"\"\n",
    "        for token in doc:\n",
    "            \"\"\"\n",
    "            Named Entity Type\n",
    "            \"\"\"\n",
    "            #print(\"NE Type:\", token.ent_type)\n",
    "            ne.append(token.ent_type_)\n",
    "\n",
    "            \"\"\"\n",
    "            IOB code of named entity tag. B means the token begins an entity, \n",
    "            I means it is inside an entity, O means it is outside an entity, and \n",
    "            '' means no entity tag is set.\n",
    "            \"\"\"\n",
    "            #print(\"IOB:\", token.ent_iob) \n",
    "            iob.append(token.ent_iob_)\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain alphabetic characters?\n",
    "            \"\"\"\n",
    "            #print(\"Alpha:\", token.is_alpha)\n",
    "            alpha.append(str(token.is_alpha))\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain digits?\n",
    "            \"\"\"\n",
    "            #print(\"Digits:\", token.is_digit)\n",
    "            isdigit.append(str(token.is_digit))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token punctuation?\n",
    "            \"\"\"\n",
    "            #print(\"Punc:\", token.is_punct)\n",
    "            ispunc.append(str(token.is_punct))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token like urls?\n",
    "            \"\"\"\n",
    "            #print(\"Url:\", token.like_url)\n",
    "            isurl.append(str(token.like_url))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token out-of-vocabulary?\n",
    "            \"\"\"\n",
    "            #print(\"OOV:\", token.is_oov)\n",
    "            isoov.append(str(token.is_oov))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token a stopword?\n",
    "            \"\"\"\n",
    "            #print(\"Stop:\", token.is_stop)\n",
    "            isstop.append(str(token.is_stop))\n",
    "\n",
    "            \"\"\"\n",
    "            Coarse-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Coarse POS:\", token.pos_)\n",
    "            coarsepos.append(token.pos_)\n",
    "\n",
    "            \"\"\"\n",
    "            Fine-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Fine POS:\", token.tag_)\n",
    "            finepos.append(token.tag_)\n",
    "\n",
    "            \"\"\"\n",
    "            Syntactic Dependency Relation\n",
    "            \"\"\"\n",
    "            #print(\"Dependency:\", token.dep_)\n",
    "            dep.append(token.dep_)\n",
    "\n",
    "            \"\"\"\n",
    "            Character offset\n",
    "            \"\"\"\n",
    "            #print(\"Offset:\", token.idx)\n",
    "            coff = token.idx\n",
    "            if(coff<100):\n",
    "                off = 1\n",
    "            elif(coff<200): \n",
    "                off =2\n",
    "            elif(coff<300): \n",
    "                off =3\n",
    "            elif(coff<400): \n",
    "                off =4\n",
    "            elif(coff<500): \n",
    "                off =5\n",
    "            elif(coff<600): \n",
    "                off =6\n",
    "            elif(coff<700): \n",
    "                off =7\n",
    "            elif(coff<800): \n",
    "                off =8\n",
    "            elif(coff<900): \n",
    "                off =9\n",
    "            elif(coff<1000): \n",
    "                off =10\n",
    "            else: \n",
    "                off =11\n",
    "            offset.append(off)\n",
    "            \n",
    "            #print(type(token.is_alpha))\n",
    "            #break\n",
    "\n",
    "        sentence_ne.append(ne)\n",
    "        sentence_iob.append(iob)\n",
    "        sentence_alpha.append(alpha)\n",
    "        sentence_isdigit.append(isdigit)\n",
    "        sentence_ispunc.append(ispunc)\n",
    "        sentence_isurl.append(isurl)\n",
    "        sentence_isoov.append(isoov)\n",
    "        sentence_isstop.append(isstop)\n",
    "        sentence_coarsepos.append(coarsepos)\n",
    "        sentence_finepos.append(finepos)\n",
    "        sentence_dep.append(dep)\n",
    "        sentence_offset.append(offset)\n",
    "    df[\"ne\"] = sentence_ne\n",
    "    df[\"iob\"] = sentence_iob\n",
    "    df[\"alpha\"] = sentence_alpha\n",
    "    df[\"digit\"] = sentence_isdigit\n",
    "    df[\"punc\"] = sentence_ispunc\n",
    "    df[\"url\"] = sentence_isurl\n",
    "    df[\"oov\"] = sentence_isoov\n",
    "    df[\"stop\"] = sentence_isstop\n",
    "    df[\"coarsepos\"] = sentence_coarsepos\n",
    "    df[\"finepos\"] = sentence_finepos\n",
    "    df[\"dep\"] = sentence_dep\n",
    "    df[\"offset\"] = sentence_offset\n",
    "    \n",
    "    print(len(sentence_ne) == len(sentence_iob) == len(sentence_alpha) == len(sentence_isdigit) == \\\n",
    "          len(sentence_ispunc) == len(sentence_isurl) == len(sentence_isoov) == len(sentence_isstop) \\\n",
    "          == len(sentence_coarsepos) == len(sentence_finepos) == len(sentence_dep) == len(sentence_offset))\n",
    "\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_intermediate_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46417,
     "status": "ok",
     "timestamp": 1597427164845,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "UAaBZiwrycQS"
   },
   "outputs": [],
   "source": [
    "def get_nlp_encodings(i):\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    print(\"Inside get_nlp_encodings\")\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    merging the output labels to predictors\n",
    "    \"\"\"\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "    print(\"The size of training and test sets:\", df_train.shape, df_test.shape)\n",
    "    r,c = df_train.shape\n",
    "    #print(\"Rows in training data =\", r)\n",
    "    \"\"\"\n",
    "    merging training and test data to one dataframe\n",
    "    \"\"\"\n",
    "    df = pd.concat([df_train,df_test])\n",
    "    #print(\"Final df:\", df.shape)\n",
    "    #print(df.head(2))\n",
    "    \"\"\"\n",
    "    generating the nlp features using spacy\n",
    "    \"\"\"\n",
    "    df_nlp = generate_nlp_features(df, i, 'all')\n",
    "    #print(\"After-spacy:\", df_nlp.shape, df_nlp.columns.values)\n",
    "    #print(df_nlp.head(2))\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding the nlp features using multilabelbinarizer\n",
    "    \"\"\"\n",
    "    df_nlp_enc = encode_nlp_features(df_nlp, 'all_nlp_enc', i)\n",
    "    #print(df_nlp_enc.shape)\n",
    "    #print(df_nlp_enc.head(2))\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    df_nlp_enc.drop(['Utterance_Number', 'Duration', 'Speaker', 'System_Number', 'Search_Task', 'Previous_User_Utterance', 'Previous_User_Speech_Act', 'Previous_Speech_Act', 'Previous_Search_Act','Search_acts'], axis=1, inplace=True)\n",
    "                     #['Transcript', 'Query_counter', 'Length', 'If_Intermediary', 'Complexity', 'Speech_acts'], axis=1, inplace=True)\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    train_split = df_nlp_enc.iloc[:r,:]\n",
    "    test_split = df_nlp_enc.iloc[r:,:]\n",
    "    print(\"Size after split:\",train_split.shape, test_split.shape, df_Y_train.shape, df_Y_test.shape)\n",
    "    return train_split, test_split, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F108rkzjys3d"
   },
   "source": [
    "# The Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50580,
     "status": "ok",
     "timestamp": 1597427170909,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "4EnT574ZB5Yl",
    "outputId": "1e3a19a8-27f8-4191-f390-c05ba9ab26c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Layer\n",
      "  Downloading https://files.pythonhosted.org/packages/43/9c/bedf88724d34e8f5caa4ce0555d22fc846a2cf17409653b777ad474b2605/layer-0.1.14-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from Layer) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.31.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.18.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.12.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.2.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow->Layer) (49.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.17.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Installing collected packages: Layer\n",
      "Successfully installed Layer-0.1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49872,
     "status": "ok",
     "timestamp": 1597427170910,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "cjblrAR8yiVZ"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "class AttentionLayer(Layer):\n",
    "   \n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CG-0wBxOqeGb"
   },
   "source": [
    "# DNN model creation and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48193,
     "status": "ok",
     "timestamp": 1597427170912,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "W_qwgVVuywu3"
   },
   "outputs": [],
   "source": [
    "def create_bilstm_meta_nlp_bert_channels(reshaped_data_nlp, reshaped_data_meta):\n",
    "    \n",
    "    idx = Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "    masks = Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "    ## pre-trained bert with config\n",
    "    config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "    config.output_hidden_states = False\n",
    "    nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "    bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "    ## fine-tuning\n",
    "    x = GlobalAveragePooling1D()(bert_out)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    y_out = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_nlp.shape\n",
    "    inp_nlp = Input(shape=(timesteps,features), name='input_nlp')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x1 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_nlp) \n",
    "    x1, sent_coeffs1 = AttentionLayer(features,return_coefficients=True,name='sent_attention1')(x1)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x1 = Dense(4, activation=\"softmax\")(x1) #output layer  \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_meta.shape\n",
    "    inp_meta = Input(shape=(timesteps,features), name='input_meta')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x2 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_meta) \n",
    "    x2, sent_coeffs2 = AttentionLayer(features,return_coefficients=True,name='sent_attention2')(x2)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x2 = Dropout(0.25)(x2)\n",
    "    x2 = Dense(4, activation=\"softmax\")(x2) #output layer  \n",
    "      \n",
    "    \n",
    "    \"\"\"\n",
    "    Merging outputs of nlp and meta\n",
    "    \"\"\"\n",
    "    merge_nlp_bert = concatenate([x1,x2,y_out])\n",
    "\n",
    "\n",
    "    output_meta_nlp_bert = Dense(4, activation=\"softmax\")(merge_nlp_bert)\n",
    "    model_meta_nlp_bert_combined = Model(inputs=[inp_nlp, inp_meta, idx, masks], outputs=output_meta_nlp_bert)\n",
    "    \n",
    "    return model_meta_nlp_bert_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45627,
     "status": "ok",
     "timestamp": 1597427171554,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Sfq9h5Pvzq5b"
   },
   "outputs": [],
   "source": [
    "def execute_bilstm_meta_nlp_bert_channel(i):\n",
    "    \"\"\"\n",
    "    nlp data\n",
    "    \"\"\"\n",
    "    df_train_nlp_encodings, df_test_nlp_encodings, df_Y_train_nlp, df_Y_test_nlp = get_nlp_encodings(i)\n",
    "    train_nlp_enc = df_train_nlp_encodings.values\n",
    "    test_nlp_enc = df_test_nlp_encodings.values\n",
    "    \n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_nlp_encodings.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_nlp = np.concatenate([train_nlp_enc[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_nlp = X_train_reshaped_nlp.reshape(staggering, timestamp, c) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_nlp)\n",
    "    print(X_train_reshaped_nlp.shape)\n",
    "    \n",
    "    r2, c2 = df_test_nlp_encodings.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_nlp = np.concatenate([test_nlp_enc[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_nlp = X_test_reshaped_nlp.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_nlp)\n",
    "    print(X_test_reshaped_nlp.shape)\n",
    "    \n",
    "    df_Y_train_nlp = df_Y_train_nlp.iloc[timestamp-1:,]\n",
    "    df_Y_test_nlp = df_Y_test_nlp.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_nlp.shape, df_Y_test_nlp.shape)    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    meta data\n",
    "    \"\"\"\n",
    "    df_train_meta, df_test_meta, df_Y_train_meta, df_Y_test_meta = get_metadata_features(i)\n",
    "    train_meta = df_train_meta.values\n",
    "    test_meta = df_test_meta.values\n",
    "        \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "        \n",
    "    r, c = df_train_meta.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_meta = np.concatenate([train_meta[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_meta = X_train_reshaped_meta.reshape(staggering, timestamp, c) # c is the number of features\n",
    "      \n",
    "    r2, c2 = df_test_meta.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_meta = np.concatenate([test_meta[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_meta = X_test_reshaped_meta.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "        \n",
    "    print(\"Meta data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_meta)\n",
    "    print(X_train_reshaped_meta.shape)\n",
    "        \n",
    "    print(\"Meta data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_meta)\n",
    "    print(X_test_reshaped_meta.shape)\n",
    "        \n",
    "    df_Y_train_meta = df_Y_train_meta.iloc[timestamp-1:,]\n",
    "    df_Y_test_meta = df_Y_test_meta.iloc[timestamp-1:,]\n",
    "    \n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_meta.shape, df_Y_test_meta.shape)\n",
    "\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    bert data\n",
    "    \"\"\"\n",
    "\n",
    "    df_train_bert,df_test_bert,df_Y_train_bert,df_Y_test_bert = load_files(i)\n",
    "    y_train_bert = df_Y_train_bert['Search_acts'].to_list()\n",
    "    y_test_bert = df_Y_test_bert['Search_acts'].to_list()\n",
    "    X_train_id, X_train_mask, X_test_id, X_test_mask = generate_maskid(df_train_bert,df_test_bert)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "    \n",
    "    model_meta_nlp_bert = create_bilstm_meta_nlp_bert_channels(X_train_reshaped_nlp,X_train_reshaped_meta)\n",
    "\n",
    "\n",
    "    print(\"\\n\\n THESE ARE THE LAYERS\")\n",
    "    for layer in model_meta_nlp_bert.layers:\n",
    "      if(layer.name == 'input_idx' or layer.name == 'input_masks' or layer.name == 'tf_distil_bert_model'):\n",
    "        layer.trainable = False\n",
    "      \n",
    "    model_meta_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    model_meta_nlp_bert.summary()  \n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel --starts\n",
    "    \"\"\"\n",
    "    encoding the output labels\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_Y_train_nlp)\n",
    "    training_op_labels_encoded = encoder.transform(df_Y_train_nlp)\n",
    "    test_op_labels_encoded = encoder.transform(df_Y_test_nlp)\n",
    "    print(\"Output Labels\\n-------------------\")\n",
    "    print(training_op_labels_encoded.shape, test_op_labels_encoded.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    converting the output labels to one-hot form\n",
    "    \"\"\"\n",
    "    training_op_labels_onehot= np_utils.to_categorical(training_op_labels_encoded)\n",
    "    test_op_labels_onehot = np_utils.to_categorical(test_op_labels_encoded)\n",
    "\n",
    "    print(training_op_labels_onehot.shape, len(training_op_labels_onehot))\n",
    "    print(test_op_labels_onehot.shape, len(test_op_labels_onehot))\n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel -ends\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # X_train_reshaped_nlp, X_test_reshaped_nlp\n",
    "    # X_train_we, X_test_we\n",
    "    # training_op_labels_onehot, test_op_labels_onehot\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #change the input_\n",
    "    #train data\n",
    "   \n",
    "    X_train_reshaped_meta = np.asarray(X_train_reshaped_meta, dtype='float32')\n",
    "    model_meta_nlp_bert_train = model_meta_nlp_bert.fit(x=[X_train_reshaped_nlp,X_train_reshaped_meta, X_train_id,X_train_mask], y=training_op_labels_onehot,batch_size=BATCH_SIZE, epochs=EPOCHS,  validation_split=0.1)\n",
    "    # load model if required \n",
    "    # compile model\n",
    "\n",
    "\n",
    "    model_meta_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    # evaluate model\n",
    "    print(model_meta_nlp_bert.metrics_names)\n",
    "    #change the input_\n",
    "\n",
    "    #loss, binary_accuracy, categorical_accuracy = model_nlp_bert.evaluate(x=[X_test_reshaped_nlp,X_test_id,X_test_mask], y=test_op_labels_onehot,batch_size=BATCH_SIZE,verbose=1)\n",
    "    \n",
    "    #print(loss, binary_accuracy, categorical_accuracy)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    predict the probabilty of output classes\n",
    "    and pick the best one\n",
    "    \"\"\"\n",
    "    #change the input_\n",
    "    X_test_reshaped_meta = np.asarray(X_test_reshaped_meta, dtype='float32')\n",
    "    prediction_vector = model_meta_nlp_bert.predict(x=[X_test_reshaped_nlp,X_test_reshaped_meta, X_test_id,X_test_mask], \\\n",
    "                           batch_size=BATCH_SIZE,\\\n",
    "                           verbose=1)\n",
    "    predicted_classes = np.argmax(prediction_vector, axis=1)\n",
    "    original_classes = np.argmax(test_op_labels_onehot, axis=1)\n",
    "    accuracy = metrics.accuracy_score(original_classes, predicted_classes)\n",
    "\n",
    "    print(\"ACCURACY:\")\n",
    "    print(accuracy)\n",
    "    \"\"\"\n",
    "    # verification of correctness:\n",
    "    total_correct = sum(original_classes == predicted_classes)\n",
    "    print(\"Total number of correct predictions:\",total_correct)\n",
    "    print(\"Accuracy:\",total_correct/len(test_op_labels_onehot))\n",
    "    acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
    "    print('Overall accuracy: {} %'.format(acc*100))\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(predicted_classes, original_classes)\n",
    "    print(\"\\n\", conf_mat, \"\\n\")\n",
    "    \n",
    "    return predicted_classes, accuracy, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOtihh1fqyiS"
   },
   "source": [
    "# Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33471,
     "status": "ok",
     "timestamp": 1597427988253,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ZnlXZmJJ1LFf",
    "outputId": "b4a4bdde-cfba-4bd0-e166-2fc2b9f93a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (406, 10) (102, 10)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (406, 123) (102, 123) (406, 1) (102, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(406, 1, 123)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(102, 1, 123)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n",
      "Inside get_metadata_features\n",
      "Meta data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[0.0 7.0 0 ... 1 2 2]]\n",
      "\n",
      " [[0.0 43.0 0 ... 1 11 1]]\n",
      "\n",
      " [[0.0 6.0 0 ... 1 1 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0 5.0 0 ... 10 2 2]]\n",
      "\n",
      " [[0.0 41.0 0 ... 1 1 3]]\n",
      "\n",
      " [[0.0 16.0 0 ... 12 12 3]]]\n",
      "(406, 1, 8)\n",
      "Meta data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[0.0 8.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 6.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 14.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 24.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 79.0 0 1 1 9 9 2]]\n",
      "\n",
      " [[0.0 15.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 6.0 0 1 1 1 1 2]]\n",
      "\n",
      " [[0.0 9.0 0 0 0 1 1 2]]\n",
      "\n",
      " [[0.0 13.0 0 0 1 1 11 2]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 11 2 1]]\n",
      "\n",
      " [[0.0 3.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 37.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 64.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 107.0 0 0 1 1 2 4]]\n",
      "\n",
      " [[0.0 35.0 0 0 2 10 10 1]]\n",
      "\n",
      " [[0.0 4.0 0 0 1 1 2 4]]\n",
      "\n",
      " [[0.0 7.0 0 0 1 1 11 4]]\n",
      "\n",
      " [[0.0 37.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 1 1 11 1]]\n",
      "\n",
      " [[0.0 7.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 20.0 0 1 1 1 2 2]]\n",
      "\n",
      " [[0.0 9.0 0 1 2 1 2 3]]\n",
      "\n",
      " [[0.0 19.0 0 1 2 1 2 3]]\n",
      "\n",
      " [[0.0 9.0 0 1 1 6 2 1]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 34.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 25.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 25.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 9.0 0 1 2 1 2 1]]\n",
      "\n",
      " [[0.0 11.0 0 1 1 6 2 3]]\n",
      "\n",
      " [[0.0 3.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 1 2 4]]\n",
      "\n",
      " [[0.0 21.0 0 0 0 1 2 1]]\n",
      "\n",
      " [[0.0 50.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 64.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 3.0 0 1 1 10 10 2]]\n",
      "\n",
      " [[0.0 11.0 0 0 0 6 2 3]]\n",
      "\n",
      " [[0.0 8.0 0 1 1 1 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 6 5 3]]\n",
      "\n",
      " [[0.0 26.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 55.0 0 0 1 1 2 2]]\n",
      "\n",
      " [[0.0 26.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 21.0 0 1 1 1 2 4]]\n",
      "\n",
      " [[0.0 15.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 15.0 0 0 0 12 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 1]]\n",
      "\n",
      " [[0.0 13.0 0 1 2 1 2 1]]\n",
      "\n",
      " [[0.0 16.0 0 1 1 6 6 3]]\n",
      "\n",
      " [[0.0 28.0 0 0 2 10 2 3]]\n",
      "\n",
      " [[0.0 12.0 0 1 1 10 10 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 14.0 0 1 1 1 1 4]]\n",
      "\n",
      " [[0.0 67.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 9.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 6.0 0 0 0 10 2 3]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 0 0 1 2 4]]\n",
      "\n",
      " [[0.0 7.0 0 0 0 1 2 2]]\n",
      "\n",
      " [[0.0 46.0 0 1 1 1 2 3]]\n",
      "\n",
      " [[0.0 10.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 10 10 1]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 1 5 2]]\n",
      "\n",
      " [[0.0 15.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 8.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 0 1 2 4]]\n",
      "\n",
      " [[0.0 4.0 0 1 2 10 2 3]]\n",
      "\n",
      " [[0.0 5.0 0 1 2 1 1 2]]\n",
      "\n",
      " [[0.0 59.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 13.0 0 0 2 1 2 2]]\n",
      "\n",
      " [[0.0 5.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 12.0 0 1 1 10 2 2]]\n",
      "\n",
      " [[0.0 53.0 0 1 1 10 11 3]]\n",
      "\n",
      " [[0.0 13.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 18.0 0 1 1 10 10 2]]\n",
      "\n",
      " [[0.0 4.0 0 1 2 1 1 2]]\n",
      "\n",
      " [[0.0 6.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 16.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 16.0 0 1 1 1 5 3]]\n",
      "\n",
      " [[0.0 17.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 9.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 55.0 0 0 1 1 2 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 11 2 3]]\n",
      "\n",
      " [[0.0 17.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 8.0 0 0 0 1 2 3]]\n",
      "\n",
      " [[0.0 7.0 0 0 1 1 11 1]]\n",
      "\n",
      " [[0.0 5.0 0 0 1 1 1 4]]\n",
      "\n",
      " [[0.0 15.0 0 0 2 1 2 1]]\n",
      "\n",
      " [[0.0 27.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 10.0 0 1 2 10 2 1]]\n",
      "\n",
      " [[0.0 4.0 0 1 1 10 10 1]]\n",
      "\n",
      " [[0.0 7.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 10.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 27.0 0 1 2 1 1 3]]\n",
      "\n",
      " [[0.0 47.0 0 0 1 1 2 3]]\n",
      "\n",
      " [[0.0 15.0 0 1 2 10 10 4]]\n",
      "\n",
      " [[0.0 31.0 0 1 1 10 2 1]]\n",
      "\n",
      " [[0.0 57.0 0 0 2 1 2 3]]\n",
      "\n",
      " [[0.0 16.0 0 1 2 10 11 1]]]\n",
      "(102, 1, 8)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_27\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 123)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_meta (InputLayer)         [(None, 1, 8)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_26 (Bidirectional (None, 1, 64)        39936       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_27 (Bidirectional (None, 1, 64)        10496       input_meta[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_13 (TFDist ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  8118        bidirectional_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention2 (AttentionLayer [(None, 64), (None,  528         bidirectional_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 768)          0           tf_distil_bert_model_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_329 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_330 (Dropout)           (None, 64)           0           sent_attention2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 64)           49216       global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 4)            260         dropout_329[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 4)            260         dropout_330[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 4)            260         dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 12)           0           dense_67[0][0]                   \n",
      "                                                                 dense_68[0][0]                   \n",
      "                                                                 dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 4)            52          concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 66,472,006\n",
      "Trainable params: 66,472,006\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(406,) (102,)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 1.4127 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3260WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f1ab58392f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "12/12 [==============================] - 7s 622ms/step - loss: 1.4127 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3260 - val_loss: 1.4150 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.3171\n",
      "[]\n",
      "4/4 [==============================] - 0s 93ms/step\n",
      "ACCURACY:\n",
      "0.3137254901960784\n",
      "\n",
      " [[ 0  0  0  0]\n",
      " [24 17 11  3]\n",
      " [13 12 15  7]\n",
      " [ 0  0  0  0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "\"\"\"\n",
    "Required for NLP model\n",
    "\"\"\"\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()   \n",
    "\n",
    "# set parameters for word embeddings\n",
    "embed_size = 100 # how big is each word vector\n",
    "vocab_size = 25000 # how many unique words to use (i.e num rows in embedding vector) max\n",
    "input_length = 100 # max number of words in the input \n",
    "\n",
    "#set parameters for bilstm\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=1\n",
    "\n",
    "#EMBEDDING_FILE='glove.6B.100d.txt'    \n",
    "\n",
    "file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-bert-attn/bilstm_nlp_bert_attn_op.txt','w') #overwrites previous\n",
    "file.close()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    df_prediction = pd.DataFrame()\n",
    "    df_accuracy =  pd.DataFrame()\n",
    "    file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-bert-attn/bilstm_meta_nlp_bert_attn_op.txt','a') #append mode \n",
    "    \"\"\"\n",
    "    Change the range before executing\n",
    "    \"\"\"\n",
    "    for i in range(1,2):\n",
    "        outputname = 'meta_meta_nlp_bert_attn'+ str(i)        \n",
    "        predictions, acc, conf_matrix = execute_bilstm_meta_nlp_bert_channel(i)\n",
    "        df_prediction[outputname] = predictions\n",
    "        df_accuracy[i] = [acc]\n",
    "        file.write(\"\\nIteration:\" + str(i) + \"\\nCategorical Accuracy:\" + str(acc) + \n",
    "                    \"\\nConfusion Matrix:\\n\" + str(conf_matrix) + \"\\n\\n\")\n",
    "        df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-bert-attn/predictions_bilstm_meta_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "        df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-bert-attn/accuracy_bilstm_meta_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "    \n",
    "    df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-bert-attn/predictions_bilstm_meta_nlp_bert_attn.csv')    \n",
    "    df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/meta-nlp-bert-attn/accuracy_bilstm_nlp_meta_bert_attn.csv')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOVi8tWCg9mt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei8ZJF5c7h0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPCzuWSTXPnKBa8JlsK6CZE",
   "collapsed_sections": [],
   "name": "Search_Tasks_meta_nlp_bert_attn_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2a0ad0e8d4344dd0bea2b3963d81adfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e30f401f9fbb4b7382ab0881e56924a2",
      "placeholder": "​",
      "style": "IPY_MODEL_4009910ba5b8491b8cea23902e036496",
      "value": " 442/442 [00:29&lt;00:00, 14.9B/s]"
     }
    },
    "4009910ba5b8491b8cea23902e036496": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5dde9ce5cd2141159b0c6e9888c96e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b72331ddef8f4d1cbac390509df6eb80",
       "IPY_MODEL_2a0ad0e8d4344dd0bea2b3963d81adfc"
      ],
      "layout": "IPY_MODEL_e2e48bfb501b454b968bf8386cab0e8e"
     }
    },
    "74191a15d53a4f5db9bb54b159e760a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86ec9b9a6b364ac484c04e73d8597f84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "875671c42b8d49e482c6b3e1323172a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86ec9b9a6b364ac484c04e73d8597f84",
      "placeholder": "​",
      "style": "IPY_MODEL_a721d1680a0d4b91a7e3881793a402ff",
      "value": " 363M/363M [00:29&lt;00:00, 12.4MB/s]"
     }
    },
    "90e9b7bbfc4f45d4a2376ff0cd1ed422": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9e71b438a4e409385a535029547af3e",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99804226178a4bec95d8d4c799773909",
      "value": 363423424
     }
    },
    "93c2b114654b4b8088c3c21c27a93263": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99804226178a4bec95d8d4c799773909": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9e848744877a4a569347039d7d0b1ec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_90e9b7bbfc4f45d4a2376ff0cd1ed422",
       "IPY_MODEL_875671c42b8d49e482c6b3e1323172a0"
      ],
      "layout": "IPY_MODEL_bbd8104c97c44add90e6147f4d0b71cb"
     }
    },
    "a4c7cc3c35b84e46b3da8eeac1d946f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0264c51c84d4d3984fcb763bf6e3afa",
       "IPY_MODEL_e68ee058f6a44ff68adc284b9d56e892"
      ],
      "layout": "IPY_MODEL_e638abf8b82f438cb6b1a5cedc477d34"
     }
    },
    "a721d1680a0d4b91a7e3881793a402ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af13ffb5f26b47439255bd0523393678": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0264c51c84d4d3984fcb763bf6e3afa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f70563f2fd1f4a4e979ffa7a8a838e2f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7cfb8428c6f467993d91fdc40b8b70f",
      "value": 231508
     }
    },
    "b72331ddef8f4d1cbac390509df6eb80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af13ffb5f26b47439255bd0523393678",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5ad8bc1269d42b486fd7fb881a1e030",
      "value": 442
     }
    },
    "b7cfb8428c6f467993d91fdc40b8b70f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bbd8104c97c44add90e6147f4d0b71cb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9e71b438a4e409385a535029547af3e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2e48bfb501b454b968bf8386cab0e8e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e30f401f9fbb4b7382ab0881e56924a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5ad8bc1269d42b486fd7fb881a1e030": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e638abf8b82f438cb6b1a5cedc477d34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e68ee058f6a44ff68adc284b9d56e892": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93c2b114654b4b8088c3c21c27a93263",
      "placeholder": "​",
      "style": "IPY_MODEL_74191a15d53a4f5db9bb54b159e760a2",
      "value": " 232k/232k [00:00&lt;00:00, 941kB/s]"
     }
    },
    "f70563f2fd1f4a4e979ffa7a8a838e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

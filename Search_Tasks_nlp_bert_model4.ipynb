{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1751,
     "status": "ok",
     "timestamp": 1597359612744,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "KQecsX9Tuw68",
    "outputId": "a53a8a5c-6906-487e-c45e-0e54863a15a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9535,
     "status": "ok",
     "timestamp": 1597359503087,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "F1xqAIbVuxqg",
    "outputId": "b1aeae5a-aee0-4143-ed3b-8909e1bf1001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 2.8MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 9.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 18.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 23.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1bb85013c6655afd2a08ce01d41556998e2bb987acbd5f0ee41bef64941cbf4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSu-QYYnuzh0"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3736,
     "status": "ok",
     "timestamp": 1597359509633,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "D_ckyu5su1XI",
    "outputId": "1deea3dc-9558-4f14-ccc2-bc4ae4adc359"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSoAhawbu3GY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "388b1305d0624f45a619ea6e40ae734e",
      "55ac35983ce74a8ab02a850898020c6e",
      "68e952a361e54935ab2c860bd1c394d4",
      "cc8af2cbafe4425dba8fe34b8c9c32fe",
      "4600562f678441889f4736b54a9edd02",
      "0cac0198f07f42e8b1a4793e3e35400d",
      "b2f964bd91034e2da5255162eec7ed9f",
      "0fa984949d844f409aa663305fd5632a",
      "5df40cc07ef74c6d9aff909f653e69c2",
      "b059b7342fe04af389c1f3b8587605cf",
      "20ffe4d209ba459bbdc3f02ff4030396",
      "4d40606873984834b1107bbb6348f6f3",
      "48b4d89af17f4af08c2eedb581de2ec5",
      "ecaf680b95214efb936b1f548041fd22",
      "fe56f688635249b0af8a085446a9c8c1",
      "f285bbb2fbab491599151dc883842873"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25104,
     "status": "ok",
     "timestamp": 1597359536039,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qUyUffSWu4lo",
    "outputId": "0bf30491-e3bc-45b1-95f1-f31c9604727b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388b1305d0624f45a619ea6e40ae734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df40cc07ef74c6d9aff909f653e69c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform', 'distilbert']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp_bert = transformers.TFBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "543d58144feb47759b586c72c19f1b31",
      "4c684604fdcd4e98a50c081d23431508",
      "18b217f15b0941d4a6b37d69920feb44",
      "2591ebcc84a840b4be3a5f2c0caed9ec",
      "79afc5e1bdcc4866b70eb3bd8eaf63a8",
      "49f6fe39beaa4d93bec70d61a7898a9e",
      "8effc818b4cc4058aad56be2f4003f46",
      "3eb79a1418cf4ec1826974709aa212cf"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26316,
     "status": "ok",
     "timestamp": 1597359538894,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qMz_8LGFu7kZ",
    "outputId": "389f874f-c32d-4f64-8bf4-c94d89984cf6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543d58144feb47759b586c72c19f1b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzV6yCpJxh1D"
   },
   "source": [
    "# Generatig the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kw7Kl1sXu9_L"
   },
   "outputs": [],
   "source": [
    "def load_files(i):\n",
    "\n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "\n",
    "    #print(list(df_train.columns))\n",
    "\n",
    "    df_train = df_train[[\"Search_acts\",\"Previous_User_Utterance\"]]\n",
    "    df_train = df_train.rename(columns={\"Search_acts\":\"y\", \"Previous_User_Utterance\":\"text\"})\n",
    "\n",
    "    \n",
    "\n",
    "    df_test = df_test[[\"Search_acts\",\"Previous_User_Utterance\"]]\n",
    "    df_test = df_test.rename(columns={\"Search_acts\":\"y\", \"Previous_User_Utterance\":\"text\"})\n",
    "\n",
    "    return df_train,df_test,df_Y_train,df_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPJkdcfJxSyn"
   },
   "outputs": [],
   "source": [
    "def generate_maskid(training, test):\n",
    "  #print(training['text'])\n",
    "  corpus_train = training['text']\n",
    "  corpus_test = test['text']\n",
    "  #the length of the feature vector is 150\n",
    "  maxlen = 150\n",
    "\n",
    "  #add special tokens\n",
    "  maxqnans = np.int((maxlen-20)/2)\n",
    "  corpus_tokenized_train = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_train]\n",
    "  corpus_tokenized_test = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_test]\n",
    "  #generate masks\n",
    "  masks_train = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_train]\n",
    "  masks_test = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_test]\n",
    "\n",
    "  #padding\n",
    "  txt2seq_train = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_train]\n",
    "  txt2seq_test = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_test]\n",
    "\n",
    "  #generate idx\n",
    "  idx_train = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_train]\n",
    "  idx_test = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_test]      \n",
    "\n",
    "  ## feature matrix\n",
    "  X_train = [np.asarray(idx_train, dtype='int32'), \n",
    "            np.asarray(masks_train, dtype='int32')]\n",
    "  X_test = [np.asarray(idx_test, dtype='int32'), \n",
    "            np.asarray(masks_test, dtype='int32')]\n",
    "  \n",
    "\n",
    "  idx_train = np.asarray(idx_train, dtype='int32')\n",
    "  masks_train = np.asarray(masks_train, dtype='int32')\n",
    "  idx_test = np.asarray(idx_test, dtype='int32')\n",
    "  masks_test = np.asarray(masks_test, dtype='int32')\n",
    "\n",
    "  #print(\"txt: \", training[\"text\"].iloc[0])\n",
    "  #print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "  #print(\"idx: \", X_train[0][i])\n",
    "  #print(\"mask: \", X_train[1][i])\n",
    "  return idx_train, masks_train, idx_test, masks_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ss99j1L5xWqQ"
   },
   "outputs": [],
   "source": [
    "def model_generate(y_train):\n",
    "\n",
    "  ## inputs\n",
    "  idx = layers.Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "  masks = layers.Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "  ## pre-trained bert with config\n",
    "  config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "  config.output_hidden_states = False\n",
    "  nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "  bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "  ## fine-tuning\n",
    "  x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)\n",
    "  #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "  y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "  #nlp2 = transformers.TFDistilBertModel.from_pretrained('distilroberta-base', config=config)\n",
    "  #bert_out2 = nlp2(idx, attention_mask=masks)[0]\n",
    "  #x2 = layers.GlobalAveragePooling1D()(bert_out2)\n",
    "  #x2 = layers.Dense(32, activation=\"relu\")(x2)\n",
    "  #y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x2)\n",
    "\n",
    "\n",
    "\n",
    "  ## compile\n",
    "  model = models.Model([idx, masks], y_out)\n",
    "  for layer in model.layers[:3]:\n",
    "      layer.trainable = False\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovBhxTYzxbW1"
   },
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, model):\n",
    "\n",
    "  ## encode y\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  print(dic_y_mapping)\n",
    "  inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "  print(inverse_dic)\n",
    "  y_train2 = np.array([inverse_dic[y] for y in y_train])\n",
    "  ## train\n",
    "  training = model.fit(x=X_train, y=y_train2, batch_size=32, epochs=300, shuffle=True, verbose=1, validation_split=0.3)\n",
    "\n",
    "  training_acc = training.history['accuracy']\n",
    "  training_loss = training.history['loss']\n",
    "  #print(acc)\n",
    "  #loss, accuracy = model.evaluate(x=X_test, y=y_test, batch_size=32, verbose=1)\n",
    "  return training_acc, training_loss\n",
    "  #loss, accuracy = model.evaluate(x=X_train, y=y_train, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuVvBsLUxfeC"
   },
   "outputs": [],
   "source": [
    "def test_classification(X_test, y_test, y_train):\n",
    "\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  #print(np.unique(y_test))\n",
    "  predicted_prob = model.predict(X_test)\n",
    "  #print(predicted_prob)\n",
    "  predicted = [dic_y_mapping[np.argmax(pred)] for pred in predicted_prob]\n",
    "\n",
    "  return predicted, predicted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qfp6B9MKyl4s"
   },
   "source": [
    "# Generating the NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_PvYkfHyTeV"
   },
   "outputs": [],
   "source": [
    "def encode_nlp_features(df, name, i):\n",
    "    print(\"Inside encode_nlp_features\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"ne\")), columns=mlb.classes_, index=df.index).add_prefix('ne_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"iob\")), columns=mlb.classes_, index=df.index).add_prefix('iob_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"alpha\")), columns=mlb.classes_, index=df.index).add_prefix('al_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"digit\")), columns=mlb.classes_, index=df.index).add_prefix('dig_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"punc\")), columns=mlb.classes_, index=df.index).add_prefix('punc_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"url\")), columns=mlb.classes_, index=df.index).add_prefix('url_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"oov\")), columns=mlb.classes_, index=df.index).add_prefix('oov_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"stop\")), columns=mlb.classes_, index=df.index).add_prefix('stop_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"coarsepos\")), columns=mlb.classes_, index=df.index).add_prefix('cpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"finepos\")), columns=mlb.classes_, index=df.index).add_prefix('fpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"dep\")), columns=mlb.classes_, index=df.index).add_prefix('dep_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"offset\")), columns=mlb.classes_, index=df.index).add_prefix('off_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    #print(df.columns.values.tolist())\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lc5ouptRyX0P"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method should generate all the natural language processing features\n",
    "\"\"\"\n",
    "def generate_nlp_features(df, i, name=\"\"):\n",
    "    print(\"Inside generate_nlp_features\")\n",
    "    sentence_ne = []\n",
    "    sentence_iob = []\n",
    "    sentence_alpha = []\n",
    "    sentence_isdigit = []\n",
    "    sentence_ispunc = []\n",
    "    sentence_isurl = []\n",
    "    sentence_isoov = []\n",
    "    sentence_isstop = []\n",
    "    sentence_coarsepos = []\n",
    "    sentence_finepos = []\n",
    "    sentence_dep = []\n",
    "    sentence_offset = []\n",
    "\n",
    "    i=0\n",
    "   \n",
    "    \"\"\"NEED TO CHANGE THE FUNCTIONAL COLUMN\"\"\"\n",
    "    for entries in df['Previous_User_Utterance']: #each entry is a sentence\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        i=i+1\n",
    "\n",
    "        doc = nlp(entries) #Spacy function to get tags\n",
    "        #print(doc)\n",
    "\n",
    "        ne = []\n",
    "        iob = []\n",
    "        alpha = []\n",
    "        isdigit = []\n",
    "        ispunc = []\n",
    "        isurl = []\n",
    "        isoov = []\n",
    "        isstop = []\n",
    "        coarsepos = []\n",
    "        finepos = []\n",
    "        dep = []\n",
    "        offset = []\n",
    "\n",
    "        \"\"\"\n",
    "        For words in the sentence\n",
    "        \"\"\"\n",
    "        for token in doc:\n",
    "            \"\"\"\n",
    "            Named Entity Type\n",
    "            \"\"\"\n",
    "            #print(\"NE Type:\", token.ent_type)\n",
    "            ne.append(token.ent_type_)\n",
    "\n",
    "            \"\"\"\n",
    "            IOB code of named entity tag. B means the token begins an entity, \n",
    "            I means it is inside an entity, O means it is outside an entity, and \n",
    "            '' means no entity tag is set.\n",
    "            \"\"\"\n",
    "            #print(\"IOB:\", token.ent_iob) \n",
    "            iob.append(token.ent_iob_)\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain alphabetic characters?\n",
    "            \"\"\"\n",
    "            #print(\"Alpha:\", token.is_alpha)\n",
    "            alpha.append(str(token.is_alpha))\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain digits?\n",
    "            \"\"\"\n",
    "            #print(\"Digits:\", token.is_digit)\n",
    "            isdigit.append(str(token.is_digit))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token punctuation?\n",
    "            \"\"\"\n",
    "            #print(\"Punc:\", token.is_punct)\n",
    "            ispunc.append(str(token.is_punct))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token like urls?\n",
    "            \"\"\"\n",
    "            #print(\"Url:\", token.like_url)\n",
    "            isurl.append(str(token.like_url))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token out-of-vocabulary?\n",
    "            \"\"\"\n",
    "            #print(\"OOV:\", token.is_oov)\n",
    "            isoov.append(str(token.is_oov))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token a stopword?\n",
    "            \"\"\"\n",
    "            #print(\"Stop:\", token.is_stop)\n",
    "            isstop.append(str(token.is_stop))\n",
    "\n",
    "            \"\"\"\n",
    "            Coarse-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Coarse POS:\", token.pos_)\n",
    "            coarsepos.append(token.pos_)\n",
    "\n",
    "            \"\"\"\n",
    "            Fine-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Fine POS:\", token.tag_)\n",
    "            finepos.append(token.tag_)\n",
    "\n",
    "            \"\"\"\n",
    "            Syntactic Dependency Relation\n",
    "            \"\"\"\n",
    "            #print(\"Dependency:\", token.dep_)\n",
    "            dep.append(token.dep_)\n",
    "\n",
    "            \"\"\"\n",
    "            Character offset\n",
    "            \"\"\"\n",
    "            #print(\"Offset:\", token.idx)\n",
    "            coff = token.idx\n",
    "            if(coff<100):\n",
    "                off = 1\n",
    "            elif(coff<200): \n",
    "                off =2\n",
    "            elif(coff<300): \n",
    "                off =3\n",
    "            elif(coff<400): \n",
    "                off =4\n",
    "            elif(coff<500): \n",
    "                off =5\n",
    "            elif(coff<600): \n",
    "                off =6\n",
    "            elif(coff<700): \n",
    "                off =7\n",
    "            elif(coff<800): \n",
    "                off =8\n",
    "            elif(coff<900): \n",
    "                off =9\n",
    "            elif(coff<1000): \n",
    "                off =10\n",
    "            else: \n",
    "                off =11\n",
    "            offset.append(off)\n",
    "            \n",
    "            #print(type(token.is_alpha))\n",
    "            #break\n",
    "\n",
    "        sentence_ne.append(ne)\n",
    "        sentence_iob.append(iob)\n",
    "        sentence_alpha.append(alpha)\n",
    "        sentence_isdigit.append(isdigit)\n",
    "        sentence_ispunc.append(ispunc)\n",
    "        sentence_isurl.append(isurl)\n",
    "        sentence_isoov.append(isoov)\n",
    "        sentence_isstop.append(isstop)\n",
    "        sentence_coarsepos.append(coarsepos)\n",
    "        sentence_finepos.append(finepos)\n",
    "        sentence_dep.append(dep)\n",
    "        sentence_offset.append(offset)\n",
    "    df[\"ne\"] = sentence_ne\n",
    "    df[\"iob\"] = sentence_iob\n",
    "    df[\"alpha\"] = sentence_alpha\n",
    "    df[\"digit\"] = sentence_isdigit\n",
    "    df[\"punc\"] = sentence_ispunc\n",
    "    df[\"url\"] = sentence_isurl\n",
    "    df[\"oov\"] = sentence_isoov\n",
    "    df[\"stop\"] = sentence_isstop\n",
    "    df[\"coarsepos\"] = sentence_coarsepos\n",
    "    df[\"finepos\"] = sentence_finepos\n",
    "    df[\"dep\"] = sentence_dep\n",
    "    df[\"offset\"] = sentence_offset\n",
    "    \n",
    "    print(len(sentence_ne) == len(sentence_iob) == len(sentence_alpha) == len(sentence_isdigit) == \\\n",
    "          len(sentence_ispunc) == len(sentence_isurl) == len(sentence_isoov) == len(sentence_isstop) \\\n",
    "          == len(sentence_coarsepos) == len(sentence_finepos) == len(sentence_dep) == len(sentence_offset))\n",
    "\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_intermediate_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAaBZiwrycQS"
   },
   "outputs": [],
   "source": [
    "def get_nlp_encodings(i):\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    print(\"Inside get_nlp_encodings\")\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/search/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    merging the output labels to predictors\n",
    "    \"\"\"\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "    print(\"The size of training and test sets:\", df_train.shape, df_test.shape)\n",
    "    r,c = df_train.shape\n",
    "    #print(\"Rows in training data =\", r)\n",
    "    \"\"\"\n",
    "    merging training and test data to one dataframe\n",
    "    \"\"\"\n",
    "    df = pd.concat([df_train,df_test])\n",
    "    #print(\"Final df:\", df.shape)\n",
    "    #print(df.head(2))\n",
    "    \"\"\"\n",
    "    generating the nlp features using spacy\n",
    "    \"\"\"\n",
    "    df_nlp = generate_nlp_features(df, i, 'all')\n",
    "    #print(\"After-spacy:\", df_nlp.shape, df_nlp.columns.values)\n",
    "    #print(df_nlp.head(2))\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding the nlp features using multilabelbinarizer\n",
    "    \"\"\"\n",
    "    df_nlp_enc = encode_nlp_features(df_nlp, 'all_nlp_enc', i)\n",
    "    #print(df_nlp_enc.shape)\n",
    "    #print(df_nlp_enc.head(2))\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    df_nlp_enc.drop(['Utterance_Number', 'Duration', 'Speaker', 'System_Number', 'Search_Task', 'Previous_User_Utterance', 'Previous_User_Speech_Act', 'Previous_Speech_Act', 'Previous_Search_Act','Search_acts'], axis=1, inplace=True)\n",
    "                     #['Transcript', 'Query_counter', 'Length', 'If_Intermediary', 'Complexity', 'Speech_acts'], axis=1, inplace=True)\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    train_split = df_nlp_enc.iloc[:r,:]\n",
    "    test_split = df_nlp_enc.iloc[r:,:]\n",
    "    print(\"Size after split:\",train_split.shape, test_split.shape, df_Y_train.shape, df_Y_test.shape)\n",
    "    return train_split, test_split, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F108rkzjys3d"
   },
   "source": [
    "# The Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7069,
     "status": "ok",
     "timestamp": 1597359548474,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "4EnT574ZB5Yl",
    "outputId": "d93be596-1c67-43a4-8ba7-ea05d7d2fe3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Layer\n",
      "  Downloading https://files.pythonhosted.org/packages/43/9c/bedf88724d34e8f5caa4ce0555d22fc846a2cf17409653b777ad474b2605/layer-0.1.14-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from Layer) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.31.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.15.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.18.5)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.3.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.12.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow->Layer) (49.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.17.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Installing collected packages: Layer\n",
      "Successfully installed Layer-0.1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjblrAR8yiVZ"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "class AttentionLayer(Layer):\n",
    "   \n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_qwgVVuywu3"
   },
   "outputs": [],
   "source": [
    "def create_bilstm_nlp_bert_channels(reshaped_data_nlp):\n",
    "    \n",
    "    idx = Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "    masks = Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "    ## pre-trained bert with config\n",
    "    config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "    config.output_hidden_states = False\n",
    "    nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "    bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "    ## fine-tuning\n",
    "    x = GlobalAveragePooling1D()(bert_out)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    y_out = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_nlp.shape\n",
    "    inp_nlp = Input(shape=(timesteps,features), name='input_nlp')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x1 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_nlp) \n",
    "    x1, sent_coeffs1 = AttentionLayer(features,return_coefficients=True,name='sent_attention1')(x1)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x1 = Dense(4, activation=\"softmax\")(x1) #output layer  \n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \"\"\"\n",
    "    Merging outputs of nlp and meta\n",
    "    \"\"\"\n",
    "    merge_nlp_bert = concatenate([x1, y_out])\n",
    "\n",
    "\n",
    "    output_nlp_bert = Dense(4, activation=\"softmax\")(merge_nlp_bert)\n",
    "    model_nlp_bert_combined = Model(inputs=[inp_nlp, idx, masks], outputs=output_nlp_bert)\n",
    "    \n",
    "    return model_nlp_bert_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sfq9h5Pvzq5b"
   },
   "outputs": [],
   "source": [
    "def execute_bilstm_nlp_bert_channel(i):\n",
    "    \"\"\"\n",
    "    nlp data\n",
    "    \"\"\"\n",
    "    df_train_nlp_encodings, df_test_nlp_encodings, df_Y_train_nlp, df_Y_test_nlp = get_nlp_encodings(i)\n",
    "    train_nlp_enc = df_train_nlp_encodings.values\n",
    "    test_nlp_enc = df_test_nlp_encodings.values\n",
    "    \n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_nlp_encodings.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_nlp = np.concatenate([train_nlp_enc[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_nlp = X_train_reshaped_nlp.reshape(staggering, timestamp, c) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_nlp)\n",
    "    print(X_train_reshaped_nlp.shape)\n",
    "    \n",
    "    r2, c2 = df_test_nlp_encodings.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_nlp = np.concatenate([test_nlp_enc[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_nlp = X_test_reshaped_nlp.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_nlp)\n",
    "    print(X_test_reshaped_nlp.shape)\n",
    "    \n",
    "    df_Y_train_nlp = df_Y_train_nlp.iloc[timestamp-1:,]\n",
    "    df_Y_test_nlp = df_Y_test_nlp.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_nlp.shape, df_Y_test_nlp.shape)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    bert data\n",
    "    \"\"\"\n",
    "\n",
    "    df_train_bert,df_test_bert,df_Y_train_bert,df_Y_test_bert = load_files(i)\n",
    "    y_train_bert = df_Y_train_bert['Search_acts'].to_list()\n",
    "    y_test_bert = df_Y_test_bert['Search_acts'].to_list()\n",
    "    X_train_id, X_train_mask, X_test_id, X_test_mask = generate_maskid(df_train_bert,df_test_bert)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "    \n",
    "    model_nlp_bert = create_bilstm_nlp_bert_channels(X_train_reshaped_nlp)\n",
    "\n",
    "\n",
    "    print(\"\\n\\n THESE ARE THE LAYERS\")\n",
    "    for layer in model_nlp_bert.layers[:5]:\n",
    "      if(layer.name == 'input_nlp' or layer.name == 'bidirectional'):\n",
    "        layer.trainable = True\n",
    "      else:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    model_nlp_bert.summary()  \n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel --starts\n",
    "    \"\"\"\n",
    "    encoding the output labels\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_Y_train_nlp)\n",
    "    training_op_labels_encoded = encoder.transform(df_Y_train_nlp)\n",
    "    test_op_labels_encoded = encoder.transform(df_Y_test_nlp)\n",
    "    print(\"Output Labels\\n-------------------\")\n",
    "    print(training_op_labels_encoded.shape, test_op_labels_encoded.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    converting the output labels to one-hot form\n",
    "    \"\"\"\n",
    "    training_op_labels_onehot= np_utils.to_categorical(training_op_labels_encoded)\n",
    "    test_op_labels_onehot = np_utils.to_categorical(test_op_labels_encoded)\n",
    "\n",
    "    print(training_op_labels_onehot.shape, len(training_op_labels_onehot))\n",
    "    print(test_op_labels_onehot.shape, len(test_op_labels_onehot))\n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel -ends\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # X_train_reshaped_nlp, X_test_reshaped_nlp\n",
    "    # X_train_we, X_test_we\n",
    "    # training_op_labels_onehot, test_op_labels_onehot\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #change the input_\n",
    "    #train data\n",
    "    X_train_reshaped_nlp = np.asarray(X_train_reshaped_nlp, dtype='float32')\n",
    "    model_nlp_bert_train = model_nlp_bert.fit(x=[X_train_reshaped_nlp,X_train_id,X_train_mask], y=training_op_labels_onehot,batch_size=BATCH_SIZE, epochs=EPOCHS,  validation_split=0.1)\n",
    "    # load model if required \n",
    "    # compile model\n",
    "\n",
    "\n",
    "    model_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    # evaluate model\n",
    "    print(model_nlp_bert.metrics_names)\n",
    "    #change the input_\n",
    "\n",
    "    #loss, binary_accuracy, categorical_accuracy = model_nlp_bert.evaluate(x=[X_test_reshaped_nlp,X_test_id,X_test_mask], y=test_op_labels_onehot,batch_size=BATCH_SIZE,verbose=1)\n",
    "    \n",
    "    #print(loss, binary_accuracy, categorical_accuracy)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    predict the probabilty of output classes\n",
    "    and pick the best one\n",
    "    \"\"\"\n",
    "    #change the input_\n",
    "    X_test_reshaped_nlp = np.asarray(X_test_reshaped_nlp, dtype='float32')\n",
    "    prediction_vector = model_nlp_bert.predict(x=[X_test_reshaped_nlp,X_test_id,X_test_mask], \\\n",
    "                           batch_size=BATCH_SIZE,\\\n",
    "                           verbose=1)\n",
    "    predicted_classes = np.argmax(prediction_vector, axis=1)\n",
    "    original_classes = np.argmax(test_op_labels_onehot, axis=1)\n",
    "    accuracy = metrics.accuracy_score(original_classes, predicted_classes)\n",
    "\n",
    "    print(\"ACCURACY:\")\n",
    "    print(accuracy)\n",
    "    \"\"\"\n",
    "    # verification of correctness:\n",
    "    total_correct = sum(original_classes == predicted_classes)\n",
    "    print(\"Total number of correct predictions:\",total_correct)\n",
    "    print(\"Accuracy:\",total_correct/len(test_op_labels_onehot))\n",
    "    acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
    "    print('Overall accuracy: {} %'.format(acc*100))\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(predicted_classes, original_classes)\n",
    "    print(\"\\n\", conf_mat, \"\\n\")\n",
    "    \n",
    "    return predicted_classes, accuracy, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42825,
     "status": "ok",
     "timestamp": 1597360523465,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ZnlXZmJJ1LFf",
    "outputId": "ca0ec159-436f-44ea-d8e3-fc69a0a32421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (406, 10) (102, 10)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (406, 123) (102, 123) (406, 1) (102, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(406, 1, 123)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 1 ... 1 0 0]]]\n",
      "(102, 1, 123)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 123)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 64)        39936       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_3 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  8118        bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 768)          0           tf_distil_bert_model_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           49216       global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 4)            260         dropout_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 4)            260         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8)            0           dense_14[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4)            36          concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,460,706\n",
      "Trainable params: 57,890\n",
      "Non-trainable params: 66,402,816\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(406,) (102,)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 3s 244ms/step - loss: 1.4099 - binary_accuracy: 0.7500 - categorical_accuracy: 0.1096 - val_loss: 1.4062 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.2927\n",
      "[]\n",
      "4/4 [==============================] - 0s 59ms/step\n",
      "ACCURACY:\n",
      "0.28431372549019607\n",
      "\n",
      " [[ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [33 13 27  5]\n",
      " [ 9  4  9  2]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (406, 10) (102, 10)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (406, 123) (102, 123) (406, 1) (102, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 1 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(406, 1, 123)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]\n",
      "\n",
      " [[1 0 0 ... 1 0 0]]]\n",
      "(102, 1, 123)\n",
      "Output Labels\n",
      "------------------\n",
      "(406, 1) (102, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 123)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 64)        39936       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_4 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  8118        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 768)          0           tf_distil_bert_model_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64)           49216       global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 4)            260         dropout_136[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 4)            260         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8)            0           dense_18[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 4)            36          concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,460,706\n",
      "Trainable params: 57,890\n",
      "Non-trainable params: 66,402,816\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(406,) (102,)\n",
      "(406, 4) 406\n",
      "(102, 4) 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 3s 248ms/step - loss: 1.3840 - binary_accuracy: 0.7500 - categorical_accuracy: 0.3562 - val_loss: 1.3690 - val_binary_accuracy: 0.7500 - val_categorical_accuracy: 0.3171\n",
      "[]\n",
      "4/4 [==============================] - 0s 59ms/step\n",
      "ACCURACY:\n",
      "0.38235294117647056\n",
      "\n",
      " [[ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [34 20 39  9]\n",
      " [ 0  0  0  0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "\"\"\"\n",
    "Required for NLP model\n",
    "\"\"\"\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()   \n",
    "\n",
    "# set parameters for word embeddings\n",
    "embed_size = 100 # how big is each word vector\n",
    "vocab_size = 25000 # how many unique words to use (i.e num rows in embedding vector) max\n",
    "input_length = 100 # max number of words in the input \n",
    "\n",
    "#set parameters for bilstm\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=1 #300\n",
    "\n",
    "#EMBEDDING_FILE='glove.6B.100d.txt'    \n",
    "\n",
    "file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/nlp-bert-attn/bilstm_nlp_bert_attn_op.txt','w') #overwrites previous\n",
    "file.close()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    df_prediction = pd.DataFrame()\n",
    "    df_accuracy =  pd.DataFrame()\n",
    "    file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/nlp-bert-attn/bilstm_nlp_bert_attn_op.txt','a') #append mode \n",
    "    \"\"\"\n",
    "    Change the range before executing\n",
    "    \"\"\"\n",
    "    for i in range(14,16):\n",
    "        outputname = 'nlp_bert_attn'+ str(i)        \n",
    "        predictions, acc, conf_matrix = execute_bilstm_nlp_bert_channel(i)\n",
    "        df_prediction[outputname] = predictions\n",
    "        df_accuracy[i] = [acc]\n",
    "        file.write(\"\\nIteration:\" + str(i) + \"\\nCategorical Accuracy:\" + str(acc) + \n",
    "                    \"\\nConfusion Matrix:\\n\" + str(conf_matrix) + \"\\n\\n\")\n",
    "        df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/nlp-bert-attn/predictions_bilstm_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "        df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/nlp-bert-attn/accuracy_bilstm_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "    \n",
    "    df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/nlp-bert-attn/predictions_bilstm_nlp_bert_attn.csv')    \n",
    "    df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/Search Tasks/nlp-bert-attn/accuracy_bilstm_nlp_bert_attn.csv')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOVi8tWCg9mt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei8ZJF5c7h0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOq/vnwtGt2yqommVOWh6ib",
   "collapsed_sections": [],
   "name": "Search_Tasks_nlp_bert_attn_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0cac0198f07f42e8b1a4793e3e35400d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fa984949d844f409aa663305fd5632a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18b217f15b0941d4a6b37d69920feb44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49f6fe39beaa4d93bec70d61a7898a9e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79afc5e1bdcc4866b70eb3bd8eaf63a8",
      "value": 231508
     }
    },
    "20ffe4d209ba459bbdc3f02ff4030396": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecaf680b95214efb936b1f548041fd22",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_48b4d89af17f4af08c2eedb581de2ec5",
      "value": 363423424
     }
    },
    "2591ebcc84a840b4be3a5f2c0caed9ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3eb79a1418cf4ec1826974709aa212cf",
      "placeholder": "​",
      "style": "IPY_MODEL_8effc818b4cc4058aad56be2f4003f46",
      "value": " 232k/232k [00:00&lt;00:00, 304kB/s]"
     }
    },
    "388b1305d0624f45a619ea6e40ae734e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68e952a361e54935ab2c860bd1c394d4",
       "IPY_MODEL_cc8af2cbafe4425dba8fe34b8c9c32fe"
      ],
      "layout": "IPY_MODEL_55ac35983ce74a8ab02a850898020c6e"
     }
    },
    "3eb79a1418cf4ec1826974709aa212cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4600562f678441889f4736b54a9edd02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "48b4d89af17f4af08c2eedb581de2ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "49f6fe39beaa4d93bec70d61a7898a9e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c684604fdcd4e98a50c081d23431508": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d40606873984834b1107bbb6348f6f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f285bbb2fbab491599151dc883842873",
      "placeholder": "​",
      "style": "IPY_MODEL_fe56f688635249b0af8a085446a9c8c1",
      "value": " 363M/363M [00:04&lt;00:00, 76.6MB/s]"
     }
    },
    "543d58144feb47759b586c72c19f1b31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18b217f15b0941d4a6b37d69920feb44",
       "IPY_MODEL_2591ebcc84a840b4be3a5f2c0caed9ec"
      ],
      "layout": "IPY_MODEL_4c684604fdcd4e98a50c081d23431508"
     }
    },
    "55ac35983ce74a8ab02a850898020c6e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5df40cc07ef74c6d9aff909f653e69c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20ffe4d209ba459bbdc3f02ff4030396",
       "IPY_MODEL_4d40606873984834b1107bbb6348f6f3"
      ],
      "layout": "IPY_MODEL_b059b7342fe04af389c1f3b8587605cf"
     }
    },
    "68e952a361e54935ab2c860bd1c394d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cac0198f07f42e8b1a4793e3e35400d",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4600562f678441889f4736b54a9edd02",
      "value": 442
     }
    },
    "79afc5e1bdcc4866b70eb3bd8eaf63a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8effc818b4cc4058aad56be2f4003f46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b059b7342fe04af389c1f3b8587605cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2f964bd91034e2da5255162eec7ed9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc8af2cbafe4425dba8fe34b8c9c32fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fa984949d844f409aa663305fd5632a",
      "placeholder": "​",
      "style": "IPY_MODEL_b2f964bd91034e2da5255162eec7ed9f",
      "value": " 442/442 [00:19&lt;00:00, 22.1B/s]"
     }
    },
    "ecaf680b95214efb936b1f548041fd22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f285bbb2fbab491599151dc883842873": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe56f688635249b0af8a085446a9c8c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

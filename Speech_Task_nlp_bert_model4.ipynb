{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35929,
     "status": "ok",
     "timestamp": 1597090701845,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "KQecsX9Tuw68",
    "outputId": "04ea19d1-8c47-4256-85cd-14e89afca4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9766,
     "status": "ok",
     "timestamp": 1597090715608,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "F1xqAIbVuxqg",
    "outputId": "3ed18737-85d4-479b-dea5-d156f5c2c09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 26.1MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 20kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 30kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 40kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 51kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 61kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 71kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 81kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 92kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 102kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 112kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 122kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 133kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 143kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 153kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 163kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 174kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 184kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 194kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 204kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 215kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 225kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 235kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 245kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 256kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 266kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 276kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 286kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 296kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 307kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 317kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 327kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 337kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 348kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 358kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 368kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 378kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 389kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 399kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 409kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 419kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 430kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 440kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 450kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 460kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 471kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 481kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 491kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 501kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 512kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 522kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 532kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 542kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 552kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 563kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 573kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 583kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 593kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 604kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 614kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 624kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 634kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 645kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 655kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 665kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 675kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 686kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 696kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 706kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 716kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 727kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 737kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 747kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 757kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 768kB 4.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 778kB 4.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 23.3MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 29.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 43.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5235c50822e96e5e3fe5ee887b489b29351d2d4537d72e537b516994938df860\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13821,
     "status": "ok",
     "timestamp": 1597090721838,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "TSu-QYYnuzh0"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12853,
     "status": "ok",
     "timestamp": 1597090721844,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "D_ckyu5su1XI",
    "outputId": "1daee8a1-6054-4c88-f733-4030cb9da2b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11558,
     "status": "ok",
     "timestamp": 1597090721846,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "GSoAhawbu3GY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "4398393c7a7b4b7f939aff76150e31a9",
      "56afabb0f9c54cba8b02caf76cd59cdf",
      "0bf1734997c34b82868fcb5d451dca36",
      "f50351662d944e8a9cb9c5643663c183",
      "2c20efff4660479f96dd553d06b7d813",
      "3b45c6e108ed44fbaf75b3d9fe384107",
      "2f680d38cf7343e9b8bb1ddd09b9942a",
      "440e0a79e43b460890f1367221369fd6",
      "896ff43e0f444a029f1f486fa95f392d",
      "b66ddbb14c184b80990c725972e04391",
      "1bbfe18e36944f3b9f5bae15b45fae8a",
      "aaf942b51c854ee1b2acaf1a19d8624b",
      "802b4a111ead4e148bd22f54b7e1d40b",
      "2a70fd15ced84829ae92ad5bd32df296",
      "20757f8eebff4e3db2eb0079264cd7f2",
      "a7999066b2d445f6bfda32de03b23f3b"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34750,
     "status": "ok",
     "timestamp": 1597090746408,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qUyUffSWu4lo",
    "outputId": "8a62a416-d4fa-497d-8b30-a2b2a484f887"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4398393c7a7b4b7f939aff76150e31a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896ff43e0f444a029f1f486fa95f392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'distilbert']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp_bert = transformers.TFBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "89f1c929c5fd4d3d91fee8678d2cae5a",
      "935d250b766b43a6b3e15d9e3a7508a7",
      "9181843ab3954946870a553a2f0996ff",
      "aae4fa28289d4d9f85a9b7707c56fc99",
      "75553ac8c12145d7aedbc6058c57a156",
      "863987df04c24d06a287e83937143c41",
      "6385739fbd324077876692216cf93b9b",
      "2185afc684d74475b9ee7eab479349f9"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34832,
     "status": "ok",
     "timestamp": 1597090747379,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "qMz_8LGFu7kZ",
    "outputId": "1f3041a9-aa3e-40e7-ad27-de294c28d6a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f1c929c5fd4d3d91fee8678d2cae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzV6yCpJxh1D"
   },
   "source": [
    "# Generatig the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32292,
     "status": "ok",
     "timestamp": 1597090747384,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "kw7Kl1sXu9_L"
   },
   "outputs": [],
   "source": [
    "def load_files(i):\n",
    "\n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_test_' + str(i) + '.pkl')\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "\n",
    "    #print(list(df_train.columns))\n",
    "\n",
    "    df_train = df_train[[\"Speech_acts\",\"Content\"]]\n",
    "    df_train = df_train.rename(columns={\"Speech_acts\":\"y\", \"Content\":\"text\"})\n",
    "\n",
    "    \n",
    "\n",
    "    df_test = df_test[[\"Speech_acts\",\"Content\"]]\n",
    "    df_test = df_test.rename(columns={\"Speech_acts\":\"y\", \"Content\":\"text\"})\n",
    "\n",
    "    return df_train,df_test,df_Y_train,df_Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30934,
     "status": "ok",
     "timestamp": 1597090747386,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "GPJkdcfJxSyn"
   },
   "outputs": [],
   "source": [
    "def generate_maskid(training, test):\n",
    "  #print(training['text'])\n",
    "  corpus_train = training['text']\n",
    "  corpus_test = test['text']\n",
    "  #the length of the feature vector is 150\n",
    "  maxlen = 150\n",
    "\n",
    "  #add special tokens\n",
    "  maxqnans = np.int((maxlen-20)/2)\n",
    "  corpus_tokenized_train = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_train]\n",
    "  corpus_tokenized_test = [\"[CLS] \"+\n",
    "              \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n",
    "              str(txt).lower().strip()))[:maxqnans])+\n",
    "              \" [SEP] \" for txt in corpus_test]\n",
    "  #generate masks\n",
    "  masks_train = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_train]\n",
    "  masks_test = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in corpus_tokenized_test]\n",
    "\n",
    "  #padding\n",
    "  txt2seq_train = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_train]\n",
    "  txt2seq_test = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))-2) if len(txt.split(\" \")) != (maxlen) else txt for txt in corpus_tokenized_test]\n",
    "\n",
    "  #generate idx\n",
    "  idx_train = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_train]\n",
    "  idx_test = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq_test]      \n",
    "\n",
    "  ## feature matrix\n",
    "  X_train = [np.asarray(idx_train, dtype='int32'), \n",
    "            np.asarray(masks_train, dtype='int32')]\n",
    "  X_test = [np.asarray(idx_test, dtype='int32'), \n",
    "            np.asarray(masks_test, dtype='int32')]\n",
    "  \n",
    "\n",
    "  idx_train = np.asarray(idx_train, dtype='int32')\n",
    "  masks_train = np.asarray(masks_train, dtype='int32')\n",
    "  idx_test = np.asarray(idx_test, dtype='int32')\n",
    "  masks_test = np.asarray(masks_test, dtype='int32')\n",
    "\n",
    "  #print(\"txt: \", training[\"text\"].iloc[0])\n",
    "  #print(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\n",
    "  #print(\"idx: \", X_train[0][i])\n",
    "  #print(\"mask: \", X_train[1][i])\n",
    "  return idx_train, masks_train, idx_test, masks_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29952,
     "status": "ok",
     "timestamp": 1597090747387,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Ss99j1L5xWqQ"
   },
   "outputs": [],
   "source": [
    "def model_generate(y_train):\n",
    "\n",
    "  ## inputs\n",
    "  idx = layers.Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "  masks = layers.Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "  ## pre-trained bert with config\n",
    "  config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "  config.output_hidden_states = False\n",
    "  nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "  bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "  ## fine-tuning\n",
    "  x = layers.GlobalAveragePooling1D()(bert_out)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)\n",
    "  #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "  y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x)\n",
    "\n",
    "  #nlp2 = transformers.TFDistilBertModel.from_pretrained('distilroberta-base', config=config)\n",
    "  #bert_out2 = nlp2(idx, attention_mask=masks)[0]\n",
    "  #x2 = layers.GlobalAveragePooling1D()(bert_out2)\n",
    "  #x2 = layers.Dense(32, activation=\"relu\")(x2)\n",
    "  #y_out = layers.Dense(len(np.unique(y_train)), activation='softmax')(x2)\n",
    "\n",
    "\n",
    "\n",
    "  ## compile\n",
    "  model = models.Model([idx, masks], y_out)\n",
    "  for layer in model.layers[:3]:\n",
    "      layer.trainable = False\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28590,
     "status": "ok",
     "timestamp": 1597090747389,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ovBhxTYzxbW1"
   },
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, model):\n",
    "\n",
    "  ## encode y\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  print(dic_y_mapping)\n",
    "  inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "  print(inverse_dic)\n",
    "  y_train2 = np.array([inverse_dic[y] for y in y_train])\n",
    "  ## train\n",
    "  training = model.fit(x=X_train, y=y_train2, batch_size=32, epochs=300, shuffle=True, verbose=1, validation_split=0.3)\n",
    "\n",
    "  training_acc = training.history['accuracy']\n",
    "  training_loss = training.history['loss']\n",
    "  #print(acc)\n",
    "  #loss, accuracy = model.evaluate(x=X_test, y=y_test, batch_size=32, verbose=1)\n",
    "  return training_acc, training_loss\n",
    "  #loss, accuracy = model.evaluate(x=X_train, y=y_train, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27645,
     "status": "ok",
     "timestamp": 1597090747390,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "OuVvBsLUxfeC"
   },
   "outputs": [],
   "source": [
    "def test_classification(X_test, y_test, y_train):\n",
    "\n",
    "  dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "  #print(np.unique(y_test))\n",
    "  predicted_prob = model.predict(X_test)\n",
    "  #print(predicted_prob)\n",
    "  predicted = [dic_y_mapping[np.argmax(pred)] for pred in predicted_prob]\n",
    "\n",
    "  return predicted, predicted_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qfp6B9MKyl4s"
   },
   "source": [
    "# Generating the NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25589,
     "status": "ok",
     "timestamp": 1597090747391,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "T_PvYkfHyTeV"
   },
   "outputs": [],
   "source": [
    "def encode_nlp_features(df, name, i):\n",
    "    print(\"Inside encode_nlp_features\")\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"ne\")), columns=mlb.classes_, index=df.index).add_prefix('ne_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"iob\")), columns=mlb.classes_, index=df.index).add_prefix('iob_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"alpha\")), columns=mlb.classes_, index=df.index).add_prefix('al_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"digit\")), columns=mlb.classes_, index=df.index).add_prefix('dig_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"punc\")), columns=mlb.classes_, index=df.index).add_prefix('punc_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"url\")), columns=mlb.classes_, index=df.index).add_prefix('url_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"oov\")), columns=mlb.classes_, index=df.index).add_prefix('oov_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"stop\")), columns=mlb.classes_, index=df.index).add_prefix('stop_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"coarsepos\")), columns=mlb.classes_, index=df.index).add_prefix('cpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"finepos\")), columns=mlb.classes_, index=df.index).add_prefix('fpos_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"dep\")), columns=mlb.classes_, index=df.index).add_prefix('dep_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    df = pd.concat([df,(pd.DataFrame(mlb.fit_transform(df.pop(\"offset\")), columns=mlb.classes_, index=df.index).add_prefix('off_'))], axis=1)\n",
    "    #print(df.shape)\n",
    "    #print(df.columns.values.tolist())\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24944,
     "status": "ok",
     "timestamp": 1597090748088,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Lc5ouptRyX0P"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method should generate all the natural language processing features\n",
    "\"\"\"\n",
    "def generate_nlp_features(df, i, name=\"\"):\n",
    "    print(\"Inside generate_nlp_features\")\n",
    "    sentence_ne = []\n",
    "    sentence_iob = []\n",
    "    sentence_alpha = []\n",
    "    sentence_isdigit = []\n",
    "    sentence_ispunc = []\n",
    "    sentence_isurl = []\n",
    "    sentence_isoov = []\n",
    "    sentence_isstop = []\n",
    "    sentence_coarsepos = []\n",
    "    sentence_finepos = []\n",
    "    sentence_dep = []\n",
    "    sentence_offset = []\n",
    "\n",
    "    i=0\n",
    "   \n",
    "    \"\"\"NEED TO CHANGE THE FUNCTIONAL COLUMN\"\"\"\n",
    "    for entries in df[\"Content\"]: #each entry is a sentence\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        i=i+1\n",
    "\n",
    "        doc = nlp(entries) #Spacy function to get tags\n",
    "        #print(doc)\n",
    "\n",
    "        ne = []\n",
    "        iob = []\n",
    "        alpha = []\n",
    "        isdigit = []\n",
    "        ispunc = []\n",
    "        isurl = []\n",
    "        isoov = []\n",
    "        isstop = []\n",
    "        coarsepos = []\n",
    "        finepos = []\n",
    "        dep = []\n",
    "        offset = []\n",
    "\n",
    "        \"\"\"\n",
    "        For words in the sentence\n",
    "        \"\"\"\n",
    "        for token in doc:\n",
    "            \"\"\"\n",
    "            Named Entity Type\n",
    "            \"\"\"\n",
    "            #print(\"NE Type:\", token.ent_type)\n",
    "            ne.append(token.ent_type_)\n",
    "\n",
    "            \"\"\"\n",
    "            IOB code of named entity tag. B means the token begins an entity, \n",
    "            I means it is inside an entity, O means it is outside an entity, and \n",
    "            '' means no entity tag is set.\n",
    "            \"\"\"\n",
    "            #print(\"IOB:\", token.ent_iob) \n",
    "            iob.append(token.ent_iob_)\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain alphabetic characters?\n",
    "            \"\"\"\n",
    "            #print(\"Alpha:\", token.is_alpha)\n",
    "            alpha.append(str(token.is_alpha))\n",
    "\n",
    "            \"\"\"\n",
    "            Does the token contain digits?\n",
    "            \"\"\"\n",
    "            #print(\"Digits:\", token.is_digit)\n",
    "            isdigit.append(str(token.is_digit))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token punctuation?\n",
    "            \"\"\"\n",
    "            #print(\"Punc:\", token.is_punct)\n",
    "            ispunc.append(str(token.is_punct))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token like urls?\n",
    "            \"\"\"\n",
    "            #print(\"Url:\", token.like_url)\n",
    "            isurl.append(str(token.like_url))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token out-of-vocabulary?\n",
    "            \"\"\"\n",
    "            #print(\"OOV:\", token.is_oov)\n",
    "            isoov.append(str(token.is_oov))\n",
    "\n",
    "            \"\"\"\n",
    "            Is the token a stopword?\n",
    "            \"\"\"\n",
    "            #print(\"Stop:\", token.is_stop)\n",
    "            isstop.append(str(token.is_stop))\n",
    "\n",
    "            \"\"\"\n",
    "            Coarse-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Coarse POS:\", token.pos_)\n",
    "            coarsepos.append(token.pos_)\n",
    "\n",
    "            \"\"\"\n",
    "            Fine-grained POS\n",
    "            \"\"\"\n",
    "            #print(\"Fine POS:\", token.tag_)\n",
    "            finepos.append(token.tag_)\n",
    "\n",
    "            \"\"\"\n",
    "            Syntactic Dependency Relation\n",
    "            \"\"\"\n",
    "            #print(\"Dependency:\", token.dep_)\n",
    "            dep.append(token.dep_)\n",
    "\n",
    "            \"\"\"\n",
    "            Character offset\n",
    "            \"\"\"\n",
    "            #print(\"Offset:\", token.idx)\n",
    "            coff = token.idx\n",
    "            if(coff<100):\n",
    "                off = 1\n",
    "            elif(coff<200): \n",
    "                off =2\n",
    "            elif(coff<300): \n",
    "                off =3\n",
    "            elif(coff<400): \n",
    "                off =4\n",
    "            elif(coff<500): \n",
    "                off =5\n",
    "            elif(coff<600): \n",
    "                off =6\n",
    "            elif(coff<700): \n",
    "                off =7\n",
    "            elif(coff<800): \n",
    "                off =8\n",
    "            elif(coff<900): \n",
    "                off =9\n",
    "            elif(coff<1000): \n",
    "                off =10\n",
    "            else: \n",
    "                off =11\n",
    "            offset.append(off)\n",
    "            \n",
    "            #print(type(token.is_alpha))\n",
    "            #break\n",
    "\n",
    "        sentence_ne.append(ne)\n",
    "        sentence_iob.append(iob)\n",
    "        sentence_alpha.append(alpha)\n",
    "        sentence_isdigit.append(isdigit)\n",
    "        sentence_ispunc.append(ispunc)\n",
    "        sentence_isurl.append(isurl)\n",
    "        sentence_isoov.append(isoov)\n",
    "        sentence_isstop.append(isstop)\n",
    "        sentence_coarsepos.append(coarsepos)\n",
    "        sentence_finepos.append(finepos)\n",
    "        sentence_dep.append(dep)\n",
    "        sentence_offset.append(offset)\n",
    "    df[\"ne\"] = sentence_ne\n",
    "    df[\"iob\"] = sentence_iob\n",
    "    df[\"alpha\"] = sentence_alpha\n",
    "    df[\"digit\"] = sentence_isdigit\n",
    "    df[\"punc\"] = sentence_ispunc\n",
    "    df[\"url\"] = sentence_isurl\n",
    "    df[\"oov\"] = sentence_isoov\n",
    "    df[\"stop\"] = sentence_isstop\n",
    "    df[\"coarsepos\"] = sentence_coarsepos\n",
    "    df[\"finepos\"] = sentence_finepos\n",
    "    df[\"dep\"] = sentence_dep\n",
    "    df[\"offset\"] = sentence_offset\n",
    "    \n",
    "    print(len(sentence_ne) == len(sentence_iob) == len(sentence_alpha) == len(sentence_isdigit) == \\\n",
    "          len(sentence_ispunc) == len(sentence_isurl) == len(sentence_isoov) == len(sentence_isstop) \\\n",
    "          == len(sentence_coarsepos) == len(sentence_finepos) == len(sentence_dep) == len(sentence_offset))\n",
    "\n",
    "    df.to_pickle('output\\\\nlp-meta-attn\\\\df_' + name + '_nlp_features_intermediate_' + str(i) + '.pkl')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23595,
     "status": "ok",
     "timestamp": 1597090748089,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "UAaBZiwrycQS"
   },
   "outputs": [],
   "source": [
    "def get_nlp_encodings(i):\n",
    "    \"\"\"\n",
    "    Obtain the dataset\n",
    "    \"\"\"\n",
    "    print(\"Inside get_nlp_encodings\")\n",
    "    # Extracted the text for nlp embeddings  \n",
    "    df_X_train= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_train_' + str(i) + '.pkl')\n",
    "    df_X_test= pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/X_test_' + str(i) + '.pkl')\n",
    "    df_Y_train=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_train_' + str(i) + '.pkl')\n",
    "    df_Y_test=pd.read_pickle('/content/drive/My Drive/Python Notebook/SCS_CONVEX/data-files/Y_test_' + str(i) + '.pkl')\n",
    "    #print(df_X_train.shape) #(835, 5)\n",
    "    #print(df_Y_train.shape) #(835, 1)\n",
    "    #print(df_X_test.shape) #(209, 5)\n",
    "    #print(df_Y_test.shape) #(209, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    merging the output labels to predictors\n",
    "    \"\"\"\n",
    "    df_train = pd.concat([df_X_train,df_Y_train], axis=1)\n",
    "    df_test = pd.concat([df_X_test,df_Y_test], axis=1)\n",
    "    print(\"The size of training and test sets:\", df_train.shape, df_test.shape)\n",
    "    r,c = df_train.shape\n",
    "    #print(\"Rows in training data =\", r)\n",
    "    \"\"\"\n",
    "    merging training and test data to one dataframe\n",
    "    \"\"\"\n",
    "    df = pd.concat([df_train,df_test])\n",
    "    #print(\"Final df:\", df.shape)\n",
    "    #print(df.head(2))\n",
    "    \"\"\"\n",
    "    generating the nlp features using spacy\n",
    "    \"\"\"\n",
    "    df_nlp = generate_nlp_features(df, i, 'all')\n",
    "    #print(\"After-spacy:\", df_nlp.shape, df_nlp.columns.values)\n",
    "    #print(df_nlp.head(2))\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding the nlp features using multilabelbinarizer\n",
    "    \"\"\"\n",
    "    df_nlp_enc = encode_nlp_features(df_nlp, 'all_nlp_enc', i)\n",
    "    #print(df_nlp_enc.shape)\n",
    "    #print(df_nlp_enc.head(2))\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    df_nlp_enc.drop(['Content', 'Utterance_Number', 'Duration', 'Speaker', 'System_Number', 'Search_Task', 'Previous_User_Utterance', 'Previous_User_Speech_Act', 'Previous_Speech_Act', 'Previous_Search_Act', 'Speech_acts'], axis=1, inplace=True)\n",
    "                     #['Transcript', 'Query_counter', 'Length', 'If_Intermediary', 'Complexity', 'Speech_acts'], axis=1, inplace=True)\n",
    "    #print(df_nlp_enc.shape, df_nlp_enc.columns.values.tolist())\n",
    "    train_split = df_nlp_enc.iloc[:r,:]\n",
    "    test_split = df_nlp_enc.iloc[r:,:]\n",
    "    print(\"Size after split:\",train_split.shape, test_split.shape, df_Y_train.shape, df_Y_test.shape)\n",
    "    return train_split, test_split, df_Y_train, df_Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F108rkzjys3d"
   },
   "source": [
    "# The Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26347,
     "status": "ok",
     "timestamp": 1597090752504,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "4EnT574ZB5Yl",
    "outputId": "eae9c268-8536-49e6-8847-7b9dae4d0d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Layer\n",
      "  Downloading https://files.pythonhosted.org/packages/43/9c/bedf88724d34e8f5caa4ce0555d22fc846a2cf17409653b777ad474b2605/layer-0.1.14-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from Layer) (2.3.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.34.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.30.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.15.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (2.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.12.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->Layer) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (49.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.17.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->Layer) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->Layer) (3.1.0)\n",
      "Installing collected packages: Layer\n",
      "Successfully installed Layer-0.1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24853,
     "status": "ok",
     "timestamp": 1597090752506,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "cjblrAR8yiVZ"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        # Initializer \n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weight = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23810,
     "status": "ok",
     "timestamp": 1597090752508,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "W_qwgVVuywu3"
   },
   "outputs": [],
   "source": [
    "def create_bilstm_nlp_bert_channels(reshaped_data_nlp):\n",
    "    \n",
    "    idx = Input((150), dtype=\"int32\", name=\"input_idx\")\n",
    "    masks = Input((150), dtype=\"int32\", name=\"input_masks\")\n",
    "    ## pre-trained bert with config\n",
    "    config = transformers.DistilBertConfig(dropout=0.2,attention_dropout=0.2)\n",
    "    config.output_hidden_states = False\n",
    "    nlp_bert = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "    bert_out = nlp_bert(idx, attention_mask=masks)[0]\n",
    "    ## fine-tuning\n",
    "    x = GlobalAveragePooling1D()(bert_out)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    #x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    y_out = Dense(12, activation='softmax')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    samples,timesteps,features = reshaped_data_nlp.shape\n",
    "    inp_nlp = Input(shape=(timesteps,features), name='input_nlp')\n",
    "    # lstm needs (samples,timesteps,features) tensor as the input\n",
    "    x1 = Bidirectional(LSTM(BATCH_SIZE, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(inp_nlp) \n",
    "    x1, sent_coeffs1 = AttentionLayer(features,return_coefficients=True,name='sent_attention1')(x1)\n",
    "    #x1 = GlobalMaxPool1D()(x1)\n",
    "    #x1 = Dense(100, activation=\"relu\")(x1)\n",
    "    x1 = Dropout(0.25)(x1)\n",
    "    x1 = Dense(12, activation=\"softmax\")(x1) #output layer  \n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \"\"\"\n",
    "    Merging outputs of nlp and meta\n",
    "    \"\"\"\n",
    "    merge_nlp_bert = concatenate([x1, y_out])\n",
    "\n",
    "\n",
    "    output_nlp_bert = Dense(12, activation=\"softmax\")(merge_nlp_bert)\n",
    "    model_nlp_bert_combined = Model(inputs=[inp_nlp, idx, masks], outputs=output_nlp_bert)\n",
    "    \n",
    "    return model_nlp_bert_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21371,
     "status": "ok",
     "timestamp": 1597090752889,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "Sfq9h5Pvzq5b"
   },
   "outputs": [],
   "source": [
    "def execute_bilstm_nlp_bert_channel(i):\n",
    "    \"\"\"\n",
    "    nlp data\n",
    "    \"\"\"\n",
    "    df_train_nlp_encodings, df_test_nlp_encodings, df_Y_train_nlp, df_Y_test_nlp = get_nlp_encodings(i)\n",
    "    train_nlp_enc = df_train_nlp_encodings.values\n",
    "    test_nlp_enc = df_test_nlp_encodings.values\n",
    "    \n",
    "    \n",
    "    timestamp = 1 #number of successive sequences combined\n",
    "    \n",
    "    r, c = df_train_nlp_encodings.shape\n",
    "    staggering = r - timestamp + 1 # final number of instances generated \n",
    "    X_train_reshaped_nlp = np.concatenate([train_nlp_enc[x:x+timestamp,:] for x in range(r-timestamp+1)])\n",
    "    X_train_reshaped_nlp = X_train_reshaped_nlp.reshape(staggering, timestamp, c) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Training set\\n------------------\")\n",
    "    print(X_train_reshaped_nlp)\n",
    "    print(X_train_reshaped_nlp.shape)\n",
    "    \n",
    "    r2, c2 = df_test_nlp_encodings.shape\n",
    "    staggering2 = r2 - timestamp + 1 # final number of instances generated \n",
    "    X_test_reshaped_nlp = np.concatenate([test_nlp_enc[x:x+timestamp,:] for x in range(r2-timestamp+1)])\n",
    "    X_test_reshaped_nlp = X_test_reshaped_nlp.reshape(staggering2, timestamp, c2) # c is the number of features\n",
    "    \n",
    "    print(\"NLP data\\n------------------\")\n",
    "    print(\"Test set\\n------------------\")\n",
    "    print(X_test_reshaped_nlp)\n",
    "    print(X_test_reshaped_nlp.shape)\n",
    "    \n",
    "    df_Y_train_nlp = df_Y_train_nlp.iloc[timestamp-1:,]\n",
    "    df_Y_test_nlp = df_Y_test_nlp.iloc[timestamp-1:,]\n",
    "    \n",
    "    print(\"Output Labels\\n------------------\")\n",
    "    print(df_Y_train_nlp.shape, df_Y_test_nlp.shape)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    bert data\n",
    "    \"\"\"\n",
    "\n",
    "    df_train_bert,df_test_bert,df_Y_train_bert,df_Y_test_bert = load_files(i)\n",
    "    y_train_bert = df_Y_train_bert['Speech_acts'].to_list()\n",
    "    y_test_bert = df_Y_test_bert['Speech_acts'].to_list()\n",
    "    X_train_id, X_train_mask, X_test_id, X_test_mask = generate_maskid(df_train_bert,df_test_bert)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "    \n",
    "    model_nlp_bert = create_bilstm_nlp_bert_channels(X_train_reshaped_nlp)\n",
    "\n",
    "\n",
    "    print(\"\\n\\n THESE ARE THE LAYERS\")\n",
    "    for layer in model_nlp_bert.layers[:5]:\n",
    "      if(layer.name == 'input_nlp' or layer.name == 'bidirectional'):\n",
    "        layer.trainable = True\n",
    "      else:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    model_nlp_bert.summary()  \n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel --starts\n",
    "    \"\"\"\n",
    "    encoding the output labels\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_Y_train_nlp)\n",
    "    training_op_labels_encoded = encoder.transform(df_Y_train_nlp)\n",
    "    test_op_labels_encoded = encoder.transform(df_Y_test_nlp)\n",
    "    print(\"Output Labels\\n-------------------\")\n",
    "    print(training_op_labels_encoded.shape, test_op_labels_encoded.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    converting the output labels to one-hot form\n",
    "    \"\"\"\n",
    "    training_op_labels_onehot= np_utils.to_categorical(training_op_labels_encoded)\n",
    "    test_op_labels_onehot = np_utils.to_categorical(test_op_labels_encoded)\n",
    "\n",
    "    print(training_op_labels_onehot.shape, len(training_op_labels_onehot))\n",
    "    print(test_op_labels_onehot.shape, len(test_op_labels_onehot))\n",
    "    \n",
    "    #$$ this section is unique depending on the type of channel -ends\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # can save and read back for iterations\n",
    "    # X_train_reshaped_nlp, X_test_reshaped_nlp\n",
    "    # X_train_we, X_test_we\n",
    "    # training_op_labels_onehot, test_op_labels_onehot\n",
    "    # ----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    $$$$ ---------------\n",
    "    $$$$ CHANGE HERE _word_meta _word_nlp _nlp_meta _word_nlp_meta\n",
    "    $$$$ ---------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #change the input_\n",
    "    #train data\n",
    "   \n",
    "    model_nlp_bert_train = model_nlp_bert.fit(x=[X_train_reshaped_nlp,X_train_id,X_train_mask], y=training_op_labels_onehot,batch_size=BATCH_SIZE, epochs=EPOCHS,  validation_split=0.1)\n",
    "    # load model if required \n",
    "    # compile model\n",
    "\n",
    "\n",
    "    model_nlp_bert.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    \n",
    "    # evaluate model\n",
    "    print(model_nlp_bert.metrics_names)\n",
    "    #change the input_\n",
    "\n",
    "    #loss, binary_accuracy, categorical_accuracy = model_nlp_bert.evaluate(x=[X_test_reshaped_nlp,X_test_id,X_test_mask], y=test_op_labels_onehot,batch_size=BATCH_SIZE,verbose=1)\n",
    "    \n",
    "    #print(loss, binary_accuracy, categorical_accuracy)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    predict the probabilty of output classes\n",
    "    and pick the best one\n",
    "    \"\"\"\n",
    "    #change the input_\n",
    "    prediction_vector = model_nlp_bert.predict(x=[X_test_reshaped_nlp,X_test_id,X_test_mask], \\\n",
    "                           batch_size=BATCH_SIZE,\\\n",
    "                           verbose=1)\n",
    "    predicted_classes = np.argmax(prediction_vector, axis=1)\n",
    "    original_classes = np.argmax(test_op_labels_onehot, axis=1)\n",
    "    accuracy = metrics.accuracy_score(original_classes, predicted_classes)\n",
    "\n",
    "    print(\"ACCURACY:\")\n",
    "    print(accuracy)\n",
    "    \"\"\"\n",
    "    # verification of correctness:\n",
    "    total_correct = sum(original_classes == predicted_classes)\n",
    "    print(\"Total number of correct predictions:\",total_correct)\n",
    "    print(\"Accuracy:\",total_correct/len(test_op_labels_onehot))\n",
    "    acc = np.sum(conf_mat.diagonal()) / np.sum(conf_mat)\n",
    "    print('Overall accuracy: {} %'.format(acc*100))\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(predicted_classes, original_classes)\n",
    "    print(\"\\n\", conf_mat, \"\\n\")\n",
    "    \n",
    "    return predicted_classes, accuracy, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5062894,
     "status": "ok",
     "timestamp": 1597095798267,
     "user": {
      "displayName": "Satanu Ghosh",
      "photoUrl": "",
      "userId": "04517623213450830512"
     },
     "user_tz": -330
    },
    "id": "ZnlXZmJJ1LFf",
    "outputId": "185ee4fd-cda5-4370-bcf7-33954511d2a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12)           780         dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12)           780         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 24)           0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 12)           300         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 66,471,026\n",
      "Trainable params: 108,146\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 9s 220ms/step - loss: 2.3367 - binary_accuracy: 0.9167 - categorical_accuracy: 0.3311 - val_loss: 2.1880 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.5170\n",
      "Epoch 2/300\n",
      "42/42 [==============================] - 8s 182ms/step - loss: 2.0736 - binary_accuracy: 0.9167 - categorical_accuracy: 0.5856 - val_loss: 1.9908 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.5102\n",
      "Epoch 3/300\n",
      "42/42 [==============================] - 8s 183ms/step - loss: 1.8931 - binary_accuracy: 0.9167 - categorical_accuracy: 0.5886 - val_loss: 1.8697 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.5238\n",
      "Epoch 4/300\n",
      "42/42 [==============================] - 8s 184ms/step - loss: 1.7757 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6098 - val_loss: 1.7733 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.5578\n",
      "Epoch 5/300\n",
      "42/42 [==============================] - 8s 185ms/step - loss: 1.6791 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6136 - val_loss: 1.6918 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.5782\n",
      "Epoch 6/300\n",
      "42/42 [==============================] - 8s 186ms/step - loss: 1.5936 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6530 - val_loss: 1.6241 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6054\n",
      "Epoch 7/300\n",
      "42/42 [==============================] - 8s 187ms/step - loss: 1.5245 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6689 - val_loss: 1.5602 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6259\n",
      "Epoch 8/300\n",
      "42/42 [==============================] - 8s 188ms/step - loss: 1.4610 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6758 - val_loss: 1.5057 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6395\n",
      "Epoch 9/300\n",
      "42/42 [==============================] - 8s 190ms/step - loss: 1.4104 - binary_accuracy: 0.9190 - categorical_accuracy: 0.6833 - val_loss: 1.4533 - val_binary_accuracy: 0.9252 - val_categorical_accuracy: 0.6463\n",
      "Epoch 10/300\n",
      "42/42 [==============================] - 8s 190ms/step - loss: 1.3590 - binary_accuracy: 0.9280 - categorical_accuracy: 0.6811 - val_loss: 1.4123 - val_binary_accuracy: 0.9252 - val_categorical_accuracy: 0.6531\n",
      "Epoch 11/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.3152 - binary_accuracy: 0.9280 - categorical_accuracy: 0.6924 - val_loss: 1.3761 - val_binary_accuracy: 0.9252 - val_categorical_accuracy: 0.6667\n",
      "Epoch 12/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.2681 - binary_accuracy: 0.9282 - categorical_accuracy: 0.6962 - val_loss: 1.3371 - val_binary_accuracy: 0.9252 - val_categorical_accuracy: 0.6667\n",
      "Epoch 13/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.2326 - binary_accuracy: 0.9313 - categorical_accuracy: 0.6955 - val_loss: 1.3122 - val_binary_accuracy: 0.9303 - val_categorical_accuracy: 0.6531\n",
      "Epoch 14/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.1942 - binary_accuracy: 0.9412 - categorical_accuracy: 0.6985 - val_loss: 1.2824 - val_binary_accuracy: 0.9393 - val_categorical_accuracy: 0.6599\n",
      "Epoch 15/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.1566 - binary_accuracy: 0.9479 - categorical_accuracy: 0.7030 - val_loss: 1.2523 - val_binary_accuracy: 0.9393 - val_categorical_accuracy: 0.6531\n",
      "Epoch 16/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 1.1300 - binary_accuracy: 0.9477 - categorical_accuracy: 0.7023 - val_loss: 1.2306 - val_binary_accuracy: 0.9393 - val_categorical_accuracy: 0.6463\n",
      "Epoch 17/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.0963 - binary_accuracy: 0.9482 - categorical_accuracy: 0.7045 - val_loss: 1.2028 - val_binary_accuracy: 0.9393 - val_categorical_accuracy: 0.6599\n",
      "Epoch 18/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.0674 - binary_accuracy: 0.9484 - categorical_accuracy: 0.7000 - val_loss: 1.1652 - val_binary_accuracy: 0.9393 - val_categorical_accuracy: 0.6667\n",
      "Epoch 19/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.0420 - binary_accuracy: 0.9488 - categorical_accuracy: 0.7053 - val_loss: 1.1544 - val_binary_accuracy: 0.9382 - val_categorical_accuracy: 0.6667\n",
      "Epoch 20/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.0130 - binary_accuracy: 0.9483 - categorical_accuracy: 0.7553 - val_loss: 1.1165 - val_binary_accuracy: 0.9388 - val_categorical_accuracy: 0.7823\n",
      "Epoch 21/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.9858 - binary_accuracy: 0.9489 - categorical_accuracy: 0.8136 - val_loss: 1.0914 - val_binary_accuracy: 0.9410 - val_categorical_accuracy: 0.8027\n",
      "Epoch 22/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.9509 - binary_accuracy: 0.9552 - categorical_accuracy: 0.8348 - val_loss: 1.0459 - val_binary_accuracy: 0.9490 - val_categorical_accuracy: 0.8095\n",
      "Epoch 23/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 0.9180 - binary_accuracy: 0.9586 - categorical_accuracy: 0.8394 - val_loss: 1.0243 - val_binary_accuracy: 0.9512 - val_categorical_accuracy: 0.8231\n",
      "Epoch 24/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.8985 - binary_accuracy: 0.9592 - categorical_accuracy: 0.8348 - val_loss: 0.9944 - val_binary_accuracy: 0.9524 - val_categorical_accuracy: 0.8299\n",
      "Epoch 25/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.8724 - binary_accuracy: 0.9599 - categorical_accuracy: 0.8417 - val_loss: 0.9761 - val_binary_accuracy: 0.9512 - val_categorical_accuracy: 0.8299\n",
      "Epoch 26/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.8548 - binary_accuracy: 0.9599 - categorical_accuracy: 0.8432 - val_loss: 0.9627 - val_binary_accuracy: 0.9563 - val_categorical_accuracy: 0.8299\n",
      "Epoch 27/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8311 - binary_accuracy: 0.9641 - categorical_accuracy: 0.8432 - val_loss: 0.9387 - val_binary_accuracy: 0.9563 - val_categorical_accuracy: 0.8299\n",
      "Epoch 28/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.8150 - binary_accuracy: 0.9645 - categorical_accuracy: 0.8402 - val_loss: 0.9282 - val_binary_accuracy: 0.9575 - val_categorical_accuracy: 0.8095\n",
      "Epoch 29/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7928 - binary_accuracy: 0.9655 - categorical_accuracy: 0.8477 - val_loss: 0.9100 - val_binary_accuracy: 0.9575 - val_categorical_accuracy: 0.8231\n",
      "Epoch 30/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7810 - binary_accuracy: 0.9655 - categorical_accuracy: 0.8462 - val_loss: 0.8996 - val_binary_accuracy: 0.9575 - val_categorical_accuracy: 0.8095\n",
      "Epoch 31/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7647 - binary_accuracy: 0.9652 - categorical_accuracy: 0.8553 - val_loss: 0.8852 - val_binary_accuracy: 0.9569 - val_categorical_accuracy: 0.8231\n",
      "Epoch 32/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7529 - binary_accuracy: 0.9653 - categorical_accuracy: 0.8545 - val_loss: 0.8655 - val_binary_accuracy: 0.9586 - val_categorical_accuracy: 0.8367\n",
      "Epoch 33/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7374 - binary_accuracy: 0.9652 - categorical_accuracy: 0.8553 - val_loss: 0.8583 - val_binary_accuracy: 0.9580 - val_categorical_accuracy: 0.8367\n",
      "Epoch 34/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7300 - binary_accuracy: 0.9648 - categorical_accuracy: 0.8576 - val_loss: 0.8504 - val_binary_accuracy: 0.9615 - val_categorical_accuracy: 0.8367\n",
      "Epoch 35/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7084 - binary_accuracy: 0.9737 - categorical_accuracy: 0.8568 - val_loss: 0.8499 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8367\n",
      "Epoch 36/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6956 - binary_accuracy: 0.9756 - categorical_accuracy: 0.8636 - val_loss: 0.8402 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8367\n",
      "Epoch 37/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6828 - binary_accuracy: 0.9773 - categorical_accuracy: 0.8636 - val_loss: 0.8078 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8367\n",
      "Epoch 38/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6778 - binary_accuracy: 0.9756 - categorical_accuracy: 0.8561 - val_loss: 0.8091 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 39/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6697 - binary_accuracy: 0.9758 - categorical_accuracy: 0.8614 - val_loss: 0.8241 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8367\n",
      "Epoch 40/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6624 - binary_accuracy: 0.9766 - categorical_accuracy: 0.8636 - val_loss: 0.8144 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8231\n",
      "Epoch 41/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6463 - binary_accuracy: 0.9773 - categorical_accuracy: 0.8636 - val_loss: 0.8047 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8163\n",
      "Epoch 42/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6362 - binary_accuracy: 0.9775 - categorical_accuracy: 0.8652 - val_loss: 0.7885 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8367\n",
      "Epoch 43/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6359 - binary_accuracy: 0.9765 - categorical_accuracy: 0.8614 - val_loss: 0.7909 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8367\n",
      "Epoch 44/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6190 - binary_accuracy: 0.9780 - categorical_accuracy: 0.8652 - val_loss: 0.7757 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8367\n",
      "Epoch 45/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6179 - binary_accuracy: 0.9773 - categorical_accuracy: 0.8621 - val_loss: 0.7932 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 46/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.6018 - binary_accuracy: 0.9784 - categorical_accuracy: 0.8689 - val_loss: 0.7601 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 47/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5961 - binary_accuracy: 0.9786 - categorical_accuracy: 0.8682 - val_loss: 0.7828 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 48/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5880 - binary_accuracy: 0.9783 - categorical_accuracy: 0.8682 - val_loss: 0.7576 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8367\n",
      "Epoch 49/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5831 - binary_accuracy: 0.9785 - categorical_accuracy: 0.8644 - val_loss: 0.7870 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8095\n",
      "Epoch 50/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5868 - binary_accuracy: 0.9781 - categorical_accuracy: 0.8697 - val_loss: 0.7785 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8231\n",
      "Epoch 51/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5756 - binary_accuracy: 0.9789 - categorical_accuracy: 0.8674 - val_loss: 0.7491 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 52/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5649 - binary_accuracy: 0.9793 - categorical_accuracy: 0.8705 - val_loss: 0.7675 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8231\n",
      "Epoch 53/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5566 - binary_accuracy: 0.9795 - categorical_accuracy: 0.8727 - val_loss: 0.7406 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8435\n",
      "Epoch 54/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5554 - binary_accuracy: 0.9794 - categorical_accuracy: 0.8712 - val_loss: 0.7484 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8231\n",
      "Epoch 55/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5387 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8750 - val_loss: 0.7464 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8299\n",
      "Epoch 56/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5503 - binary_accuracy: 0.9780 - categorical_accuracy: 0.8636 - val_loss: 0.7241 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 57/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5345 - binary_accuracy: 0.9794 - categorical_accuracy: 0.8727 - val_loss: 0.7334 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8435\n",
      "Epoch 58/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5244 - binary_accuracy: 0.9800 - categorical_accuracy: 0.8742 - val_loss: 0.7786 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8163\n",
      "Epoch 59/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5222 - binary_accuracy: 0.9795 - categorical_accuracy: 0.8735 - val_loss: 0.7302 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 60/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 0.5321 - binary_accuracy: 0.9789 - categorical_accuracy: 0.8689 - val_loss: 0.7032 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 61/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 0.5142 - binary_accuracy: 0.9793 - categorical_accuracy: 0.8735 - val_loss: 0.7175 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 62/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.5070 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8773 - val_loss: 0.7118 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 63/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5067 - binary_accuracy: 0.9797 - categorical_accuracy: 0.8727 - val_loss: 0.7204 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8367\n",
      "Epoch 64/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4959 - binary_accuracy: 0.9804 - categorical_accuracy: 0.8803 - val_loss: 0.7120 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8299\n",
      "Epoch 65/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4922 - binary_accuracy: 0.9804 - categorical_accuracy: 0.8742 - val_loss: 0.7122 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 66/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4911 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8758 - val_loss: 0.7211 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 67/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4886 - binary_accuracy: 0.9804 - categorical_accuracy: 0.8803 - val_loss: 0.7040 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 68/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4820 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8780 - val_loss: 0.6896 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 69/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4791 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8750 - val_loss: 0.7109 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8435\n",
      "Epoch 70/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4759 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8750 - val_loss: 0.6843 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 71/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4704 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8803 - val_loss: 0.7032 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 72/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4706 - binary_accuracy: 0.9806 - categorical_accuracy: 0.8780 - val_loss: 0.7037 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 73/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4742 - binary_accuracy: 0.9804 - categorical_accuracy: 0.8758 - val_loss: 0.6794 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 74/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4648 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8735 - val_loss: 0.6814 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 75/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4769 - binary_accuracy: 0.9796 - categorical_accuracy: 0.8720 - val_loss: 0.6897 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 76/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4707 - binary_accuracy: 0.9799 - categorical_accuracy: 0.8705 - val_loss: 0.6779 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8435\n",
      "Epoch 77/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4664 - binary_accuracy: 0.9798 - categorical_accuracy: 0.8735 - val_loss: 0.6853 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 78/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4497 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8788 - val_loss: 0.6875 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 79/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4409 - binary_accuracy: 0.9815 - categorical_accuracy: 0.8803 - val_loss: 0.6619 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8435\n",
      "Epoch 80/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4408 - binary_accuracy: 0.9810 - categorical_accuracy: 0.8758 - val_loss: 0.6498 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8503\n",
      "Epoch 81/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4440 - binary_accuracy: 0.9805 - categorical_accuracy: 0.8780 - val_loss: 0.6601 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 82/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4316 - binary_accuracy: 0.9813 - categorical_accuracy: 0.8811 - val_loss: 0.6582 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 83/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4444 - binary_accuracy: 0.9804 - categorical_accuracy: 0.8795 - val_loss: 0.6437 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 84/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4374 - binary_accuracy: 0.9809 - categorical_accuracy: 0.8765 - val_loss: 0.6536 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 85/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4348 - binary_accuracy: 0.9809 - categorical_accuracy: 0.8795 - val_loss: 0.6746 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8503\n",
      "Epoch 86/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4284 - binary_accuracy: 0.9812 - categorical_accuracy: 0.8826 - val_loss: 0.6679 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8367\n",
      "Epoch 87/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4273 - binary_accuracy: 0.9813 - categorical_accuracy: 0.8811 - val_loss: 0.6526 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 88/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4290 - binary_accuracy: 0.9808 - categorical_accuracy: 0.8795 - val_loss: 0.6700 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 89/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4166 - binary_accuracy: 0.9818 - categorical_accuracy: 0.8864 - val_loss: 0.6282 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8503\n",
      "Epoch 90/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4149 - binary_accuracy: 0.9815 - categorical_accuracy: 0.8879 - val_loss: 0.6540 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8503\n",
      "Epoch 91/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4113 - binary_accuracy: 0.9818 - categorical_accuracy: 0.8902 - val_loss: 0.6541 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8435\n",
      "Epoch 92/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4149 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8833 - val_loss: 0.6455 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 93/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4103 - binary_accuracy: 0.9818 - categorical_accuracy: 0.8841 - val_loss: 0.6457 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 94/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4026 - binary_accuracy: 0.9825 - categorical_accuracy: 0.8894 - val_loss: 0.6433 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 95/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3964 - binary_accuracy: 0.9823 - categorical_accuracy: 0.8924 - val_loss: 0.6531 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 96/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4042 - binary_accuracy: 0.9817 - categorical_accuracy: 0.8864 - val_loss: 0.6495 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8503\n",
      "Epoch 97/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3924 - binary_accuracy: 0.9820 - categorical_accuracy: 0.8894 - val_loss: 0.6570 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8503\n",
      "Epoch 98/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3931 - binary_accuracy: 0.9823 - categorical_accuracy: 0.8924 - val_loss: 0.6813 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 99/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3865 - binary_accuracy: 0.9827 - categorical_accuracy: 0.8947 - val_loss: 0.6322 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8571\n",
      "Epoch 100/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3879 - binary_accuracy: 0.9823 - categorical_accuracy: 0.8871 - val_loss: 0.6418 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 101/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3840 - binary_accuracy: 0.9823 - categorical_accuracy: 0.8902 - val_loss: 0.6403 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8571\n",
      "Epoch 102/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3800 - binary_accuracy: 0.9826 - categorical_accuracy: 0.8917 - val_loss: 0.6496 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8503\n",
      "Epoch 103/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3718 - binary_accuracy: 0.9833 - categorical_accuracy: 0.8947 - val_loss: 0.6220 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8571\n",
      "Epoch 104/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3760 - binary_accuracy: 0.9830 - categorical_accuracy: 0.8902 - val_loss: 0.6425 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8503\n",
      "Epoch 105/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3818 - binary_accuracy: 0.9827 - categorical_accuracy: 0.8894 - val_loss: 0.6468 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8571\n",
      "Epoch 106/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3662 - binary_accuracy: 0.9829 - categorical_accuracy: 0.8932 - val_loss: 0.6286 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 107/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3681 - binary_accuracy: 0.9831 - categorical_accuracy: 0.8992 - val_loss: 0.6354 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 108/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3664 - binary_accuracy: 0.9835 - categorical_accuracy: 0.9023 - val_loss: 0.6628 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8503\n",
      "Epoch 109/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3569 - binary_accuracy: 0.9835 - categorical_accuracy: 0.9015 - val_loss: 0.6446 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8571\n",
      "Epoch 110/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3582 - binary_accuracy: 0.9831 - categorical_accuracy: 0.9015 - val_loss: 0.6544 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8503\n",
      "Epoch 111/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3501 - binary_accuracy: 0.9840 - categorical_accuracy: 0.9030 - val_loss: 0.6535 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 112/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3409 - binary_accuracy: 0.9846 - categorical_accuracy: 0.9083 - val_loss: 0.6506 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 113/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3436 - binary_accuracy: 0.9843 - categorical_accuracy: 0.9061 - val_loss: 0.6637 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 114/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3430 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9053 - val_loss: 0.6531 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 115/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3421 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9061 - val_loss: 0.6554 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 116/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3407 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9008 - val_loss: 0.6869 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8435\n",
      "Epoch 117/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3375 - binary_accuracy: 0.9843 - categorical_accuracy: 0.9083 - val_loss: 0.6432 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 118/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3351 - binary_accuracy: 0.9841 - categorical_accuracy: 0.9038 - val_loss: 0.6581 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 119/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3368 - binary_accuracy: 0.9840 - categorical_accuracy: 0.9023 - val_loss: 0.6598 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 120/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3433 - binary_accuracy: 0.9834 - categorical_accuracy: 0.8992 - val_loss: 0.6244 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 121/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3316 - binary_accuracy: 0.9850 - categorical_accuracy: 0.9045 - val_loss: 0.6278 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 122/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3226 - binary_accuracy: 0.9847 - categorical_accuracy: 0.9091 - val_loss: 0.6333 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8435\n",
      "Epoch 123/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3332 - binary_accuracy: 0.9845 - categorical_accuracy: 0.9098 - val_loss: 0.6351 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8367\n",
      "Epoch 124/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3193 - binary_accuracy: 0.9859 - categorical_accuracy: 0.9129 - val_loss: 0.6299 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8435\n",
      "Epoch 125/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 0.3267 - binary_accuracy: 0.9849 - categorical_accuracy: 0.9068 - val_loss: 0.6531 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 126/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3233 - binary_accuracy: 0.9844 - categorical_accuracy: 0.9068 - val_loss: 0.6940 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 127/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3274 - binary_accuracy: 0.9845 - categorical_accuracy: 0.9061 - val_loss: 0.6527 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8503\n",
      "Epoch 128/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3103 - binary_accuracy: 0.9858 - categorical_accuracy: 0.9167 - val_loss: 0.6625 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 129/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3209 - binary_accuracy: 0.9844 - categorical_accuracy: 0.9053 - val_loss: 0.6634 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 130/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3323 - binary_accuracy: 0.9847 - categorical_accuracy: 0.9038 - val_loss: 0.6499 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 131/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3118 - binary_accuracy: 0.9849 - categorical_accuracy: 0.9091 - val_loss: 0.6395 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 132/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3110 - binary_accuracy: 0.9856 - categorical_accuracy: 0.9083 - val_loss: 0.6398 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 133/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3086 - binary_accuracy: 0.9854 - categorical_accuracy: 0.9114 - val_loss: 0.6350 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 134/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3160 - binary_accuracy: 0.9847 - categorical_accuracy: 0.9045 - val_loss: 0.6333 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8503\n",
      "Epoch 135/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 0.2996 - binary_accuracy: 0.9857 - categorical_accuracy: 0.9121 - val_loss: 0.6287 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8571\n",
      "Epoch 136/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3110 - binary_accuracy: 0.9851 - categorical_accuracy: 0.9068 - val_loss: 0.6495 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 137/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3049 - binary_accuracy: 0.9862 - categorical_accuracy: 0.9129 - val_loss: 0.6542 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 138/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3051 - binary_accuracy: 0.9848 - categorical_accuracy: 0.9076 - val_loss: 0.6314 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 139/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3065 - binary_accuracy: 0.9851 - categorical_accuracy: 0.9091 - val_loss: 0.6721 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8503\n",
      "Epoch 140/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2985 - binary_accuracy: 0.9863 - categorical_accuracy: 0.9152 - val_loss: 0.6533 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 141/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2979 - binary_accuracy: 0.9856 - categorical_accuracy: 0.9091 - val_loss: 0.6258 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8571\n",
      "Epoch 142/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2978 - binary_accuracy: 0.9856 - categorical_accuracy: 0.9091 - val_loss: 0.6410 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 143/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2988 - binary_accuracy: 0.9855 - categorical_accuracy: 0.9061 - val_loss: 0.6783 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 144/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2973 - binary_accuracy: 0.9858 - categorical_accuracy: 0.9114 - val_loss: 0.6415 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8571\n",
      "Epoch 145/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2940 - binary_accuracy: 0.9864 - categorical_accuracy: 0.9152 - val_loss: 0.6717 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 146/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3039 - binary_accuracy: 0.9855 - categorical_accuracy: 0.9068 - val_loss: 0.7151 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 147/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2984 - binary_accuracy: 0.9860 - categorical_accuracy: 0.9098 - val_loss: 0.6770 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 148/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2975 - binary_accuracy: 0.9859 - categorical_accuracy: 0.9114 - val_loss: 0.6748 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 149/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2927 - binary_accuracy: 0.9861 - categorical_accuracy: 0.9121 - val_loss: 0.6707 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 150/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2824 - binary_accuracy: 0.9868 - categorical_accuracy: 0.9182 - val_loss: 0.6451 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8571\n",
      "Epoch 151/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2808 - binary_accuracy: 0.9874 - categorical_accuracy: 0.9205 - val_loss: 0.6492 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8571\n",
      "Epoch 152/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2894 - binary_accuracy: 0.9860 - categorical_accuracy: 0.9106 - val_loss: 0.6718 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 153/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2875 - binary_accuracy: 0.9866 - categorical_accuracy: 0.9152 - val_loss: 0.6446 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 154/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2886 - binary_accuracy: 0.9851 - categorical_accuracy: 0.9106 - val_loss: 0.6989 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 155/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2895 - binary_accuracy: 0.9855 - categorical_accuracy: 0.9106 - val_loss: 0.6602 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8435\n",
      "Epoch 156/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2888 - binary_accuracy: 0.9861 - categorical_accuracy: 0.9129 - val_loss: 0.6580 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 157/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2745 - binary_accuracy: 0.9865 - categorical_accuracy: 0.9136 - val_loss: 0.6650 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 158/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2733 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9197 - val_loss: 0.6667 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8503\n",
      "Epoch 159/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2824 - binary_accuracy: 0.9864 - categorical_accuracy: 0.9159 - val_loss: 0.6701 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 160/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2851 - binary_accuracy: 0.9858 - categorical_accuracy: 0.9121 - val_loss: 0.7002 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 161/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2789 - binary_accuracy: 0.9864 - categorical_accuracy: 0.9144 - val_loss: 0.6677 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 162/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2802 - binary_accuracy: 0.9860 - categorical_accuracy: 0.9129 - val_loss: 0.6641 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 163/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2766 - binary_accuracy: 0.9866 - categorical_accuracy: 0.9197 - val_loss: 0.6773 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 164/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2675 - binary_accuracy: 0.9867 - categorical_accuracy: 0.9159 - val_loss: 0.6470 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 165/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2683 - binary_accuracy: 0.9870 - categorical_accuracy: 0.9182 - val_loss: 0.6554 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 166/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2708 - binary_accuracy: 0.9868 - categorical_accuracy: 0.9174 - val_loss: 0.6718 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 167/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2760 - binary_accuracy: 0.9862 - categorical_accuracy: 0.9136 - val_loss: 0.6859 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8231\n",
      "Epoch 168/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2663 - binary_accuracy: 0.9871 - categorical_accuracy: 0.9197 - val_loss: 0.6653 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 169/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2654 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9197 - val_loss: 0.6671 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8299\n",
      "Epoch 170/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2738 - binary_accuracy: 0.9864 - categorical_accuracy: 0.9144 - val_loss: 0.6796 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8231\n",
      "Epoch 171/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2876 - binary_accuracy: 0.9863 - categorical_accuracy: 0.9159 - val_loss: 0.6725 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8163\n",
      "Epoch 172/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2784 - binary_accuracy: 0.9860 - categorical_accuracy: 0.9159 - val_loss: 0.6897 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8163\n",
      "Epoch 173/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2689 - binary_accuracy: 0.9860 - categorical_accuracy: 0.9144 - val_loss: 0.7075 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8163\n",
      "Epoch 174/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2749 - binary_accuracy: 0.9863 - categorical_accuracy: 0.9129 - val_loss: 0.6952 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 175/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2629 - binary_accuracy: 0.9871 - categorical_accuracy: 0.9197 - val_loss: 0.6683 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8367\n",
      "Epoch 176/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2666 - binary_accuracy: 0.9865 - categorical_accuracy: 0.9152 - val_loss: 0.6391 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8503\n",
      "Epoch 177/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2575 - binary_accuracy: 0.9869 - categorical_accuracy: 0.9212 - val_loss: 0.6831 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8163\n",
      "Epoch 178/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2725 - binary_accuracy: 0.9851 - categorical_accuracy: 0.9068 - val_loss: 0.6738 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 179/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2587 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9205 - val_loss: 0.7102 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 180/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2594 - binary_accuracy: 0.9868 - categorical_accuracy: 0.9182 - val_loss: 0.6812 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 181/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2534 - binary_accuracy: 0.9869 - categorical_accuracy: 0.9182 - val_loss: 0.6760 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8231\n",
      "Epoch 182/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2491 - binary_accuracy: 0.9876 - categorical_accuracy: 0.9227 - val_loss: 0.6820 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 183/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2519 - binary_accuracy: 0.9878 - categorical_accuracy: 0.9235 - val_loss: 0.6687 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8299\n",
      "Epoch 184/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2517 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9205 - val_loss: 0.6693 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8299\n",
      "Epoch 185/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2558 - binary_accuracy: 0.9871 - categorical_accuracy: 0.9182 - val_loss: 0.6981 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8231\n",
      "Epoch 186/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2511 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9227 - val_loss: 0.6675 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8231\n",
      "Epoch 187/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2560 - binary_accuracy: 0.9864 - categorical_accuracy: 0.9182 - val_loss: 0.6515 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8299\n",
      "Epoch 188/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2524 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9197 - val_loss: 0.6797 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 189/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2503 - binary_accuracy: 0.9878 - categorical_accuracy: 0.9250 - val_loss: 0.6710 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 190/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2412 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9265 - val_loss: 0.6639 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8367\n",
      "Epoch 191/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2464 - binary_accuracy: 0.9871 - categorical_accuracy: 0.9197 - val_loss: 0.6642 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 192/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2405 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9227 - val_loss: 0.6819 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8163\n",
      "Epoch 193/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2414 - binary_accuracy: 0.9885 - categorical_accuracy: 0.9265 - val_loss: 0.6755 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 194/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2470 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9212 - val_loss: 0.6855 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8231\n",
      "Epoch 195/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2474 - binary_accuracy: 0.9870 - categorical_accuracy: 0.9197 - val_loss: 0.6970 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 196/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2454 - binary_accuracy: 0.9870 - categorical_accuracy: 0.9159 - val_loss: 0.6937 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 197/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2699 - binary_accuracy: 0.9868 - categorical_accuracy: 0.9159 - val_loss: 0.6941 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 198/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2487 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9189 - val_loss: 0.6674 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 199/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2377 - binary_accuracy: 0.9880 - categorical_accuracy: 0.9258 - val_loss: 0.6805 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 200/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2424 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9205 - val_loss: 0.6936 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 201/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2309 - binary_accuracy: 0.9882 - categorical_accuracy: 0.9273 - val_loss: 0.6745 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 202/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2431 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9174 - val_loss: 0.6716 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8231\n",
      "Epoch 203/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2446 - binary_accuracy: 0.9872 - categorical_accuracy: 0.9220 - val_loss: 0.6931 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 204/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2382 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9189 - val_loss: 0.6677 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 205/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2351 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9235 - val_loss: 0.7196 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8163\n",
      "Epoch 206/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2360 - binary_accuracy: 0.9885 - categorical_accuracy: 0.9288 - val_loss: 0.6792 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8231\n",
      "Epoch 207/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2299 - binary_accuracy: 0.9888 - categorical_accuracy: 0.9288 - val_loss: 0.7282 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8163\n",
      "Epoch 208/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2253 - binary_accuracy: 0.9889 - categorical_accuracy: 0.9326 - val_loss: 0.7581 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 209/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2325 - binary_accuracy: 0.9867 - categorical_accuracy: 0.9189 - val_loss: 0.6799 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 210/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 0.2429 - binary_accuracy: 0.9878 - categorical_accuracy: 0.9235 - val_loss: 0.7001 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 211/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2389 - binary_accuracy: 0.9877 - categorical_accuracy: 0.9212 - val_loss: 0.7143 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 212/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2335 - binary_accuracy: 0.9878 - categorical_accuracy: 0.9258 - val_loss: 0.7508 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 213/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2327 - binary_accuracy: 0.9874 - categorical_accuracy: 0.9212 - val_loss: 0.7387 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 214/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2302 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9258 - val_loss: 0.6906 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 215/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2289 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9242 - val_loss: 0.6869 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 216/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2276 - binary_accuracy: 0.9869 - categorical_accuracy: 0.9197 - val_loss: 0.6883 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 217/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2274 - binary_accuracy: 0.9886 - categorical_accuracy: 0.9250 - val_loss: 0.6642 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 218/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2303 - binary_accuracy: 0.9874 - categorical_accuracy: 0.9197 - val_loss: 0.7207 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 219/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2196 - binary_accuracy: 0.9890 - categorical_accuracy: 0.9303 - val_loss: 0.7377 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 220/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2231 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9242 - val_loss: 0.6700 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 221/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2262 - binary_accuracy: 0.9877 - categorical_accuracy: 0.9235 - val_loss: 0.7434 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 222/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2274 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9250 - val_loss: 0.7412 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8163\n",
      "Epoch 223/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2285 - binary_accuracy: 0.9878 - categorical_accuracy: 0.9250 - val_loss: 0.7225 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8231\n",
      "Epoch 224/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2226 - binary_accuracy: 0.9874 - categorical_accuracy: 0.9227 - val_loss: 0.6939 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8435\n",
      "Epoch 225/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2251 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9258 - val_loss: 0.7811 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7959\n",
      "Epoch 226/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2374 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9212 - val_loss: 0.6851 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 227/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2286 - binary_accuracy: 0.9874 - categorical_accuracy: 0.9220 - val_loss: 0.7085 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8299\n",
      "Epoch 228/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2252 - binary_accuracy: 0.9882 - categorical_accuracy: 0.9250 - val_loss: 0.7585 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 229/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2236 - binary_accuracy: 0.9880 - categorical_accuracy: 0.9273 - val_loss: 0.6525 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 230/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2283 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9250 - val_loss: 0.7394 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 231/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2202 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9273 - val_loss: 0.6681 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 232/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2291 - binary_accuracy: 0.9867 - categorical_accuracy: 0.9167 - val_loss: 0.7300 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 233/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2209 - binary_accuracy: 0.9886 - categorical_accuracy: 0.9280 - val_loss: 0.7730 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 234/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2246 - binary_accuracy: 0.9886 - categorical_accuracy: 0.9250 - val_loss: 0.7162 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 235/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2258 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9220 - val_loss: 0.7435 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8163\n",
      "Epoch 236/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2192 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9250 - val_loss: 0.7289 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 237/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2120 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9280 - val_loss: 0.7456 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 238/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2237 - binary_accuracy: 0.9880 - categorical_accuracy: 0.9280 - val_loss: 0.7701 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 239/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2216 - binary_accuracy: 0.9876 - categorical_accuracy: 0.9235 - val_loss: 0.7182 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 240/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2190 - binary_accuracy: 0.9875 - categorical_accuracy: 0.9227 - val_loss: 0.7145 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 241/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2111 - binary_accuracy: 0.9886 - categorical_accuracy: 0.9265 - val_loss: 0.7272 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 242/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2299 - binary_accuracy: 0.9876 - categorical_accuracy: 0.9227 - val_loss: 0.7261 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 243/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2156 - binary_accuracy: 0.9886 - categorical_accuracy: 0.9280 - val_loss: 0.7086 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 244/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2270 - binary_accuracy: 0.9878 - categorical_accuracy: 0.9258 - val_loss: 0.7949 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8163\n",
      "Epoch 245/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2175 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9265 - val_loss: 0.6797 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 246/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2222 - binary_accuracy: 0.9873 - categorical_accuracy: 0.9197 - val_loss: 0.6905 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8095\n",
      "Epoch 247/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2219 - binary_accuracy: 0.9866 - categorical_accuracy: 0.9197 - val_loss: 0.6855 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 248/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2129 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9280 - val_loss: 0.6700 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 249/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2130 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9258 - val_loss: 0.7243 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8095\n",
      "Epoch 250/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2207 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9242 - val_loss: 0.7625 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 251/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2082 - binary_accuracy: 0.9898 - categorical_accuracy: 0.9341 - val_loss: 0.7519 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 252/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2082 - binary_accuracy: 0.9888 - categorical_accuracy: 0.9311 - val_loss: 0.7093 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8231\n",
      "Epoch 253/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2181 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9280 - val_loss: 0.7146 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 254/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2054 - binary_accuracy: 0.9890 - categorical_accuracy: 0.9326 - val_loss: 0.7251 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 255/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2087 - binary_accuracy: 0.9880 - categorical_accuracy: 0.9273 - val_loss: 0.7779 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 256/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1990 - binary_accuracy: 0.9890 - categorical_accuracy: 0.9348 - val_loss: 0.7421 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8027\n",
      "Epoch 257/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2134 - binary_accuracy: 0.9880 - categorical_accuracy: 0.9280 - val_loss: 0.7442 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8095\n",
      "Epoch 258/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2118 - binary_accuracy: 0.9880 - categorical_accuracy: 0.9250 - val_loss: 0.7948 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8163\n",
      "Epoch 259/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2062 - binary_accuracy: 0.9891 - categorical_accuracy: 0.9311 - val_loss: 0.7190 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 260/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2179 - binary_accuracy: 0.9876 - categorical_accuracy: 0.9250 - val_loss: 0.7512 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8299\n",
      "Epoch 261/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2134 - binary_accuracy: 0.9888 - categorical_accuracy: 0.9303 - val_loss: 0.7519 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8231\n",
      "Epoch 262/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2119 - binary_accuracy: 0.9887 - categorical_accuracy: 0.9318 - val_loss: 0.6879 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 263/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2169 - binary_accuracy: 0.9885 - categorical_accuracy: 0.9288 - val_loss: 0.8079 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 264/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2055 - binary_accuracy: 0.9889 - categorical_accuracy: 0.9318 - val_loss: 0.6980 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 265/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2060 - binary_accuracy: 0.9889 - categorical_accuracy: 0.9295 - val_loss: 0.7467 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8163\n",
      "Epoch 266/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2167 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9265 - val_loss: 0.7764 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 267/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2073 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9273 - val_loss: 0.7407 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 268/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2082 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9303 - val_loss: 0.7178 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 269/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2064 - binary_accuracy: 0.9876 - categorical_accuracy: 0.9227 - val_loss: 0.7173 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 270/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2117 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9242 - val_loss: 0.7136 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8163\n",
      "Epoch 271/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2029 - binary_accuracy: 0.9891 - categorical_accuracy: 0.9311 - val_loss: 0.7399 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 272/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2083 - binary_accuracy: 0.9893 - categorical_accuracy: 0.9326 - val_loss: 0.7583 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 273/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2136 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9265 - val_loss: 0.7932 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 274/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2017 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9288 - val_loss: 0.7191 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 275/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2106 - binary_accuracy: 0.9879 - categorical_accuracy: 0.9235 - val_loss: 0.7259 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 276/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2139 - binary_accuracy: 0.9870 - categorical_accuracy: 0.9189 - val_loss: 0.7469 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8163\n",
      "Epoch 277/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2006 - binary_accuracy: 0.9891 - categorical_accuracy: 0.9333 - val_loss: 0.7414 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 278/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2063 - binary_accuracy: 0.9890 - categorical_accuracy: 0.9326 - val_loss: 0.7444 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 279/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2056 - binary_accuracy: 0.9888 - categorical_accuracy: 0.9295 - val_loss: 0.7471 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 280/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1946 - binary_accuracy: 0.9897 - categorical_accuracy: 0.9371 - val_loss: 0.7212 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 281/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2074 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9242 - val_loss: 0.7001 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8027\n",
      "Epoch 282/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2081 - binary_accuracy: 0.9881 - categorical_accuracy: 0.9295 - val_loss: 0.7840 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8163\n",
      "Epoch 283/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1976 - binary_accuracy: 0.9894 - categorical_accuracy: 0.9333 - val_loss: 0.7954 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8095\n",
      "Epoch 284/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1960 - binary_accuracy: 0.9886 - categorical_accuracy: 0.9318 - val_loss: 0.7295 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8299\n",
      "Epoch 285/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1985 - binary_accuracy: 0.9887 - categorical_accuracy: 0.9318 - val_loss: 0.7455 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8027\n",
      "Epoch 286/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1954 - binary_accuracy: 0.9891 - categorical_accuracy: 0.9341 - val_loss: 0.7689 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 287/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1929 - binary_accuracy: 0.9899 - categorical_accuracy: 0.9402 - val_loss: 0.7703 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 288/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2043 - binary_accuracy: 0.9891 - categorical_accuracy: 0.9326 - val_loss: 0.7560 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 289/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1956 - binary_accuracy: 0.9889 - categorical_accuracy: 0.9326 - val_loss: 0.7182 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 290/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.1983 - binary_accuracy: 0.9890 - categorical_accuracy: 0.9341 - val_loss: 0.7856 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8095\n",
      "Epoch 291/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2093 - binary_accuracy: 0.9892 - categorical_accuracy: 0.9333 - val_loss: 0.6913 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 292/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1964 - binary_accuracy: 0.9884 - categorical_accuracy: 0.9295 - val_loss: 0.7300 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8231\n",
      "Epoch 293/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.1969 - binary_accuracy: 0.9885 - categorical_accuracy: 0.9311 - val_loss: 0.6741 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "Epoch 294/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1846 - binary_accuracy: 0.9888 - categorical_accuracy: 0.9333 - val_loss: 0.7020 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8095\n",
      "Epoch 295/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2031 - binary_accuracy: 0.9891 - categorical_accuracy: 0.9341 - val_loss: 0.7994 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8163\n",
      "Epoch 296/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1991 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9288 - val_loss: 0.7146 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8299\n",
      "Epoch 297/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1975 - binary_accuracy: 0.9883 - categorical_accuracy: 0.9303 - val_loss: 0.7374 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 298/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.1938 - binary_accuracy: 0.9882 - categorical_accuracy: 0.9273 - val_loss: 0.7178 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 299/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.1875 - binary_accuracy: 0.9899 - categorical_accuracy: 0.9409 - val_loss: 0.7249 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8163\n",
      "Epoch 300/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.1922 - binary_accuracy: 0.9893 - categorical_accuracy: 0.9356 - val_loss: 0.7208 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8299\n",
      "[]\n",
      "12/12 [==============================] - 2s 147ms/step\n",
      "ACCURACY:\n",
      "0.7956403269754768\n",
      "\n",
      " [[60  0  0  1  0  0  0  1  4  0  0  1]\n",
      " [ 0 16  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 1  2  3 52  0  0  1  0  0  2  0  0]\n",
      " [ 1  0  3  0 54  0  0  1  0  0  0  0]\n",
      " [ 1  0  0  0  0  3  0  0  0  0  0  0]\n",
      " [ 2  0  0  0  0  2 50  0  0  0  0 18]\n",
      " [ 4  0  0  0  0  0  1 23  2  0  0  2]\n",
      " [ 0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  1  0  0  0  0  9  0  4  0  1 31]] \n",
      "\n",
      "Inside get_nlp_encodings\n",
      "The size of training and test sets: (1467, 11) (367, 11)\n",
      "Inside generate_nlp_features\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "True\n",
      "Inside encode_nlp_features\n",
      "Size after split: (1467, 151) (367, 151) (1467, 1) (367, 1)\n",
      "NLP data\n",
      "------------------\n",
      "Training set\n",
      "------------------\n",
      "[[[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]]\n",
      "(1467, 1, 151)\n",
      "NLP data\n",
      "------------------\n",
      "Test set\n",
      "------------------\n",
      "[[[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1 0 1 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 0 0 ... 0 0 0]]]\n",
      "(367, 1, 151)\n",
      "Output Labels\n",
      "------------------\n",
      "(1467, 1) (367, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "\n",
      "\n",
      " THESE ARE THE LAYERS\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_nlp (InputLayer)          [(None, 1, 151)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_idx (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 64)        47104       input_nlp[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti ((None, 150, 768),)  66362880    input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_attention1 (AttentionLayer [(None, 64), (None,  9966        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf_distil_bert_model_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 64)           0           sent_attention1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           49216       global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 12)           780         dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 12)           780         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24)           0           dense_6[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           300         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 66,471,026\n",
      "Trainable params: 61,042\n",
      "Non-trainable params: 66,409,984\n",
      "__________________________________________________________________________________________________\n",
      "Output Labels\n",
      "-------------------\n",
      "(1467,) (367,)\n",
      "(1467, 12) 1467\n",
      "(367, 12) 367\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 10s 227ms/step - loss: 2.3450 - binary_accuracy: 0.9167 - categorical_accuracy: 0.3765 - val_loss: 2.1874 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.5170\n",
      "Epoch 2/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 2.1272 - binary_accuracy: 0.9167 - categorical_accuracy: 0.5795 - val_loss: 2.0571 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6190\n",
      "Epoch 3/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 2.0119 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6068 - val_loss: 1.9618 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6190\n",
      "Epoch 4/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 1.9188 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6068 - val_loss: 1.8772 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6259\n",
      "Epoch 5/300\n",
      "42/42 [==============================] - 8s 195ms/step - loss: 1.8362 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6114 - val_loss: 1.8016 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6190\n",
      "Epoch 6/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.7629 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6220 - val_loss: 1.7427 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6599\n",
      "Epoch 7/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.6937 - binary_accuracy: 0.9167 - categorical_accuracy: 0.6955 - val_loss: 1.6771 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6871\n",
      "Epoch 8/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 1.6328 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7008 - val_loss: 1.6235 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6803\n",
      "Epoch 9/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 1.5768 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7038 - val_loss: 1.5742 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6803\n",
      "Epoch 10/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 1.5238 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7061 - val_loss: 1.5343 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6871\n",
      "Epoch 11/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.4769 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7076 - val_loss: 1.4901 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6871\n",
      "Epoch 12/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.4300 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7091 - val_loss: 1.4523 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6803\n",
      "Epoch 13/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.3911 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7076 - val_loss: 1.4110 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6939\n",
      "Epoch 14/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 1.3539 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7098 - val_loss: 1.3798 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6871\n",
      "Epoch 15/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.3163 - binary_accuracy: 0.9167 - categorical_accuracy: 0.7098 - val_loss: 1.3453 - val_binary_accuracy: 0.9167 - val_categorical_accuracy: 0.6871\n",
      "Epoch 16/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.2866 - binary_accuracy: 0.9182 - categorical_accuracy: 0.7106 - val_loss: 1.3079 - val_binary_accuracy: 0.9257 - val_categorical_accuracy: 0.6871\n",
      "Epoch 17/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.2565 - binary_accuracy: 0.9282 - categorical_accuracy: 0.7068 - val_loss: 1.2865 - val_binary_accuracy: 0.9325 - val_categorical_accuracy: 0.6939\n",
      "Epoch 18/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.2230 - binary_accuracy: 0.9351 - categorical_accuracy: 0.7106 - val_loss: 1.2614 - val_binary_accuracy: 0.9342 - val_categorical_accuracy: 0.6939\n",
      "Epoch 19/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.1961 - binary_accuracy: 0.9374 - categorical_accuracy: 0.7121 - val_loss: 1.2274 - val_binary_accuracy: 0.9359 - val_categorical_accuracy: 0.6871\n",
      "Epoch 20/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.1706 - binary_accuracy: 0.9382 - categorical_accuracy: 0.7098 - val_loss: 1.2080 - val_binary_accuracy: 0.9359 - val_categorical_accuracy: 0.6939\n",
      "Epoch 21/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.1454 - binary_accuracy: 0.9388 - categorical_accuracy: 0.7106 - val_loss: 1.1795 - val_binary_accuracy: 0.9359 - val_categorical_accuracy: 0.6939\n",
      "Epoch 22/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 1.1251 - binary_accuracy: 0.9390 - categorical_accuracy: 0.7098 - val_loss: 1.1610 - val_binary_accuracy: 0.9365 - val_categorical_accuracy: 0.6939\n",
      "Epoch 23/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.1021 - binary_accuracy: 0.9400 - categorical_accuracy: 0.7098 - val_loss: 1.1400 - val_binary_accuracy: 0.9354 - val_categorical_accuracy: 0.6939\n",
      "Epoch 24/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.0789 - binary_accuracy: 0.9422 - categorical_accuracy: 0.7129 - val_loss: 1.1218 - val_binary_accuracy: 0.9382 - val_categorical_accuracy: 0.6871\n",
      "Epoch 25/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.0609 - binary_accuracy: 0.9437 - categorical_accuracy: 0.7136 - val_loss: 1.1091 - val_binary_accuracy: 0.9393 - val_categorical_accuracy: 0.6871\n",
      "Epoch 26/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.0448 - binary_accuracy: 0.9438 - categorical_accuracy: 0.7174 - val_loss: 1.0897 - val_binary_accuracy: 0.9388 - val_categorical_accuracy: 0.6871\n",
      "Epoch 27/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.0255 - binary_accuracy: 0.9445 - categorical_accuracy: 0.7235 - val_loss: 1.0698 - val_binary_accuracy: 0.9410 - val_categorical_accuracy: 0.6939\n",
      "Epoch 28/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 1.0097 - binary_accuracy: 0.9446 - categorical_accuracy: 0.7311 - val_loss: 1.0602 - val_binary_accuracy: 0.9416 - val_categorical_accuracy: 0.7483\n",
      "Epoch 29/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.9963 - binary_accuracy: 0.9454 - categorical_accuracy: 0.7424 - val_loss: 1.0563 - val_binary_accuracy: 0.9416 - val_categorical_accuracy: 0.7347\n",
      "Epoch 30/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.9807 - binary_accuracy: 0.9458 - categorical_accuracy: 0.7439 - val_loss: 1.0268 - val_binary_accuracy: 0.9410 - val_categorical_accuracy: 0.7551\n",
      "Epoch 31/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.9654 - binary_accuracy: 0.9472 - categorical_accuracy: 0.7515 - val_loss: 1.0220 - val_binary_accuracy: 0.9410 - val_categorical_accuracy: 0.7483\n",
      "Epoch 32/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.9514 - binary_accuracy: 0.9472 - categorical_accuracy: 0.7538 - val_loss: 1.0009 - val_binary_accuracy: 0.9427 - val_categorical_accuracy: 0.7619\n",
      "Epoch 33/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.9415 - binary_accuracy: 0.9485 - categorical_accuracy: 0.7515 - val_loss: 0.9952 - val_binary_accuracy: 0.9433 - val_categorical_accuracy: 0.7619\n",
      "Epoch 34/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.9288 - binary_accuracy: 0.9487 - categorical_accuracy: 0.7553 - val_loss: 0.9751 - val_binary_accuracy: 0.9427 - val_categorical_accuracy: 0.7619\n",
      "Epoch 35/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.9154 - binary_accuracy: 0.9490 - categorical_accuracy: 0.7576 - val_loss: 0.9637 - val_binary_accuracy: 0.9427 - val_categorical_accuracy: 0.7619\n",
      "Epoch 36/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.9093 - binary_accuracy: 0.9512 - categorical_accuracy: 0.7545 - val_loss: 0.9561 - val_binary_accuracy: 0.9467 - val_categorical_accuracy: 0.7619\n",
      "Epoch 37/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8960 - binary_accuracy: 0.9525 - categorical_accuracy: 0.7568 - val_loss: 0.9458 - val_binary_accuracy: 0.9495 - val_categorical_accuracy: 0.7551\n",
      "Epoch 38/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8848 - binary_accuracy: 0.9547 - categorical_accuracy: 0.7576 - val_loss: 0.9292 - val_binary_accuracy: 0.9552 - val_categorical_accuracy: 0.7619\n",
      "Epoch 39/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8744 - binary_accuracy: 0.9563 - categorical_accuracy: 0.7583 - val_loss: 0.9244 - val_binary_accuracy: 0.9563 - val_categorical_accuracy: 0.7619\n",
      "Epoch 40/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8657 - binary_accuracy: 0.9580 - categorical_accuracy: 0.7568 - val_loss: 0.9157 - val_binary_accuracy: 0.9569 - val_categorical_accuracy: 0.7619\n",
      "Epoch 41/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8616 - binary_accuracy: 0.9586 - categorical_accuracy: 0.7561 - val_loss: 0.9156 - val_binary_accuracy: 0.9569 - val_categorical_accuracy: 0.7619\n",
      "Epoch 42/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8503 - binary_accuracy: 0.9597 - categorical_accuracy: 0.7583 - val_loss: 0.9018 - val_binary_accuracy: 0.9558 - val_categorical_accuracy: 0.7619\n",
      "Epoch 43/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8410 - binary_accuracy: 0.9610 - categorical_accuracy: 0.7598 - val_loss: 0.8953 - val_binary_accuracy: 0.9563 - val_categorical_accuracy: 0.7619\n",
      "Epoch 44/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8367 - binary_accuracy: 0.9616 - categorical_accuracy: 0.7583 - val_loss: 0.8899 - val_binary_accuracy: 0.9580 - val_categorical_accuracy: 0.7483\n",
      "Epoch 45/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.8300 - binary_accuracy: 0.9611 - categorical_accuracy: 0.7583 - val_loss: 0.8898 - val_binary_accuracy: 0.9569 - val_categorical_accuracy: 0.7551\n",
      "Epoch 46/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8269 - binary_accuracy: 0.9615 - categorical_accuracy: 0.7568 - val_loss: 0.8872 - val_binary_accuracy: 0.9575 - val_categorical_accuracy: 0.7551\n",
      "Epoch 47/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8191 - binary_accuracy: 0.9609 - categorical_accuracy: 0.7568 - val_loss: 0.9023 - val_binary_accuracy: 0.9580 - val_categorical_accuracy: 0.7347\n",
      "Epoch 48/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8126 - binary_accuracy: 0.9610 - categorical_accuracy: 0.7606 - val_loss: 0.8685 - val_binary_accuracy: 0.9586 - val_categorical_accuracy: 0.7551\n",
      "Epoch 49/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.8056 - binary_accuracy: 0.9625 - categorical_accuracy: 0.7591 - val_loss: 0.8709 - val_binary_accuracy: 0.9586 - val_categorical_accuracy: 0.7483\n",
      "Epoch 50/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.7996 - binary_accuracy: 0.9626 - categorical_accuracy: 0.7598 - val_loss: 0.8564 - val_binary_accuracy: 0.9609 - val_categorical_accuracy: 0.7551\n",
      "Epoch 51/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7980 - binary_accuracy: 0.9622 - categorical_accuracy: 0.7591 - val_loss: 0.8561 - val_binary_accuracy: 0.9620 - val_categorical_accuracy: 0.7551\n",
      "Epoch 52/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.7914 - binary_accuracy: 0.9622 - categorical_accuracy: 0.7591 - val_loss: 0.8473 - val_binary_accuracy: 0.9620 - val_categorical_accuracy: 0.7551\n",
      "Epoch 53/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.7800 - binary_accuracy: 0.9624 - categorical_accuracy: 0.7591 - val_loss: 0.8405 - val_binary_accuracy: 0.9626 - val_categorical_accuracy: 0.7551\n",
      "Epoch 54/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.7720 - binary_accuracy: 0.9633 - categorical_accuracy: 0.7598 - val_loss: 0.8436 - val_binary_accuracy: 0.9620 - val_categorical_accuracy: 0.7483\n",
      "Epoch 55/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7696 - binary_accuracy: 0.9614 - categorical_accuracy: 0.7583 - val_loss: 0.8356 - val_binary_accuracy: 0.9637 - val_categorical_accuracy: 0.7551\n",
      "Epoch 56/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.7670 - binary_accuracy: 0.9638 - categorical_accuracy: 0.7621 - val_loss: 0.8493 - val_binary_accuracy: 0.9632 - val_categorical_accuracy: 0.7415\n",
      "Epoch 57/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7573 - binary_accuracy: 0.9621 - categorical_accuracy: 0.7621 - val_loss: 0.8159 - val_binary_accuracy: 0.9643 - val_categorical_accuracy: 0.7551\n",
      "Epoch 58/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7588 - binary_accuracy: 0.9631 - categorical_accuracy: 0.7598 - val_loss: 0.8251 - val_binary_accuracy: 0.9637 - val_categorical_accuracy: 0.7483\n",
      "Epoch 59/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7504 - binary_accuracy: 0.9622 - categorical_accuracy: 0.7629 - val_loss: 0.8051 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7551\n",
      "Epoch 60/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7457 - binary_accuracy: 0.9630 - categorical_accuracy: 0.7674 - val_loss: 0.8264 - val_binary_accuracy: 0.9643 - val_categorical_accuracy: 0.7483\n",
      "Epoch 61/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7422 - binary_accuracy: 0.9628 - categorical_accuracy: 0.7712 - val_loss: 0.7993 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7687\n",
      "Epoch 62/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7435 - binary_accuracy: 0.9626 - categorical_accuracy: 0.7667 - val_loss: 0.8029 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7687\n",
      "Epoch 63/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7334 - binary_accuracy: 0.9628 - categorical_accuracy: 0.7705 - val_loss: 0.7997 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.7687\n",
      "Epoch 64/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.7300 - binary_accuracy: 0.9637 - categorical_accuracy: 0.7727 - val_loss: 0.8011 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7687\n",
      "Epoch 65/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7242 - binary_accuracy: 0.9645 - categorical_accuracy: 0.7742 - val_loss: 0.8034 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7619\n",
      "Epoch 66/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7172 - binary_accuracy: 0.9636 - categorical_accuracy: 0.7735 - val_loss: 0.7923 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7619\n",
      "Epoch 67/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7130 - binary_accuracy: 0.9641 - categorical_accuracy: 0.7720 - val_loss: 0.8012 - val_binary_accuracy: 0.9637 - val_categorical_accuracy: 0.7687\n",
      "Epoch 68/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.7014 - binary_accuracy: 0.9650 - categorical_accuracy: 0.7773 - val_loss: 0.7928 - val_binary_accuracy: 0.9643 - val_categorical_accuracy: 0.7619\n",
      "Epoch 69/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6922 - binary_accuracy: 0.9653 - categorical_accuracy: 0.7909 - val_loss: 0.7856 - val_binary_accuracy: 0.9654 - val_categorical_accuracy: 0.7619\n",
      "Epoch 70/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6843 - binary_accuracy: 0.9655 - categorical_accuracy: 0.7886 - val_loss: 0.7759 - val_binary_accuracy: 0.9643 - val_categorical_accuracy: 0.7687\n",
      "Epoch 71/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6849 - binary_accuracy: 0.9642 - categorical_accuracy: 0.7886 - val_loss: 0.7843 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7823\n",
      "Epoch 72/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6727 - binary_accuracy: 0.9657 - categorical_accuracy: 0.7955 - val_loss: 0.7750 - val_binary_accuracy: 0.9654 - val_categorical_accuracy: 0.7823\n",
      "Epoch 73/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6754 - binary_accuracy: 0.9656 - categorical_accuracy: 0.7848 - val_loss: 0.7793 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7823\n",
      "Epoch 74/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6668 - binary_accuracy: 0.9655 - categorical_accuracy: 0.7894 - val_loss: 0.7996 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7823\n",
      "Epoch 75/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6603 - binary_accuracy: 0.9664 - categorical_accuracy: 0.7924 - val_loss: 0.7833 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7755\n",
      "Epoch 76/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6602 - binary_accuracy: 0.9659 - categorical_accuracy: 0.7939 - val_loss: 0.7732 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7823\n",
      "Epoch 77/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6491 - binary_accuracy: 0.9662 - categorical_accuracy: 0.7902 - val_loss: 0.7799 - val_binary_accuracy: 0.9654 - val_categorical_accuracy: 0.7687\n",
      "Epoch 78/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6527 - binary_accuracy: 0.9662 - categorical_accuracy: 0.7864 - val_loss: 0.7738 - val_binary_accuracy: 0.9643 - val_categorical_accuracy: 0.7891\n",
      "Epoch 79/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6505 - binary_accuracy: 0.9655 - categorical_accuracy: 0.7924 - val_loss: 0.7620 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7891\n",
      "Epoch 80/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6361 - binary_accuracy: 0.9674 - categorical_accuracy: 0.7947 - val_loss: 0.7887 - val_binary_accuracy: 0.9643 - val_categorical_accuracy: 0.7823\n",
      "Epoch 81/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6399 - binary_accuracy: 0.9674 - categorical_accuracy: 0.7932 - val_loss: 0.7607 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7823\n",
      "Epoch 82/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 0.6319 - binary_accuracy: 0.9666 - categorical_accuracy: 0.7917 - val_loss: 0.7615 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7959\n",
      "Epoch 83/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 0.6283 - binary_accuracy: 0.9672 - categorical_accuracy: 0.7947 - val_loss: 0.7663 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7959\n",
      "Epoch 84/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6247 - binary_accuracy: 0.9677 - categorical_accuracy: 0.8023 - val_loss: 0.7451 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.8027\n",
      "Epoch 85/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6213 - binary_accuracy: 0.9680 - categorical_accuracy: 0.8030 - val_loss: 0.7622 - val_binary_accuracy: 0.9654 - val_categorical_accuracy: 0.7891\n",
      "Epoch 86/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6328 - binary_accuracy: 0.9655 - categorical_accuracy: 0.7886 - val_loss: 0.7722 - val_binary_accuracy: 0.9637 - val_categorical_accuracy: 0.7823\n",
      "Epoch 87/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6237 - binary_accuracy: 0.9674 - categorical_accuracy: 0.7992 - val_loss: 0.7423 - val_binary_accuracy: 0.9660 - val_categorical_accuracy: 0.7959\n",
      "Epoch 88/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6139 - binary_accuracy: 0.9683 - categorical_accuracy: 0.8038 - val_loss: 0.7849 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.7687\n",
      "Epoch 89/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.6121 - binary_accuracy: 0.9679 - categorical_accuracy: 0.7970 - val_loss: 0.7308 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7959\n",
      "Epoch 90/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.6096 - binary_accuracy: 0.9676 - categorical_accuracy: 0.7977 - val_loss: 0.7339 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.7959\n",
      "Epoch 91/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5955 - binary_accuracy: 0.9694 - categorical_accuracy: 0.8038 - val_loss: 0.7276 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8095\n",
      "Epoch 92/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5967 - binary_accuracy: 0.9686 - categorical_accuracy: 0.8030 - val_loss: 0.7502 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.7959\n",
      "Epoch 93/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5985 - binary_accuracy: 0.9696 - categorical_accuracy: 0.8076 - val_loss: 0.7539 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.7891\n",
      "Epoch 94/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5983 - binary_accuracy: 0.9692 - categorical_accuracy: 0.8068 - val_loss: 0.7857 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.7755\n",
      "Epoch 95/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5895 - binary_accuracy: 0.9692 - categorical_accuracy: 0.8038 - val_loss: 0.7451 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7891\n",
      "Epoch 96/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5877 - binary_accuracy: 0.9692 - categorical_accuracy: 0.8068 - val_loss: 0.7490 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7823\n",
      "Epoch 97/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5736 - binary_accuracy: 0.9699 - categorical_accuracy: 0.8136 - val_loss: 0.7198 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8095\n",
      "Epoch 98/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5860 - binary_accuracy: 0.9696 - categorical_accuracy: 0.8061 - val_loss: 0.7605 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.7891\n",
      "Epoch 99/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5896 - binary_accuracy: 0.9687 - categorical_accuracy: 0.8008 - val_loss: 0.7280 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7891\n",
      "Epoch 100/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5833 - binary_accuracy: 0.9686 - categorical_accuracy: 0.8045 - val_loss: 0.7115 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 101/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5635 - binary_accuracy: 0.9697 - categorical_accuracy: 0.8098 - val_loss: 0.7391 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7891\n",
      "Epoch 102/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5634 - binary_accuracy: 0.9695 - categorical_accuracy: 0.8159 - val_loss: 0.7130 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8095\n",
      "Epoch 103/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5732 - binary_accuracy: 0.9682 - categorical_accuracy: 0.8068 - val_loss: 0.7127 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8027\n",
      "Epoch 104/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5765 - binary_accuracy: 0.9687 - categorical_accuracy: 0.8076 - val_loss: 0.7388 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7959\n",
      "Epoch 105/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5665 - binary_accuracy: 0.9689 - categorical_accuracy: 0.8136 - val_loss: 0.7234 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 106/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5593 - binary_accuracy: 0.9693 - categorical_accuracy: 0.8114 - val_loss: 0.7144 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 107/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5593 - binary_accuracy: 0.9687 - categorical_accuracy: 0.8144 - val_loss: 0.7293 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7891\n",
      "Epoch 108/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5531 - binary_accuracy: 0.9693 - categorical_accuracy: 0.8114 - val_loss: 0.7399 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7891\n",
      "Epoch 109/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5495 - binary_accuracy: 0.9700 - categorical_accuracy: 0.8174 - val_loss: 0.7306 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7891\n",
      "Epoch 110/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5444 - binary_accuracy: 0.9699 - categorical_accuracy: 0.8205 - val_loss: 0.7342 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7891\n",
      "Epoch 111/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5433 - binary_accuracy: 0.9697 - categorical_accuracy: 0.8182 - val_loss: 0.7326 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7823\n",
      "Epoch 112/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5458 - binary_accuracy: 0.9695 - categorical_accuracy: 0.8152 - val_loss: 0.7177 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7959\n",
      "Epoch 113/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5458 - binary_accuracy: 0.9695 - categorical_accuracy: 0.8167 - val_loss: 0.7765 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7959\n",
      "Epoch 114/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5484 - binary_accuracy: 0.9693 - categorical_accuracy: 0.8144 - val_loss: 0.7705 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7891\n",
      "Epoch 115/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5377 - binary_accuracy: 0.9706 - categorical_accuracy: 0.8189 - val_loss: 0.7438 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7891\n",
      "Epoch 116/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5341 - binary_accuracy: 0.9705 - categorical_accuracy: 0.8152 - val_loss: 0.7574 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7891\n",
      "Epoch 117/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5422 - binary_accuracy: 0.9699 - categorical_accuracy: 0.8152 - val_loss: 0.7636 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7823\n",
      "Epoch 118/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5391 - binary_accuracy: 0.9710 - categorical_accuracy: 0.8220 - val_loss: 0.7331 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.8027\n",
      "Epoch 119/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5208 - binary_accuracy: 0.9706 - categorical_accuracy: 0.8189 - val_loss: 0.7396 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7959\n",
      "Epoch 120/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5307 - binary_accuracy: 0.9707 - categorical_accuracy: 0.8197 - val_loss: 0.7233 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.7891\n",
      "Epoch 121/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5297 - binary_accuracy: 0.9711 - categorical_accuracy: 0.8265 - val_loss: 0.7113 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8027\n",
      "Epoch 122/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5127 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8258 - val_loss: 0.6870 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8027\n",
      "Epoch 123/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5208 - binary_accuracy: 0.9714 - categorical_accuracy: 0.8242 - val_loss: 0.6972 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.7959\n",
      "Epoch 124/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5188 - binary_accuracy: 0.9708 - categorical_accuracy: 0.8235 - val_loss: 0.6841 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8095\n",
      "Epoch 125/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5258 - binary_accuracy: 0.9701 - categorical_accuracy: 0.8167 - val_loss: 0.7252 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8027\n",
      "Epoch 126/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5160 - binary_accuracy: 0.9706 - categorical_accuracy: 0.8235 - val_loss: 0.7188 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 127/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5105 - binary_accuracy: 0.9715 - categorical_accuracy: 0.8235 - val_loss: 0.7184 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 128/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5034 - binary_accuracy: 0.9722 - categorical_accuracy: 0.8303 - val_loss: 0.6942 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 129/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5185 - binary_accuracy: 0.9707 - categorical_accuracy: 0.8220 - val_loss: 0.6882 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8095\n",
      "Epoch 130/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5097 - binary_accuracy: 0.9707 - categorical_accuracy: 0.8250 - val_loss: 0.7021 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8095\n",
      "Epoch 131/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5044 - binary_accuracy: 0.9709 - categorical_accuracy: 0.8212 - val_loss: 0.6937 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 132/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5105 - binary_accuracy: 0.9701 - categorical_accuracy: 0.8212 - val_loss: 0.7242 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8027\n",
      "Epoch 133/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5131 - binary_accuracy: 0.9706 - categorical_accuracy: 0.8227 - val_loss: 0.7232 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.7891\n",
      "Epoch 134/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.5189 - binary_accuracy: 0.9717 - categorical_accuracy: 0.8295 - val_loss: 0.7253 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.7959\n",
      "Epoch 135/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4996 - binary_accuracy: 0.9726 - categorical_accuracy: 0.8303 - val_loss: 0.7168 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8027\n",
      "Epoch 136/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4980 - binary_accuracy: 0.9707 - categorical_accuracy: 0.8174 - val_loss: 0.7241 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8027\n",
      "Epoch 137/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4971 - binary_accuracy: 0.9715 - categorical_accuracy: 0.8295 - val_loss: 0.7136 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8027\n",
      "Epoch 138/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.5039 - binary_accuracy: 0.9708 - categorical_accuracy: 0.8189 - val_loss: 0.7105 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8027\n",
      "Epoch 139/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4988 - binary_accuracy: 0.9705 - categorical_accuracy: 0.8197 - val_loss: 0.7017 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8027\n",
      "Epoch 140/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4998 - binary_accuracy: 0.9699 - categorical_accuracy: 0.8174 - val_loss: 0.7069 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8095\n",
      "Epoch 141/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4908 - binary_accuracy: 0.9725 - categorical_accuracy: 0.8280 - val_loss: 0.6889 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8095\n",
      "Epoch 142/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4920 - binary_accuracy: 0.9729 - categorical_accuracy: 0.8326 - val_loss: 0.7237 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.7823\n",
      "Epoch 143/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4985 - binary_accuracy: 0.9717 - categorical_accuracy: 0.8288 - val_loss: 0.7176 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 144/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4851 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8303 - val_loss: 0.6924 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8027\n",
      "Epoch 145/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4892 - binary_accuracy: 0.9714 - categorical_accuracy: 0.8311 - val_loss: 0.7022 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8027\n",
      "Epoch 146/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4815 - binary_accuracy: 0.9721 - categorical_accuracy: 0.8295 - val_loss: 0.7157 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 147/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4868 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8318 - val_loss: 0.6965 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 148/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4803 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8326 - val_loss: 0.7407 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.7959\n",
      "Epoch 149/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4778 - binary_accuracy: 0.9727 - categorical_accuracy: 0.8341 - val_loss: 0.7329 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8027\n",
      "Epoch 150/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4822 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8280 - val_loss: 0.7264 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8095\n",
      "Epoch 151/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4817 - binary_accuracy: 0.9711 - categorical_accuracy: 0.8265 - val_loss: 0.7151 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 152/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4843 - binary_accuracy: 0.9722 - categorical_accuracy: 0.8318 - val_loss: 0.7097 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8095\n",
      "Epoch 153/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4929 - binary_accuracy: 0.9710 - categorical_accuracy: 0.8212 - val_loss: 0.7349 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 154/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4906 - binary_accuracy: 0.9699 - categorical_accuracy: 0.8205 - val_loss: 0.7098 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8027\n",
      "Epoch 155/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4810 - binary_accuracy: 0.9711 - categorical_accuracy: 0.8227 - val_loss: 0.7052 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8027\n",
      "Epoch 156/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4784 - binary_accuracy: 0.9722 - categorical_accuracy: 0.8311 - val_loss: 0.6846 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 157/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4780 - binary_accuracy: 0.9729 - categorical_accuracy: 0.8348 - val_loss: 0.7383 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 158/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4780 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8326 - val_loss: 0.6939 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 159/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4831 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8280 - val_loss: 0.7304 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 160/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4728 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8295 - val_loss: 0.6701 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8231\n",
      "Epoch 161/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4809 - binary_accuracy: 0.9722 - categorical_accuracy: 0.8295 - val_loss: 0.6966 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8095\n",
      "Epoch 162/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4734 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8303 - val_loss: 0.6922 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.7959\n",
      "Epoch 163/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4773 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8258 - val_loss: 0.6856 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 164/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4836 - binary_accuracy: 0.9712 - categorical_accuracy: 0.8227 - val_loss: 0.6841 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 165/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4773 - binary_accuracy: 0.9712 - categorical_accuracy: 0.8280 - val_loss: 0.7273 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.7959\n",
      "Epoch 166/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4763 - binary_accuracy: 0.9722 - categorical_accuracy: 0.8303 - val_loss: 0.7030 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8027\n",
      "Epoch 167/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4792 - binary_accuracy: 0.9722 - categorical_accuracy: 0.8311 - val_loss: 0.7014 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 168/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.4753 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8341 - val_loss: 0.7282 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8027\n",
      "Epoch 169/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4687 - binary_accuracy: 0.9717 - categorical_accuracy: 0.8242 - val_loss: 0.6799 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8095\n",
      "Epoch 170/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4866 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8295 - val_loss: 0.6921 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 171/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4659 - binary_accuracy: 0.9727 - categorical_accuracy: 0.8348 - val_loss: 0.6955 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8095\n",
      "Epoch 172/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4649 - binary_accuracy: 0.9729 - categorical_accuracy: 0.8303 - val_loss: 0.6795 - val_binary_accuracy: 0.9700 - val_categorical_accuracy: 0.8163\n",
      "Epoch 173/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4630 - binary_accuracy: 0.9729 - categorical_accuracy: 0.8333 - val_loss: 0.6950 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8095\n",
      "Epoch 174/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4630 - binary_accuracy: 0.9721 - categorical_accuracy: 0.8288 - val_loss: 0.7035 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8095\n",
      "Epoch 175/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4682 - binary_accuracy: 0.9717 - categorical_accuracy: 0.8227 - val_loss: 0.7033 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.7959\n",
      "Epoch 176/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4526 - binary_accuracy: 0.9726 - categorical_accuracy: 0.8341 - val_loss: 0.7106 - val_binary_accuracy: 0.9649 - val_categorical_accuracy: 0.7959\n",
      "Epoch 177/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4651 - binary_accuracy: 0.9725 - categorical_accuracy: 0.8318 - val_loss: 0.7071 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8027\n",
      "Epoch 178/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4747 - binary_accuracy: 0.9709 - categorical_accuracy: 0.8242 - val_loss: 0.7255 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8027\n",
      "Epoch 179/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4677 - binary_accuracy: 0.9727 - categorical_accuracy: 0.8326 - val_loss: 0.6818 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8027\n",
      "Epoch 180/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4681 - binary_accuracy: 0.9705 - categorical_accuracy: 0.8167 - val_loss: 0.6904 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 181/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4534 - binary_accuracy: 0.9727 - categorical_accuracy: 0.8318 - val_loss: 0.6971 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 182/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4493 - binary_accuracy: 0.9732 - categorical_accuracy: 0.8394 - val_loss: 0.6883 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 183/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4581 - binary_accuracy: 0.9732 - categorical_accuracy: 0.8333 - val_loss: 0.7036 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8163\n",
      "Epoch 184/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4548 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8326 - val_loss: 0.6764 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8027\n",
      "Epoch 185/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 0.4496 - binary_accuracy: 0.9731 - categorical_accuracy: 0.8379 - val_loss: 0.6899 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8095\n",
      "Epoch 186/300\n",
      "42/42 [==============================] - 8s 191ms/step - loss: 0.4570 - binary_accuracy: 0.9723 - categorical_accuracy: 0.8265 - val_loss: 0.6804 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8095\n",
      "Epoch 187/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4569 - binary_accuracy: 0.9718 - categorical_accuracy: 0.8318 - val_loss: 0.6563 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8095\n",
      "Epoch 188/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4478 - binary_accuracy: 0.9732 - categorical_accuracy: 0.8364 - val_loss: 0.6872 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8231\n",
      "Epoch 189/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4459 - binary_accuracy: 0.9727 - categorical_accuracy: 0.8371 - val_loss: 0.6867 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8163\n",
      "Epoch 190/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4354 - binary_accuracy: 0.9740 - categorical_accuracy: 0.8462 - val_loss: 0.6649 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8027\n",
      "Epoch 191/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4539 - binary_accuracy: 0.9733 - categorical_accuracy: 0.8386 - val_loss: 0.7092 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8163\n",
      "Epoch 192/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4474 - binary_accuracy: 0.9749 - categorical_accuracy: 0.8424 - val_loss: 0.6821 - val_binary_accuracy: 0.9683 - val_categorical_accuracy: 0.8027\n",
      "Epoch 193/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4415 - binary_accuracy: 0.9754 - categorical_accuracy: 0.8477 - val_loss: 0.6637 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.8095\n",
      "Epoch 194/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4249 - binary_accuracy: 0.9747 - categorical_accuracy: 0.8439 - val_loss: 0.6698 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8095\n",
      "Epoch 195/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4329 - binary_accuracy: 0.9749 - categorical_accuracy: 0.8470 - val_loss: 0.6809 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.7959\n",
      "Epoch 196/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4205 - binary_accuracy: 0.9762 - categorical_accuracy: 0.8545 - val_loss: 0.6703 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8095\n",
      "Epoch 197/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4127 - binary_accuracy: 0.9761 - categorical_accuracy: 0.8545 - val_loss: 0.6535 - val_binary_accuracy: 0.9677 - val_categorical_accuracy: 0.8095\n",
      "Epoch 198/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4181 - binary_accuracy: 0.9753 - categorical_accuracy: 0.8508 - val_loss: 0.6563 - val_binary_accuracy: 0.9666 - val_categorical_accuracy: 0.8095\n",
      "Epoch 199/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4126 - binary_accuracy: 0.9767 - categorical_accuracy: 0.8591 - val_loss: 0.6508 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8231\n",
      "Epoch 200/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.4137 - binary_accuracy: 0.9756 - categorical_accuracy: 0.8553 - val_loss: 0.6647 - val_binary_accuracy: 0.9671 - val_categorical_accuracy: 0.8095\n",
      "Epoch 201/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4107 - binary_accuracy: 0.9768 - categorical_accuracy: 0.8583 - val_loss: 0.6403 - val_binary_accuracy: 0.9694 - val_categorical_accuracy: 0.8163\n",
      "Epoch 202/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.4081 - binary_accuracy: 0.9769 - categorical_accuracy: 0.8621 - val_loss: 0.6334 - val_binary_accuracy: 0.9688 - val_categorical_accuracy: 0.8027\n",
      "Epoch 203/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3912 - binary_accuracy: 0.9778 - categorical_accuracy: 0.8621 - val_loss: 0.6369 - val_binary_accuracy: 0.9705 - val_categorical_accuracy: 0.8367\n",
      "Epoch 204/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3914 - binary_accuracy: 0.9784 - categorical_accuracy: 0.8674 - val_loss: 0.6437 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8163\n",
      "Epoch 205/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3915 - binary_accuracy: 0.9782 - categorical_accuracy: 0.8659 - val_loss: 0.6169 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8231\n",
      "Epoch 206/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3855 - binary_accuracy: 0.9790 - categorical_accuracy: 0.8689 - val_loss: 0.6119 - val_binary_accuracy: 0.9717 - val_categorical_accuracy: 0.8299\n",
      "Epoch 207/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3786 - binary_accuracy: 0.9804 - categorical_accuracy: 0.8811 - val_loss: 0.6057 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8299\n",
      "Epoch 208/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3671 - binary_accuracy: 0.9806 - categorical_accuracy: 0.8795 - val_loss: 0.6045 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8231\n",
      "Epoch 209/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3639 - binary_accuracy: 0.9806 - categorical_accuracy: 0.8795 - val_loss: 0.6343 - val_binary_accuracy: 0.9711 - val_categorical_accuracy: 0.8231\n",
      "Epoch 210/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3648 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8803 - val_loss: 0.5871 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8367\n",
      "Epoch 211/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3663 - binary_accuracy: 0.9801 - categorical_accuracy: 0.8765 - val_loss: 0.6237 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8367\n",
      "Epoch 212/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3634 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8773 - val_loss: 0.6040 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 213/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3577 - binary_accuracy: 0.9808 - categorical_accuracy: 0.8780 - val_loss: 0.6111 - val_binary_accuracy: 0.9722 - val_categorical_accuracy: 0.8367\n",
      "Epoch 214/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3499 - binary_accuracy: 0.9812 - categorical_accuracy: 0.8818 - val_loss: 0.6019 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8435\n",
      "Epoch 215/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3493 - binary_accuracy: 0.9814 - categorical_accuracy: 0.8826 - val_loss: 0.5970 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 216/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3493 - binary_accuracy: 0.9819 - categorical_accuracy: 0.8856 - val_loss: 0.5945 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8435\n",
      "Epoch 217/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3510 - binary_accuracy: 0.9807 - categorical_accuracy: 0.8780 - val_loss: 0.5955 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8231\n",
      "Epoch 218/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3409 - binary_accuracy: 0.9811 - categorical_accuracy: 0.8841 - val_loss: 0.6010 - val_binary_accuracy: 0.9728 - val_categorical_accuracy: 0.8299\n",
      "Epoch 219/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3437 - binary_accuracy: 0.9816 - categorical_accuracy: 0.8803 - val_loss: 0.5695 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8435\n",
      "Epoch 220/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3399 - binary_accuracy: 0.9816 - categorical_accuracy: 0.8818 - val_loss: 0.5501 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8571\n",
      "Epoch 221/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3321 - binary_accuracy: 0.9819 - categorical_accuracy: 0.8864 - val_loss: 0.5801 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 222/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3278 - binary_accuracy: 0.9816 - categorical_accuracy: 0.8871 - val_loss: 0.5410 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 223/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3275 - binary_accuracy: 0.9815 - categorical_accuracy: 0.8826 - val_loss: 0.5706 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 224/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3241 - binary_accuracy: 0.9817 - categorical_accuracy: 0.8864 - val_loss: 0.5618 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 225/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3231 - binary_accuracy: 0.9816 - categorical_accuracy: 0.8856 - val_loss: 0.5370 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8503\n",
      "Epoch 226/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3340 - binary_accuracy: 0.9816 - categorical_accuracy: 0.8841 - val_loss: 0.5521 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 227/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3288 - binary_accuracy: 0.9822 - categorical_accuracy: 0.8871 - val_loss: 0.5576 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 228/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3240 - binary_accuracy: 0.9819 - categorical_accuracy: 0.8909 - val_loss: 0.5492 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 229/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3146 - binary_accuracy: 0.9818 - categorical_accuracy: 0.8917 - val_loss: 0.5638 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 230/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3141 - binary_accuracy: 0.9824 - categorical_accuracy: 0.8962 - val_loss: 0.6103 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8299\n",
      "Epoch 231/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3197 - binary_accuracy: 0.9819 - categorical_accuracy: 0.8902 - val_loss: 0.5505 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 232/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3184 - binary_accuracy: 0.9823 - categorical_accuracy: 0.8917 - val_loss: 0.5532 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8503\n",
      "Epoch 233/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3192 - binary_accuracy: 0.9811 - categorical_accuracy: 0.8826 - val_loss: 0.5612 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 234/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3165 - binary_accuracy: 0.9821 - categorical_accuracy: 0.8902 - val_loss: 0.5612 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8367\n",
      "Epoch 235/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3151 - binary_accuracy: 0.9820 - categorical_accuracy: 0.8924 - val_loss: 0.5714 - val_binary_accuracy: 0.9739 - val_categorical_accuracy: 0.8435\n",
      "Epoch 236/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3097 - binary_accuracy: 0.9818 - categorical_accuracy: 0.8886 - val_loss: 0.5541 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 237/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3098 - binary_accuracy: 0.9828 - categorical_accuracy: 0.8939 - val_loss: 0.5625 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 238/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3143 - binary_accuracy: 0.9824 - categorical_accuracy: 0.8894 - val_loss: 0.5544 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 239/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3115 - binary_accuracy: 0.9821 - categorical_accuracy: 0.8932 - val_loss: 0.5526 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 240/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.3075 - binary_accuracy: 0.9821 - categorical_accuracy: 0.8917 - val_loss: 0.5301 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 241/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3013 - binary_accuracy: 0.9824 - categorical_accuracy: 0.8917 - val_loss: 0.5046 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 242/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2997 - binary_accuracy: 0.9822 - categorical_accuracy: 0.8932 - val_loss: 0.4893 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 243/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.3101 - binary_accuracy: 0.9819 - categorical_accuracy: 0.8909 - val_loss: 0.5151 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 244/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3001 - binary_accuracy: 0.9824 - categorical_accuracy: 0.8955 - val_loss: 0.5105 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8571\n",
      "Epoch 245/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2966 - binary_accuracy: 0.9827 - categorical_accuracy: 0.9015 - val_loss: 0.5312 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8571\n",
      "Epoch 246/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2932 - binary_accuracy: 0.9828 - categorical_accuracy: 0.8970 - val_loss: 0.5398 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 247/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.3028 - binary_accuracy: 0.9824 - categorical_accuracy: 0.8970 - val_loss: 0.5680 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8367\n",
      "Epoch 248/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2977 - binary_accuracy: 0.9821 - categorical_accuracy: 0.8924 - val_loss: 0.5468 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8435\n",
      "Epoch 249/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2969 - binary_accuracy: 0.9824 - categorical_accuracy: 0.8947 - val_loss: 0.5174 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8639\n",
      "Epoch 250/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2990 - binary_accuracy: 0.9828 - categorical_accuracy: 0.8955 - val_loss: 0.5327 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 251/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2901 - binary_accuracy: 0.9831 - categorical_accuracy: 0.8955 - val_loss: 0.5348 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 252/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2793 - binary_accuracy: 0.9833 - categorical_accuracy: 0.9015 - val_loss: 0.5184 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 253/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2919 - binary_accuracy: 0.9823 - categorical_accuracy: 0.8924 - val_loss: 0.5016 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8571\n",
      "Epoch 254/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2821 - binary_accuracy: 0.9835 - categorical_accuracy: 0.9015 - val_loss: 0.5266 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 255/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2930 - binary_accuracy: 0.9828 - categorical_accuracy: 0.8970 - val_loss: 0.5382 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8435\n",
      "Epoch 256/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2890 - binary_accuracy: 0.9834 - categorical_accuracy: 0.8955 - val_loss: 0.5307 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 257/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2811 - binary_accuracy: 0.9830 - categorical_accuracy: 0.9015 - val_loss: 0.5320 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 258/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2827 - binary_accuracy: 0.9834 - categorical_accuracy: 0.9008 - val_loss: 0.5275 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8367\n",
      "Epoch 259/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2850 - binary_accuracy: 0.9833 - categorical_accuracy: 0.9000 - val_loss: 0.5116 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8639\n",
      "Epoch 260/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2869 - binary_accuracy: 0.9831 - categorical_accuracy: 0.8962 - val_loss: 0.5053 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 261/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2843 - binary_accuracy: 0.9835 - categorical_accuracy: 0.8985 - val_loss: 0.5087 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 262/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2907 - binary_accuracy: 0.9828 - categorical_accuracy: 0.8962 - val_loss: 0.5627 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8367\n",
      "Epoch 263/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2811 - binary_accuracy: 0.9830 - categorical_accuracy: 0.8955 - val_loss: 0.5023 - val_binary_accuracy: 0.9785 - val_categorical_accuracy: 0.8639\n",
      "Epoch 264/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2767 - binary_accuracy: 0.9836 - categorical_accuracy: 0.9023 - val_loss: 0.5102 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8435\n",
      "Epoch 265/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2772 - binary_accuracy: 0.9831 - categorical_accuracy: 0.8992 - val_loss: 0.5203 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8435\n",
      "Epoch 266/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2764 - binary_accuracy: 0.9837 - categorical_accuracy: 0.9030 - val_loss: 0.5346 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 267/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2849 - binary_accuracy: 0.9834 - categorical_accuracy: 0.8992 - val_loss: 0.5487 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 268/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2738 - binary_accuracy: 0.9839 - categorical_accuracy: 0.9015 - val_loss: 0.5224 - val_binary_accuracy: 0.9745 - val_categorical_accuracy: 0.8367\n",
      "Epoch 269/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2738 - binary_accuracy: 0.9836 - categorical_accuracy: 0.9045 - val_loss: 0.5159 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 270/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2724 - binary_accuracy: 0.9827 - categorical_accuracy: 0.8962 - val_loss: 0.5110 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 271/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2716 - binary_accuracy: 0.9836 - categorical_accuracy: 0.8985 - val_loss: 0.5222 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8435\n",
      "Epoch 272/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2737 - binary_accuracy: 0.9831 - categorical_accuracy: 0.9023 - val_loss: 0.5688 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 273/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2780 - binary_accuracy: 0.9841 - categorical_accuracy: 0.9030 - val_loss: 0.5782 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8503\n",
      "Epoch 274/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2865 - binary_accuracy: 0.9830 - categorical_accuracy: 0.8970 - val_loss: 0.5578 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 275/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2742 - binary_accuracy: 0.9831 - categorical_accuracy: 0.9008 - val_loss: 0.5796 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 276/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2689 - binary_accuracy: 0.9840 - categorical_accuracy: 0.9038 - val_loss: 0.5560 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 277/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2825 - binary_accuracy: 0.9828 - categorical_accuracy: 0.8970 - val_loss: 0.5838 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 278/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2743 - binary_accuracy: 0.9835 - categorical_accuracy: 0.8985 - val_loss: 0.5457 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 279/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2693 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9053 - val_loss: 0.5235 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8571\n",
      "Epoch 280/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2673 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9038 - val_loss: 0.5295 - val_binary_accuracy: 0.9779 - val_categorical_accuracy: 0.8639\n",
      "Epoch 281/300\n",
      "42/42 [==============================] - 8s 194ms/step - loss: 0.2648 - binary_accuracy: 0.9840 - categorical_accuracy: 0.9030 - val_loss: 0.5498 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8571\n",
      "Epoch 282/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2657 - binary_accuracy: 0.9836 - categorical_accuracy: 0.9000 - val_loss: 0.5490 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8571\n",
      "Epoch 283/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2652 - binary_accuracy: 0.9833 - categorical_accuracy: 0.8985 - val_loss: 0.5471 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8639\n",
      "Epoch 284/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2603 - binary_accuracy: 0.9847 - categorical_accuracy: 0.9106 - val_loss: 0.5604 - val_binary_accuracy: 0.9756 - val_categorical_accuracy: 0.8503\n",
      "Epoch 285/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2728 - binary_accuracy: 0.9835 - categorical_accuracy: 0.9023 - val_loss: 0.5214 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8571\n",
      "Epoch 286/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2598 - binary_accuracy: 0.9843 - categorical_accuracy: 0.9053 - val_loss: 0.5277 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8639\n",
      "Epoch 287/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2595 - binary_accuracy: 0.9845 - categorical_accuracy: 0.9083 - val_loss: 0.5285 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8503\n",
      "Epoch 288/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2581 - binary_accuracy: 0.9846 - categorical_accuracy: 0.9076 - val_loss: 0.5472 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 289/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2701 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9008 - val_loss: 0.5596 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 290/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2680 - binary_accuracy: 0.9832 - categorical_accuracy: 0.8992 - val_loss: 0.5324 - val_binary_accuracy: 0.9779 - val_categorical_accuracy: 0.8571\n",
      "Epoch 291/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2687 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9045 - val_loss: 0.5292 - val_binary_accuracy: 0.9779 - val_categorical_accuracy: 0.8571\n",
      "Epoch 292/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2698 - binary_accuracy: 0.9827 - categorical_accuracy: 0.8947 - val_loss: 0.5733 - val_binary_accuracy: 0.9734 - val_categorical_accuracy: 0.8367\n",
      "Epoch 293/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2770 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9008 - val_loss: 0.5626 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8503\n",
      "Epoch 294/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2629 - binary_accuracy: 0.9838 - categorical_accuracy: 0.9023 - val_loss: 0.5636 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8503\n",
      "Epoch 295/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2647 - binary_accuracy: 0.9835 - categorical_accuracy: 0.9008 - val_loss: 0.5721 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8571\n",
      "Epoch 296/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2589 - binary_accuracy: 0.9842 - categorical_accuracy: 0.9053 - val_loss: 0.5650 - val_binary_accuracy: 0.9768 - val_categorical_accuracy: 0.8503\n",
      "Epoch 297/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2572 - binary_accuracy: 0.9846 - categorical_accuracy: 0.9061 - val_loss: 0.5807 - val_binary_accuracy: 0.9762 - val_categorical_accuracy: 0.8435\n",
      "Epoch 298/300\n",
      "42/42 [==============================] - 8s 193ms/step - loss: 0.2518 - binary_accuracy: 0.9844 - categorical_accuracy: 0.9061 - val_loss: 0.5485 - val_binary_accuracy: 0.9773 - val_categorical_accuracy: 0.8707\n",
      "Epoch 299/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2487 - binary_accuracy: 0.9852 - categorical_accuracy: 0.9098 - val_loss: 0.5565 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "Epoch 300/300\n",
      "42/42 [==============================] - 8s 192ms/step - loss: 0.2569 - binary_accuracy: 0.9845 - categorical_accuracy: 0.9045 - val_loss: 0.5790 - val_binary_accuracy: 0.9751 - val_categorical_accuracy: 0.8435\n",
      "[]\n",
      "12/12 [==============================] - 2s 146ms/step\n",
      "ACCURACY:\n",
      "0.8446866485013624\n",
      "\n",
      " [[60  1  0  0  0  0  0  2  5  1  1  2]\n",
      " [ 0 26  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2 54  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  4  0 57  0  0  1  0  0  0  0]\n",
      " [ 1  0  0  0  1  1  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 72  1  0  0  0 13]\n",
      " [ 2  0  0  0  0  2  1 20  0  0  0  3]\n",
      " [ 1  0  0  0  0  0  0  0  4  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  1  3  0  0  0  0 16]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "\"\"\"\n",
    "Required for NLP model\n",
    "\"\"\"\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()   \n",
    "\n",
    "# set parameters for word embeddings\n",
    "embed_size = 100 # how big is each word vector\n",
    "vocab_size = 25000 # how many unique words to use (i.e num rows in embedding vector) max\n",
    "input_length = 100 # max number of words in the input \n",
    "\n",
    "#set parameters for bilstm\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=300\n",
    "\n",
    "#EMBEDDING_FILE='glove.6B.100d.txt'    \n",
    "\n",
    "file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/nlp-bert-attn/bilstm_nlp_bert_attn_op.txt','w') #overwrites previous\n",
    "file.close()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    df_prediction = pd.DataFrame()\n",
    "    df_accuracy =  pd.DataFrame()\n",
    "    file = open('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/nlp-bert-attn/bilstm_nlp_bert_attn_op.txt','a') #append mode \n",
    "    \"\"\"\n",
    "    Change the range before executing\n",
    "    \"\"\"\n",
    "    for i in range(14,16):\n",
    "        outputname = 'nlp_bert_attn'+ str(i)        \n",
    "        predictions, acc, conf_matrix = execute_bilstm_nlp_bert_channel(i)\n",
    "        df_prediction[outputname] = predictions\n",
    "        df_accuracy[i] = [acc]\n",
    "        file.write(\"\\nIteration:\" + str(i) + \"\\nCategorical Accuracy:\" + str(acc) + \n",
    "                    \"\\nConfusion Matrix:\\n\" + str(conf_matrix) + \"\\n\\n\")\n",
    "        df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/nlp-bert-attn/predictions_bilstm_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "        df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/nlp-bert-attn/accuracy_bilstm_nlp_bert_attn_' + str(i) + '.csv')    \n",
    "    \n",
    "    df_prediction.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/nlp-bert-attn/predictions_bilstm_nlp_bert_attn.csv')    \n",
    "    df_accuracy.to_csv('/content/drive/My Drive/Python Notebook/SCS_CONVEX/output/nlp-bert-attn/accuracy_bilstm_nlp_bert_attn.csv')    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOVi8tWCg9mt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei8ZJF5c7h0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNpfm3GRnpBXRQH2Y5MTj94",
   "collapsed_sections": [],
   "name": "nlp_bert_attn_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bf1734997c34b82868fcb5d451dca36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b45c6e108ed44fbaf75b3d9fe384107",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c20efff4660479f96dd553d06b7d813",
      "value": 442
     }
    },
    "1bbfe18e36944f3b9f5bae15b45fae8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a70fd15ced84829ae92ad5bd32df296",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_802b4a111ead4e148bd22f54b7e1d40b",
      "value": 363423424
     }
    },
    "20757f8eebff4e3db2eb0079264cd7f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2185afc684d74475b9ee7eab479349f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a70fd15ced84829ae92ad5bd32df296": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c20efff4660479f96dd553d06b7d813": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2f680d38cf7343e9b8bb1ddd09b9942a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b45c6e108ed44fbaf75b3d9fe384107": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4398393c7a7b4b7f939aff76150e31a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bf1734997c34b82868fcb5d451dca36",
       "IPY_MODEL_f50351662d944e8a9cb9c5643663c183"
      ],
      "layout": "IPY_MODEL_56afabb0f9c54cba8b02caf76cd59cdf"
     }
    },
    "440e0a79e43b460890f1367221369fd6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56afabb0f9c54cba8b02caf76cd59cdf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6385739fbd324077876692216cf93b9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75553ac8c12145d7aedbc6058c57a156": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "802b4a111ead4e148bd22f54b7e1d40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "863987df04c24d06a287e83937143c41": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "896ff43e0f444a029f1f486fa95f392d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bbfe18e36944f3b9f5bae15b45fae8a",
       "IPY_MODEL_aaf942b51c854ee1b2acaf1a19d8624b"
      ],
      "layout": "IPY_MODEL_b66ddbb14c184b80990c725972e04391"
     }
    },
    "89f1c929c5fd4d3d91fee8678d2cae5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9181843ab3954946870a553a2f0996ff",
       "IPY_MODEL_aae4fa28289d4d9f85a9b7707c56fc99"
      ],
      "layout": "IPY_MODEL_935d250b766b43a6b3e15d9e3a7508a7"
     }
    },
    "9181843ab3954946870a553a2f0996ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_863987df04c24d06a287e83937143c41",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75553ac8c12145d7aedbc6058c57a156",
      "value": 231508
     }
    },
    "935d250b766b43a6b3e15d9e3a7508a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7999066b2d445f6bfda32de03b23f3b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aae4fa28289d4d9f85a9b7707c56fc99": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2185afc684d74475b9ee7eab479349f9",
      "placeholder": "​",
      "style": "IPY_MODEL_6385739fbd324077876692216cf93b9b",
      "value": " 232k/232k [00:00&lt;00:00, 913kB/s]"
     }
    },
    "aaf942b51c854ee1b2acaf1a19d8624b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7999066b2d445f6bfda32de03b23f3b",
      "placeholder": "​",
      "style": "IPY_MODEL_20757f8eebff4e3db2eb0079264cd7f2",
      "value": " 363M/363M [00:22&lt;00:00, 16.3MB/s]"
     }
    },
    "b66ddbb14c184b80990c725972e04391": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f50351662d944e8a9cb9c5643663c183": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_440e0a79e43b460890f1367221369fd6",
      "placeholder": "​",
      "style": "IPY_MODEL_2f680d38cf7343e9b8bb1ddd09b9942a",
      "value": " 442/442 [00:22&lt;00:00, 19.7B/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
